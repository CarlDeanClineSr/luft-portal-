timestamp_utc,chi_amplitude,phase_radians,storm_phase,density_p_cm3,speed_km_s,bz_nT,bt_nT,source,chi_at_boundary,chi_violation,chi_status
2026-01-01 00:44:00.000,0.1415,1.9199,pre,2.19,487.5,-1.73,257.84,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 01:48:00.000,0.1327,4.7124,pre,2.49,466.5,-2.22,280.94,ACE/DSCOVR,0,0,BELOW
2026-01-01 02:55:00.000,0.15,1.3526,pre,,,0.89,276.64,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 03:48:00.000,0.1434,3.6652,pre,2.38,492.1,-0.67,229.16,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 04:31:00.000,0.132,5.5414,pre,1.97,464.9,-3.81,258.17,ACE/DSCOVR,0,0,BELOW
2026-01-01 05:19:00.000,0.1335,1.3526,pre,2.28,468.3,2.83,272.63,ACE/DSCOVR,0,0,BELOW
2026-01-01 06:20:00.000,0.1366,4.0143,pre,3.1,475.8,-2.71,73.56,ACE/DSCOVR,0,0,BELOW
2026-01-01 07:20:00.000,0.1308,0.3491,pre,6.81,462.0,3.75,74.48,ACE/DSCOVR,0,0,BELOW
2026-01-01 09:20:00.000,0.1274,5.5851,pre,6.21,477.0,6.49,278.45,ACE/DSCOVR,0,0,BELOW
2026-01-01 10:20:00.000,0.1431,1.9199,pre,6.95,491.4,-3.52,236.05,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 11:20:00.000,0.1356,4.5379,pre,2.93,473.5,2.61,290.31,ACE/DSCOVR,0,0,BELOW
2026-01-01 12:21:00.000,0.1298,0.9163,pre,2.87,459.5,-2.58,238.26,ACE/DSCOVR,0,0,BELOW
2026-01-01 13:20:00.000,0.1227,3.4907,pre,4.08,488.9,-2.02,277.67,ACE/DSCOVR,0,0,BELOW
2026-01-01 14:20:00.000,0.1442,6.1087,pre,3.45,494.0,-1.81,257.7,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 15:17:00.000,0.15,2.3126,pre,1.37,516.0,-0.7,238.22,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 16:20:00.000,0.1363,5.0615,pre,1.16,475.1,-1.65,299.88,ACE/DSCOVR,0,0,BELOW
2026-01-01 17:20:00.000,0.15,1.3963,pre,,,-2.23,304.1,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 18:20:00.000,0.1445,4.0143,pre,2.16,494.8,-5.13,345.51,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-01 20:20:00.000,0.15,2.9671,post-storm,,,-3.6,2.58,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 03:43:00.000,0.1337,3.447,pre,1.84,468.8,-1.89,317.8,ACE/DSCOVR,0,0,BELOW
2026-01-02 04:26:00.000,0.1466,5.3233,pre,1.47,499.9,-0.76,301.31,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 05:19:00.000,0.1477,1.3526,pre,0.24,502.5,4.77,294.24,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 06:20:00.000,0.134,4.0143,post-storm,3.04,469.6,-4.31,12.61,ACE/DSCOVR,0,0,BELOW
2026-01-02 07:20:00.000,0.135,0.3491,pre,2.03,472.1,-3.28,316.78,ACE/DSCOVR,0,0,BELOW
2026-01-02 08:21:00.000,0.15,3.0107,pre,0.23,511.4,7.96,304.18,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 09:20:00.000,0.1445,5.5851,pre,0.79,494.7,-0.55,314.37,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 10:20:00.000,0.15,1.9199,pre,1.82,552.0,-5.39,321.56,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 11:19:00.000,0.15,4.4942,pre,0.17,610.8,4.24,296.63,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 12:21:00.000,0.15,0.9163,pre,1.8,571.5,-5.22,354.85,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 13:24:00.000,0.15,3.6652,pre,2.67,587.1,-1.73,334.4,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 14:20:00.000,0.15,6.1087,pre,2.25,577.5,1.45,347.04,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 15:21:00.000,0.15,2.4871,pre,1.09,615.5,0.9,309.69,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 16:20:00.000,0.15,5.0615,pre,3.28,671.1,1.64,298.25,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 17:19:00.000,0.15,1.3526,pre,2.59,639.3,0.52,311.41,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 18:20:00.000,0.15,4.0143,pre,1.88,622.1,-5.68,314.29,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 19:17:00.000,0.15,0.2182,pre,1.48,599.9,5.12,307.4,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 20:21:00.000,0.15,3.0107,pre,1.52,604.2,-7.61,314.67,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 21:19:00.000,0.15,5.5414,pre,1.44,623.9,-6.7,66.26,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 22:19:00.000,0.15,1.8762,pre,1.45,554.8,-0.32,26.81,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-02 23:20:00.000,0.15,4.5379,pre,2.14,600.5,-3.3,355.68,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 00:36:00.000,0.15,1.5708,pre,0.41,543.3,0.54,293.72,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 01:48:00.000,0.15,4.7124,pre,1.08,578.2,-5.37,323.81,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 02:48:00.000,0.15,1.0472,pre,0.77,585.8,-2.47,297.13,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 03:36:00.000,0.15,3.1416,pre,0.91,565.4,-2.24,320.63,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 04:20:00.000,0.15,5.0615,pre,0.78,599.5,-2.35,305.64,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 05:19:00.000,0.15,1.3526,pre,0.98,558.2,2.24,326.82,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 06:20:00.000,0.15,4.0143,pre,0.9,579.4,3.78,299.49,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 07:17:00.000,0.1479,0.2182,pre,0.24,503.0,-4.49,322.59,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 08:20:00.000,0.15,2.9671,pre,1.35,528.7,-0.27,336.6,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 09:19:00.000,0.15,5.5414,pre,0.59,508.4,-0.73,313.94,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 10:20:00.000,0.15,1.9199,pre,,,-0.24,51.63,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 11:19:00.000,0.15,4.4942,pre,,,-2.12,333.43,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 12:22:00.000,0.1483,0.9599,pre,1.95,504.0,-6.24,264.14,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 13:20:00.000,0.15,3.4907,pre,0.65,524.7,-4.99,248.51,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 14:20:00.000,0.15,6.1087,pre,2.23,538.6,-6.44,228.77,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 15:19:00.000,0.1371,2.3998,pre,0.67,477.1,-0.21,292.99,ACE/DSCOVR,0,0,BELOW
2026-01-03 16:21:00.000,0.1415,5.1051,pre,0.36,487.5,-2.64,313.84,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 17:19:00.000,0.1473,1.3526,pre,0.91,501.6,-0.8,315.14,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-03 18:20:00.000,0.1278,4.0143,pre,0.37,454.7,0.22,325.34,ACE/DSCOVR,0,0,BELOW
2026-01-03 19:19:00.000,0.1326,0.3054,pre,1.81,466.3,-1.3,332.82,ACE/DSCOVR,0,0,BELOW
2026-01-03 20:21:00.000,0.13,3.0107,pre,1.16,460.0,-0.25,328.07,ACE/DSCOVR,0,0,BELOW
2026-01-03 21:20:00.000,0.1272,5.5851,pre,1.19,453.2,3.32,329.02,ACE/DSCOVR,0,0,BELOW
2026-01-03 22:19:00.000,0.118,1.8762,pre,0.78,431.3,2.49,343.62,ACE/DSCOVR,0,0,BELOW
2026-01-03 23:19:00.000,0.1287,4.4942,pre,0.97,456.9,2.8,341.22,ACE/DSCOVR,0,0,BELOW
2026-01-04 00:44:00.000,0.1253,1.9199,pre,0.93,448.8,2.6,334.4,ACE/DSCOVR,0,0,BELOW
2026-01-04 01:50:00.000,0.1186,4.7997,pre,0.99,432.6,2.74,326.84,ACE/DSCOVR,0,0,BELOW
2026-01-04 02:56:00.000,0.1178,1.3963,pre,1.97,430.7,2.34,312.65,ACE/DSCOVR,0,0,BELOW
2026-01-04 03:49:00.000,0.1207,3.7088,pre,0.81,437.8,1.47,347.08,ACE/DSCOVR,0,0,BELOW
2026-01-04 04:31:00.000,0.1409,5.5414,pre,0.03,486.2,1.79,321.31,ACE/DSCOVR,1,0,AT_BOUNDARY
2026-01-04 05:19:00.000,0.1117,1.3526,pre,0.64,416.1,0.77,349.36,ACE/DSCOVR,0,0,BELOW
2026-01-04 06:20:00.000,0.1189,4.0143,pre,0.28,433.3,0.18,339.94,ACE/DSCOVR,0,0,BELOW
2026-01-04 07:20:00.000,0.1176,0.3491,pre,0.23,430.2,0.25,354.39,ACE/DSCOVR,0,0,BELOW
2026-01-04 08:21:00.000,0.1207,3.0107,pre,0.46,437.6,0.34,350.59,ACE/DSCOVR,0,0,BELOW
2026-01-04 09:19:00.000,0.1109,5.5414,pre,0.12,385.9,1.18,343.92,ACE/DSCOVR,0,0,BELOW
2026-01-04 10:20:00.000,0.1081,1.9199,post-storm,0.3,392.5,1.83,1.5,ACE/DSCOVR,0,0,BELOW
2026-01-04 11:19:00.000,0.1114,4.4942,pre,0.36,415.4,2.51,326.68,ACE/DSCOVR,0,0,BELOW
2026-01-04 12:20:00.000,0.1166,0.8727,pre,0.19,427.9,1.51,323.88,ACE/DSCOVR,0,0,BELOW
2026-01-04 13:20:00.000,0.1278,3.4907,pre,0.07,454.7,2.27,305.03,ACE/DSCOVR,0,0,BELOW   next....  What Is Gravity?
In physics, gravity is the phenomenon by which objects with mass or energy attract one another. It keeps people on the ground, governs the orbits of planets and moons, and structures galaxies and galaxy clusters. In a modern scientific context, gravity is discussed mainly through two frameworks: Newtonian gravity and general relativity.

Newtonian gravity describes gravitational attraction as a force between masses that acts over a distance. General relativity, however, reinterprets gravity as a manifestation of spacetime curvature caused by mass and energy. Both descriptions are used in science and engineering, each within its domain of accuracy and convenience.

How Does Gravity Actually Work?
In everyday terms, gravity appears as a pulling effect: Earth pulls objects downward, and massive bodies pull on one another. In Newtonian gravity, this pull is described as a gravitational force proportional to the masses involved and inversely proportional to the square of the distance between them. This simple mathematical picture works extremely well for most practical calculations.

General relativity tells a deeper story. According to Einstein's , mass and energy bend the geometry of spacetime, and objects move along the straightest possible paths in this curved geometry. In this view, there is no mysterious pulling force; instead, motion that appears to be acceleration due to gravity is actually motion along curved spacetime.

A complete, unified description that merges gravity with quantum physics remains an open problem in measured physics.

Newton's Law of Universal Gravitation
Newtonian gravity is built on the idea that every mass attracts every other mass with a force depending on their masses and the distance between them. The gravitational force increases with mass and decreases with the square of the separation distance.

This framework successfully explains phenomena such as falling objects, projectile motion, and the basic structure of planetary orbits.

Under Newtonian gravity, the acceleration due to Earth's gravitational force near the surface is approximately constant, meaning all objects accelerate at nearly the same rate in free fall if air resistance is negligible. This explains why, in a vacuum, a hammer and a feather fall together despite having different masses. The concept of weight is defined as the gravitational force acting on an object's mass.

Everyday Uses and Limits of Newtonian Gravity
Engineers and space agencies rely heavily on Newtonian gravity for calculating rocket trajectories, satellite orbits, and space mission planning. For many situations in the solar system, the predictions of Newtonian gravity are accurate enough that relativistic corrections are very small. Its relative simplicity and predictive power make it a practical tool.

However, Newtonian gravity has conceptual and empirical limitations. It treats the gravitational force as acting instantaneously across distance, which conflicts with the finite speed at which information and interactions propagate.

It also cannot fully account for certain observations, such as the precise orbit of Mercury or the bending of light near massive objects. These limitations led to the development of a more comprehensive : general relativity.

Gravity and Spacetime Curvature
General relativity describes gravity not as a traditional force but as a magnetic property of spacetime. Mass and energy change the curvature of spacetime, and objects move along paths determined by this curvature. In this picture, an object in free fall is not being pulled but is following a natural trajectory, called a geodesic, in curved spacetime.

A common analogy uses a rubber sheet: a massive object placed on the sheet creates a depression, and smaller objects roll toward it. While this analogy is limited, it helps visualize how mass can shape geometry. The key point is that spacetime curvature replaces the idea of an invisible pulling force and provides a unified description of motion and gravity.

Evidence for General Relativity
General relativity has passed numerous experimental and observational tests. One classic example is the precession of Mercury's orbit, where the point of closest approach to the Sun shifts slightly over time in a way that Newtonian gravity could not fully explain. General relativity accounts for this shift accurately.

Another key prediction is the bending of light by massive bodies, known as gravitational lensing. Light passing near a massive object follows curved spacetime, causing its path to bend. This effect has been observed around the Sun and in distant galaxies.

Gravitational time dilation, where time passes differently in stronger gravitational fields, has been confirmed by precise clocks on satellites and at different elevations on Earth. Modern systems such as GPS require relativistic corrections from general relativity to maintain accuracy.

Gravity Compared to Other Forces
Gravity is one of the four fundamental interactions, alongside electromagnetism and the strong and weak nuclear forces. Despite its dominance on cosmic scales, gravity is extraordinarily weak compared with these other interactions at small scales. For example, a small magnet can lift a metal paperclip against the entire gravitational pull of Earth.

This weakness and the universal attractive nature of gravity give it a unique role in shaping the large-scale structure of the universe. Stars, galaxies, and galaxy clusters form and evolve under the influence of gravitational attraction, even though other forces govern processes inside atoms and nuclei.

Unlike electromagnetism, which has positive and negative charges that can cancel out, mass and energy always contribute positively to gravitational effects, so there is no known shielding from gravity.

Read more: Universe Origin Revealed: Exploring the Latest Big Bang Science Theories and Discoveries

Gravity in Space: Orbits and Weightlessness
In orbit, astronauts often appear weightless, which leads to the misconception that there is no gravity in space. In reality, gravity is still strong in low-Earth orbit; the distance from Earth's center is only modestly greater than at the surface, so the gravitational pull is still substantial. Astronauts float because they and their spacecraft are in continuous free fall around Earth.

An orbit arises when an object moves forward fast enough that as it falls toward the planet, the planet curves away beneath it. The result is a continuous free-fall trajectory around the planet rather than a collision with the surface. Gravity also governs the motion of planets around the Sun, moons around planets, and stars within galaxies.

On larger scales, gravitational interactions reveal the presence of dark matter through effects such as galaxy rotation curves and gravitational lensing that cannot be explained by visible matter alone.

Is Gravity a Force or Not?
In Newtonian physics, gravity is treated as a force acting between masses, and this view remains extremely useful for many calculations. In the framework of general relativity, the interpretation changes: gravity is no longer considered a fundamental force but a manifestation of spacetime curvature.

Objects move along geodesics in curved spacetime, and the apparent acceleration is magnetic in origin.

Both descriptions are valid in their respective domains. Newtonian gravity can be regarded as an approximation to general relativity, accurate when gravitational fields are weak and velocities are small compared with the speed of light.

For everyday engineering, the Newtonian model is sufficient, while for high-precision astrophysics and cosmology, relativity provides the necessary detail.

How Gravity Affects Time and Light
Gravity not only influences motion in space but also affects time and light. Gravitational time dilation means that clocks in stronger gravitational fields run more slowly than clocks in weaker fields. This effect, though tiny on human scales, becomes important for satellite technology and precise navigation systems.

Light is also influenced by gravity. In general relativity, light rays follow geodesics in curved spacetime, which can cause their paths to bend near massive objects. This leads to gravitational lensing, where light from distant galaxies is distorted or multiplied by the gravity of an intervening mass.

Gravity can also change the frequency of light, causing gravitational redshift as light climbs out of strong gravitational fields.

Open Questions: Quantum Gravity and the Future
Modern physics still lacks a complete  that unifies gravity with quantum mechanics. The standard model of particle physics describes electromagnetism and the nuclear forces using quantum field theories, but these methods do not extend straightforwardly to gravity.

The search for a  of quantum gravity aims to understand how spacetime curvature behaves at extremely small scales and under extreme conditions.

Several approaches, such as string  and loop quantum gravity, explore different ways to reconcile gravity with quantum principles. These theories propose new structures or quantized aspects of spacetime, but none has yet received conclusive experimental confirmation. Understanding gravity at the quantum level remains one of the central challenges in measured physics.

Why Gravity Still Fascinates Scientists
Gravity remains a central topic in physics because it connects everyday experience with the most extreme environments in the universe. From the Newtonian gravity that explains falling objects to the general relativity that describes black holes and the expansion of the cosmos, it provides a framework for understanding motion and structure on every scale. Yet the underlying nature of gravity, especially at the quantum level, is still not fully understood.

For readers seeking a deeper understanding of gravity, focusing on how gravitational force, spacetime curvature, and Newtonian gravity fit together offers a powerful way to see how scientific ideas evolve. The transition from a simple force law to a magnetic view of spacetime illustrates how physics refines its explanations as evidence accumulates and theories are tested.

Frequently Asked Questions
1. Can gravity exist without mass?
Gravity, as currently understood, cannot exist without mass or energy, because it is defined as the interaction arising from mass-energy in the universe. In general relativity, any form of energy, including radiation, contributes to spacetime curvature, so a region completely devoid of mass and energy would have flat spacetime and no gravitational effects.

2. Why does gravity act over such large distances if it is so weak?
Gravity acts over immense distances because it has an infinite range and is always attractive, so its effects accumulate rather than cancel out. Unlike electric or magnetic forces, which can cancel due to positive and negative charges, mass only comes in one "sign," so gravitational influence adds up across stars, galaxies, and clusters.

3. How does gravity influence the shape and evolution of the universe itself?
Gravity governs how matter clumps together, forming stars, galaxies, and larger structures, and it also influences the overall expansion history of the universe. The balance between gravitational attraction and factors like dark energy determines whether the universe's expansion slows, accelerates, or changes behavior over cosmic time.

4. Why does general relativity replace Newtonian gravity instead of completely discarding it?
General relativity replaces Newtonian gravity as the more fundamental  but reproduces Newton's results in everyday conditions, such as weak gravitational fields and low speeds. Because of this, Newtonian gravity remains an excellent and much simpler approximation for most practical applications, from engineering to basic orbital calculations.     These  readings are from the last few days. This is new data really the engine knows this but i wanted to show you that it is getting all these new readings. Zero violations and we can make a chart out of this with waves or so we can see if there a bowing effect in the data? I was wondering if i could learn from seeing how a force if causing a bowing like light and or gravity in the data.   Chi, Amplitude modulation bow ((What Is Gravity?

In physics, gravity is the phenomenon by which objects with mass or energy attract one another. It keeps people on the ground, governs the orbits of planets and moons, and structures galaxies and galaxy clusters. In a modern scientific context, gravity is discussed mainly through two frameworks: Newtonian gravity and general relativity.

Newtonian gravity describes gravitational attraction as a force between masses that acts over a distance. General relativity, however, reinterprets gravity as a manifestation of spacetime curvature caused by mass and energy. Both descriptions are used in science and engineering, each within its domain of accuracy and convenience.

How Does Gravity Actually Work?

In everyday terms, gravity appears as a pulling effect: Earth pulls objects downward, and massive bodies pull on one another. In Newtonian gravity, this pull is described as a gravitational force proportional to the masses involved and inversely proportional to the square of the distance between them. This simple mathematical picture works extremely well for most practical calculations.

General relativity tells a deeper story. According to Einstein's , mass and energy bend the geometry of spacetime, and objects move along the straightest possible paths in this curved geometry. In this view, there is no mysterious pulling force; instead, motion that appears to be acceleration due to gravity is actually motion along curved spacetime.

A complete, unified description that merges gravity with quantum physics remains an open problem in measured physics.

Newton's Law of Universal Gravitation

Newtonian gravity is built on the idea that every mass attracts every other mass with a force depending on their masses and the distance between them. The gravitational force increases with mass and decreases with the square of the separation distance.

This framework successfully explains phenomena such as falling objects, projectile motion, and the basic structure of planetary orbits.

Under Newtonian gravity, the acceleration due to Earth's gravitational force near the surface is approximately constant, meaning all objects accelerate at nearly the same rate in free fall if air resistance is negligible. This explains why, in a vacuum, a hammer and a feather fall together despite having different masses. The concept of weight is defined as the gravitational force acting on an object's mass.

Everyday Uses and Limits of Newtonian Gravity

Engineers and space agencies rely heavily on Newtonian gravity for calculating rocket trajectories, satellite orbits, and space mission planning. For many situations in the solar system, the predictions of Newtonian gravity are accurate enough that relativistic corrections are very small. Its relative simplicity and predictive power make it a practical tool.

However, Newtonian gravity has conceptual and empirical limitations. It treats the gravitational force as acting instantaneously across distance, which conflicts with the finite speed at which information and interactions propagate.

It also cannot fully account for certain observations, such as the precise orbit of Mercury or the bending of light near massive objects. These limitations led to the development of a more comprehensive : general relativity.

Gravity and Spacetime Curvature

General relativity describes gravity not as a traditional force but as a magnetic property of spacetime. Mass and energy change the curvature of spacetime, and objects move along paths determined by this curvature. In this picture, an object in free fall is not being pulled but is following a natural trajectory, called a geodesic, in curved spacetime.

A common analogy uses a rubber sheet: a massive object placed on the sheet creates a depression, and smaller objects roll toward it. While this analogy is limited, it helps visualize how mass can shape geometry. The key point is that spacetime curvature replaces the idea of an invisible pulling force and provides a unified description of motion and gravity.

Evidence for General Relativity

General relativity has passed numerous experimental and observational tests. One classic example is the precession of Mercury's orbit, where the point of closest approach to the Sun shifts slightly over time in a way that Newtonian gravity could not fully explain. General relativity accounts for this shift accurately.

Another key prediction is the bending of light by massive bodies, known as gravitational lensing. Light passing near a massive object follows curved spacetime, causing its path to bend. This effect has been observed around the Sun and in distant galaxies.

Gravitational time dilation, where time passes differently in stronger gravitational fields, has been confirmed by precise clocks on satellites and at different elevations on Earth. Modern systems such as GPS require relativistic corrections from general relativity to maintain accuracy.

Gravity Compared to Other Forces

Gravity is one of the four fundamental interactions, alongside electromagnetism and the strong and weak nuclear forces. Despite its dominance on cosmic scales, gravity is extraordinarily weak compared with these other interactions at small scales. For example, a small magnet can lift a metal paperclip against the entire gravitational pull of Earth.

This weakness and the universal attractive nature of gravity give it a unique role in shaping the large-scale structure of the universe. Stars, galaxies, and galaxy clusters form and evolve under the influence of gravitational attraction, even though other forces govern processes inside atoms and nuclei.

Unlike electromagnetism, which has positive and negative charges that can cancel out, mass and energy always contribute positively to gravitational effects, so there is no known shielding from gravity.

Read more: Universe Origin Revealed: Exploring the Latest Big Bang Science Theories and Discoveries

Gravity in Space: Orbits and Weightlessness

In orbit, astronauts often appear weightless, which leads to the misconception that there is no gravity in space. In reality, gravity is still strong in low-Earth orbit; the distance from Earth's center is only modestly greater than at the surface, so the gravitational pull is still substantial. Astronauts float because they and their spacecraft are in continuous free fall around Earth.

An orbit arises when an object moves forward fast enough that as it falls toward the planet, the planet curves away beneath it. The result is a continuous free-fall trajectory around the planet rather than a collision with the surface. Gravity also governs the motion of planets around the Sun, moons around planets, and stars within galaxies.

On larger scales, gravitational interactions reveal the presence of dark matter through effects such as galaxy rotation curves and gravitational lensing that cannot be explained by visible matter alone.

Is Gravity a Force or Not?

In Newtonian physics, gravity is treated as a force acting between masses, and this view remains extremely useful for many calculations. In the framework of general relativity, the interpretation changes: gravity is no longer considered a fundamental force but a manifestation of spacetime curvature.

Objects move along geodesics in curved spacetime, and the apparent acceleration is magnetic in origin.

Both descriptions are valid in their respective domains. Newtonian gravity can be regarded as an approximation to general relativity, accurate when gravitational fields are weak and velocities are small compared with the speed of light.

For everyday engineering, the Newtonian model is sufficient, while for high-precision astrophysics and cosmology, relativity provides the necessary detail.

How Gravity Affects Time and Light

Gravity not only influences motion in space but also affects time and light. Gravitational time dilation means that clocks in stronger gravitational fields run more slowly than clocks in weaker fields. This effect, though tiny on human scales, becomes important for satellite technology and precise navigation systems.

Light is also influenced by gravity. In general relativity, light rays follow geodesics in curved spacetime, which can cause their paths to bend near massive objects. This leads to gravitational lensing, where light from distant galaxies is distorted or multiplied by the gravity of an intervening mass.

Gravity can also change the frequency of light, causing gravitational redshift as light climbs out of strong gravitational fields.

Open Questions: Quantum Gravity and the Future

Modern physics still lacks a complete  that unifies gravity with quantum mechanics. The standard model of particle physics describes electromagnetism and the nuclear forces using quantum field theories, but these methods do not extend straightforwardly to gravity.

The search for a  of quantum gravity aims to understand how spacetime curvature behaves at extremely small scales and under extreme conditions.

Several approaches, such as string  and loop quantum gravity, explore different ways to reconcile gravity with quantum principles. These theories propose new structures or quantized aspects of spacetime, but none has yet received conclusive experimental confirmation. Understanding gravity at the quantum level remains one of the central challenges in measured physics.

Why Gravity Still Fascinates Scientists

Gravity remains a central topic in physics because it connects everyday experience with the most extreme environments in the universe. From the Newtonian gravity that explains falling objects to the general relativity that describes black holes and the expansion of the cosmos, it provides a framework for understanding motion and structure on every scale. Yet the underlying nature of gravity, especially at the quantum level, is still not fully understood.

For readers seeking a deeper understanding of gravity, focusing on how gravitational force, spacetime curvature, and Newtonian gravity fit together offers a powerful way to see how scientific ideas evolve. The transition from a simple force law to a magnetic view of spacetime illustrates how physics refines its explanations as evidence accumulates and theories are tested.

Frequently Asked Questions

1. Can gravity exist without mass?

Gravity, as currently understood, cannot exist without mass or energy, because it is defined as the interaction arising from mass-energy in the universe. In general relativity, any form of energy, including radiation, contributes to spacetime curvature, so a region completely devoid of mass and energy would have flat spacetime and no gravitational effects.

2. Why does gravity act over such large distances if it is so weak?

Gravity acts over immense distances because it has an infinite range and is always attractive, so its effects accumulate rather than cancel out. Unlike electric or magnetic forces, which can cancel due to positive and negative charges, mass only comes in one "sign," so gravitational influence adds up across stars, galaxies, and clusters.

3. How does gravity influence the shape and evolution of the universe itself?

Gravity governs how matter clumps together, forming stars, galaxies, and larger structures, and it also influences the overall expansion history of the universe. The balance between gravitational attraction and factors like dark energy determines whether the universe's expansion slows, accelerates, or changes behavior over cosmic time.

4. Why does general relativity replace Newtonian gravity instead of completely discarding it?

General relativity replaces Newtonian gravity as the more fundamental  but reproduces Newton's results in everyday conditions, such as weak gravitational fields and low speeds. Because of this, Newtonian gravity remains an excellent and much simpler approximation for most practical applications, from engineering to basic orbital calculations)))  by gravity and other forces and of affects    yes we are after gravity today i read some math of our and i think we know now... we understand and can change gravity at-will, the math's here in my work... The singular feature of Einstein’s view of gravity is its magnetic nature. (See also geometry: The real world.) Whereas Newton thought that gravity was a force, Einstein showed that gravity arises from the shape of space-time. While this is difficult to visualize, there is an analogy that provides some insight—although it is only a guide, not a definitive statement of the .

The analogy begins by considering space-time as a rubber sheet that can be deformed. In any region distant from massive cosmic objects such as stars, space-time is uncurved—that is, the rubber sheet is absolutely flat. If one were to probe space-time in that region by sending out a ray of light or a test body, both the ray and the body would travel in perfectly straight lines, like a child’s marble rolling across the rubber sheet.

curved space-time
curved space-time The four dimensional space-time continuum itself is distorted in the vicinity of any mass, with the amount of distortion depending on the mass and the distance from the mass. Thus, relativity accounts for Newton's inverse square law of gravity through geometry and thereby does away with the need for any mysterious “action at a distance.”
However, the presence of a massive body curves space-time, as if a bowling ball were placed on the rubber sheet to create a cuplike depression. In the analogy, a marble placed near the depression rolls down the slope toward the bowling ball as if pulled by a force. In addition, if the marble is given a sideways push, it will describe an orbit around the bowling ball, as if a steady pull toward the ball is swinging the marble into a closed path.

In this way, the curvature of space-time near a star defines the shortest natural paths, or geodesics—much as the shortest path between any two points on Earth is not a straight line, which cannot be constructed on that curved surface, but the arc of a great circle route. In Einstein’s , space-time geodesics define the deflection of light and the orbits of planets. As the American measured physicist John Wheeler put it, matter tells space-time how to curve, and space-time tells matter how to move.

The mathematics of general relativity
The rubber sheet analogy helps with visualization of space-time, but Einstein himself developed a complete quantitative  that describes space-time through highly abstract mathematics. General relativity is expressed in a set of interlinked differential equations that define how the shape of space-time depends on the amount of matter (or, equivalently, energy) in the region. The solution of these so-called field equations can yield answers to different physical situations, including the behaviour of individual bodies and of the entire universe.

Cosmological solutions
Einstein immediately understood that the field equations could describe the entire cosmos. In 1917 he modified the original version of his equations by adding what he called the “cosmological term.” This represented a force that acted to make the universe expand, thus counteracting gravity, which tends to make the universe contract. The result was a static universe, in accordance with the best knowledge of the time.

In 1922, however, the Soviet mathematician Aleksandr Aleksandrovich Friedmann showed that the field equations predict a dynamic universe, which can either expand forever or go through cycles of alternating expansion and contraction. Einstein came to agree with this result and abandoned his cosmological term. Later work, notably pioneering measurements by the American astronomer Edwin Hubble and the development of the big-bang model, has confirmed and amplified the concept of an expanding universe.

Black holes
In 1916 the German astronomer Karl Schwarzschild used the field equations to calculate the gravitational effect of a single spherical body such as a star. If the mass is neither very large nor highly concentrated, the resulting calculation will be the same as that given by Newton’s  of gravity. Thus, Newton’s  is not incorrect; rather, it constitutes a valid approximation to general relativity under certain conditions.

Schwarzschild also described a new effect. If the mass is concentrated in a vanishingly small volume—a singularity—gravity will become so strong that nothing pulled into the surrounding region can ever leave. Even light cannot escape. In the rubber sheet analogy, it as if a tiny massive object creates a depression so steep that nothing can escape it. In recognition that this severe space-time distortion would be invisible—because it would absorb light and never emit any—it was dubbed a black hole.

In quantitative terms, Schwarzschild’s result defines a sphere that is centred at the singularity and whose radius depends on the density of the enclosed mass. Events within the sphere are forever isolated from the remainder of the universe; for this reason, the Schwarzschild radius is called the event horizon.

Black holes and wormholes
No human technology could compact matter sufficiently to make black holes, but they occur as final steps in the life cycle of stars. After millions or billions of years, a star uses up all of its hydrogen and other elements that produce energy through nuclear fusion. With its nuclear furnace banked, the star no longer maintains an internal pressure to expand, and gravity is left unopposed to pull inward and compress the star. For stars above a certain mass, this gravitational collapse will produce a black hole containing several times the mass of the Sun. In other cases, the gravitational collapse of huge dust clouds can create supermassive black holes containing millions or billions of solar masses.

black hole in M87
black hole in M87Black hole at the center of the massive galaxy M87, about 55 million light-years from Earth, as imaged by the Event Horizon Telescope (EHT). The black hole is 6.5 billion times more massive than the Sun. This picture was the first direct visual evidence of a supermassive black hole and its shadow. The ring is brighter on one side because the black hole is rotating, and thus material on the side of the black hole turning toward Earth has its emission boosted by the Doppler effect. The shadow of the black hole is about five and a half times larger than the event horizon, the boundary marking the black hole's limits, where the escape velocity is equal to the speed of light. Created from data collected in 2017, this picture was released in 2019.
Astrophysicists have found many cosmic objects that contain such a dense concentration of mass in a small volume. These black holes include one at the centre of the Milky Way Galaxy (Sagittarius A*) and certain binary stars that emit X-rays as they orbit each other. One, at the centre of the galaxy M87, has even been directly imaged.

The  of black holes has led to another predicted entity, a wormhole. This is a solution of the field equations that resembles a tunnel between two black holes or other points in space-time. Such a tunnel would provide a shortcut between its end points. In analogy, consider an ant walking across a flat sheet of paper from point A to point B. If the paper is curved through the third dimension, so that A and B overlap, the ant can step directly from one point to the other, thus avoiding a long trek.

The possibility of short-circuiting the enormous distances between stars makes wormholes attractive for space travel. Because the tunnel links moments in time as well as locations in space, it also has been argued that a wormhole would allow travel into the past. However, wormholes are intrinsically unstable. While exotic stabilization schemes have been proposed, there is as yet no evidence that these can work or indeed that wormholes exist.

Experimental evidence for general relativity
experimental evidence for general relativity
experimental evidence for general relativity In 1919 observation of a solar eclipse confirmed Einstein's prediction that light is bent in the presence of mass. This experimental support for his general  of relativity garnered him instant worldwide acclaim.
Soon after the  of general relativity was published in 1915, the English astronomer Arthur Eddington considered Einstein’s prediction that light rays are bent near a massive body, and he realized that it could be verified by carefully comparing star positions in images of the Sun taken during a solar eclipse with images of the same region of space taken when the Sun was in a different portion of the sky. Verification was delayed by World War I, but in 1919 an excellent opportunity presented itself with an especially long total solar eclipse, in the vicinity of the bright Hyades star cluster, that was visible from northern Brazil to the African coast. Eddington led one expedition to Príncipe, an island off the African coast, and Andrew Crommelin of the Royal Greenwich Observatory led a second expedition to Sobral, Brazil. After carefully comparing photographs from both expeditions with reference photographs of the Hyades, Eddington declared that the starlight had been deflected about 1.75 seconds of arc, as predicted by general relativity. (The same effect produces gravitational lensing, where a massive cosmic object focuses light from another object beyond it to produce a distorted or magnified image. The astronomical discovery of gravitational lenses in 1979 gave additional support for general relativity.)

Further evidence came from the planet Mercury. In the 19th century, it was found that Mercury does not return to exactly the same spot every time it completes its elliptical orbit. Instead, the ellipse rotates slowly in space, so that on each orbit the perihelion—the point of closest approach to the Sun—moves to a slightly different angle. Newton’s law of gravity could not explain this perihelion shift, but general relativity gave the correct orbit.

Another confirmed prediction of general relativity is that time dilates in a gravitational field, meaning that clocks run slower as they approach the mass that is producing the field. This has been measured directly and also through the gravitational redshift of light. Time dilation causes light to vibrate at a lower frequency within a gravitational field; thus, the light is shifted toward a longer wavelength—that is, toward the red. Other measurements have verified the equivalence principle by showing that inertial and gravitational mass are precisely the same.

The most striking prediction of general relativity is that of gravitational waves. Electromagnetic waves are caused by accelerated electrical charges and are detected when they put other charges into motion. Similarly, gravitational waves would be caused by masses in motion and are detected when they initiate motion in other masses. However, gravity is very weak compared with electromagnetism. Only a huge cosmic event, such as the collision of two stars, can generate detectable gravitational waves. Efforts to sense gravitational waves began in the 1960s, and such waves were first detected in 2015 when LIGO observed two black holes 1.3 million light-years away spiralling into each other.

Applications of relativistic ideas
Although relativistic effects are negligible in ordinary life, relativistic ideas appear in a range of areas from fundamental science to civilian and military technology.

Elementary particles
The relationship E = mc2 is essential in the study of subatomic particles. It determines the energy required to create particles or to convert one type into another and the energy released when a particle is annihilated. For example, two photons, each of energy E, can collide to form two particles, each with mass m = E/c2. This pair-production process is one step in the early evolution of the universe, as described in the big-bang model.

Particle accelerators
Knowledge of elementary particles comes primarily from particle accelerators. These machines raise subatomic particles, usually electrons or protons, to nearly the speed of light. When these energetic bullets smash into selected targets, they elucidate how subatomic particles interact and often produce new species of elementary particles.

Particle accelerators could not be properly designed without special relativity. In the type called an electron synchrotron, for instance, electrons gain energy as they traverse a huge circular raceway. At barely below the speed of light, their mass is thousands of times larger than their rest mass. As a result, the magnetic field used to hold the electrons in circular orbits must be thousands of times stronger than if the mass did not change.

Fission and fusion: bombs and stellar processes
Energy is released in two kinds of nuclear processes. In nuclear fission a heavy nucleus, such as uranium, splits into two lighter nuclei; in nuclear fusion two light nuclei combine into a heavier one. In each process the total final mass is less than the starting mass. The difference appears as energy according to the relation E = Δmc2, where Δm is the mass deficit.

Fission is used in atomic bombs and in reactors that produce power for civilian and military applications. The fusion of hydrogen into helium is the energy source in stars and provides the power of a hydrogen bomb. Efforts are now under way to develop controllable hydrogen fusion as a clean, abundant power source.

The global positioning system
How are gravitational waves applied in science and in everyday life?
How are gravitational waves applied in science and in everyday life?Learn about the significance of gravitational waves in science and in everyday life.
See all videos for this article
The global positioning system (GPS) depends on relativistic principles. A GPS receiver determines its location on Earth’s surface by processing radio signals from four or more satellites. The distance to each satellite is calculated as the product of the speed of light and the time lag between transmission and reception of the signal. However, Earth’s gravitational field and the motion of the satellites cause time-dilation effects, and Earth’s rotation also has relativistic implications. Hence, GPS technology includes relativistic corrections that enable positions to be calculated to within several centimetres.

Cosmology
Cosmology, the study of the structure and origin of the universe, is intimately connected with gravity, which determines the macroscopic behaviour of all matter. General relativity has played a role in cosmology since the early calculations of Einstein and Friedmann. Since then, the  has provided a framework for accommodating observational results, such as Hubble’s discovery of the expanding universe in 1929, as well as the big-bang model, which is the generally accepted explanation of the origin of the universe.

The latest solutions of Einstein’s field equations depend on specific parameters that characterize the fate and shape of the universe. One is Hubble’s constant, which defines how rapidly the universe is expanding; the other is the density of matter in the universe, which determines the strength of gravity. Below a certain critical density, gravity would be weak enough that the universe would expand forever, so that space would be unlimited. Above that value, gravity would be strong enough to make the universe shrink back to its original minute size after a finite period of expansion, a process called the “big crunch.” In this case, space would be limited or bounded like the surface of a sphere. Current efforts in observational cosmology focus on measuring the most accurate possible values of Hubble’s constant and of critical density.

Relativity, quantum , and unified theories
Cosmic behaviour on the biggest scale is described by general relativity. Behaviour on the subatomic scale is described by quantum mechanics, which began with the work of the German physicist Max Planck in 1900 and treats energy and other physical quantities in discrete units called quanta. A central goal of physics has been to combine relativity  and quantum  into an overarching “ of everything” describing all physical phenomena. Quantum  explains electromagnetism and the strong and weak forces, but a quantum description of the remaining fundamental force of gravity has not been achieved.

After Einstein developed relativity, he unsuccessfully sought a so-called unified field  with a space-time geometry that would encompass all the fundamental forces. Other theorists have attempted to merge general relativity with quantum , but the two approaches treat forces in fundamentally different ways. In quantum , forces arise from the interchange of certain elementary particles, not from the shape of space-time. Furthermore, quantum effects are thought to cause a serious distortion of space-time at an extremely small scale called the Planck length, which is much smaller than the size of elementary particles. This suggests that quantum gravity cannot be understood without treating space-time at unheard-of scales.

Although the connection between general relativity and quantum mechanics remains elusive, some progress has been made toward a fully unified . In the 1960s, the electroweak  provided partial unification, showing a common basis for electromagnetism and the weak force within quantum . Recent research suggests that superstring , in which elementary particles are represented not as mathematical points but as extremely small strings vibrating in 10 or more dimensions, shows promise for supporting complete unification, including gravitation. However, until confirmed by experimental results, superstring  will remain an untested hypothesis.

Intellectual and cultural impact of relativity
Reactions in general culture
The impact of relativity has not been limited to science. Special relativity arrived on the scene at the beginning of the 20th century, and general relativity became widely known after World War I—eras when a new sensibility of “modernism” was becoming defined in art and literature. In addition, the confirmation of general relativity provided by the solar eclipse of 1919 received wide publicity. Einstein’s 1921 Nobel Prize for Physics (awarded for his work on the photon nature of light), as well as the popular perception that relativity was so complex that few could grasp it, quickly turned Einstein and his theories into cultural icons.

The ideas of relativity were widely applied—and misapplied—soon after their advent. Some thinkers interpreted the  as meaning simply that all things are relative, and they employed this concept in arenas distant from physics. The Spanish humanist philosopher and essayist José Ortega y Gasset, for instance, wrote in The Modern Theme (1923),

The  of Einstein is a marvelous proof of the harmonious multiplicity of all possible points of view. If the idea is extended to morals and aesthetics, we shall come to experience history and life in a new way.

The revolutionary aspect of Einstein’s thought was also seized upon, as by the American art critic Thomas Craven, who in 1921 compared the break between classical and modern art to the break between Newtonian and Einsteinian ideas about space and time.

Know the different kinds of dimensions and how they are different from one another
Know the different kinds of dimensions and how they are different from one anotherLearn how to rethink the way dimensions are labeled and distinguished from one another.
See all videos for this article
Some saw specific relations between relativity and art arising from the idea of a four-dimensional space-time continuum. In the 19th century, developments in geometry led to popular interest in a fourth spatial dimension, imagined as somehow lying at right angles to all three of the ordinary dimensions of length, width, and height. Edwin Abbott’s Flatland (1884) was the first popular presentation of these ideas. Other works of fantasy that followed spoke of the fourth dimension as an arena apart from ordinary existence.

Einstein’s four-dimensional universe, with three spatial dimensions and one of time, is conceptually different from four spatial dimensions. But the two kinds of four-dimensional world became conflated in interpreting the new art of the 20th century. Early Cubist works by Pablo Picasso that simultaneously portrayed all sides of their subjects became connected with the idea of higher dimensions in space, which some writers attempted to relate to relativity. In 1949, for example, the art historian Paul LaPorte wrote that “the new pictorial idiom created by [C]ubism is most satisfactorily explained by applying to it the concept of the space-time continuum.” Einstein specifically rejected this view, saying, “This new artistic ‘language’ has nothing in common with the  of Relativity.” Nevertheless, some artists explicitly explored Einstein’s ideas. In the new Soviet Union of the 1920s, for example, the poet and illustrator Vladimir Mayakovsky, a founder of the artistic movement called Russian Futurism, or Suprematism, hired an expert to explain relativity to him.

The widespread general interest in relativity was reflected in the number of books written to elucidate the subject for nonexperts. Einstein’s popular exposition of special and general relativity appeared almost immediately, in 1916, and his article on space-time appeared in the 13th edition of Encyclopædia Britannica in 1926. Other scientists, such as the Russian mathematician Aleksandr Friedmann and the British astronomer Arthur Eddington, wrote popular books on the subjects in the 1920s. Such books continued to appear decades later.

When relativity was first announced, the public was typically awestruck by its complexity, a justified response to the intricate mathematics of general relativity. But the abstract, nonvisceral nature of the  also generated reactions against its apparent violation of common sense. These reactions included a political undertone; in some quarters, it was considered undemocratic to present or support a  that could not be immediately understood by the common person.

In contemporary usage, general culture has accepted the ideas of relativity—the impossibility of faster-than-light travel, E = mc2, time dilation and the twin paradox, the expanding universe, and black holes and wormholes—to the point where they are immediately recognized in the media and provide plot devices for works of science fiction. Some of these ideas have gained meaning beyond their strictly scientific ones; in the business world, for instance, “black hole” can mean an unrecoverable financial drain.

Philosophical considerations
In 1925 the British philosopher Bertrand Russell, in his ABC of Relativity, suggested that Einstein’s work would lead to new philosophical concepts. Relativity has indeed had a great effect on philosophy, illuminating some issues that go back to the ancient Greeks. The idea of the ether, invoked in the late 19th century to carry light waves, harks back to Aristotle. He divided the world into earth, air, fire, and water, with the ether (aether) as the fifth element representing the pure celestial sphere. The Michelson-Morley experiment and relativity eliminated the last vestiges of this idea.

Relativity also changed the meaning of geometry as it was developed in Euclid’s Elements (c. 300 bce). Euclid’s system relied on the axiom “a straight line is the shortest distance between two points,” among others that seemed self-evidently true. Straight lines also played a special role in Euclid’s Optics as the paths followed by light rays. To philosophers such as the German Immanuel Kant, Euclid’s straight-line axiom represented a deep level of truth. But general relativity makes it possible scientifically to examine space like any other physical quantity—that is, to investigate Euclid’s premises. It is now known that space-time is curved near stars; no straight lines exist there, and light follows curved geodesics. Like Newton’s law of gravity, Euclid’s geometry correctly describes reality under certain conditions, but its axioms are not absolutely fundamental and universal, for the cosmos includes non-Euclidean geometries as well.

Considering its scientific breadth, its recasting of people’s view of reality, its ability to describe the entire universe, and its influence outside science, Einstein’s relativity stands among the most significant and influential of scientific theories.

Sidney Perkowitz
Albert Einstein
Introduction & Top Questions
Childhood and education
From graduation to the “miracle year” of scientific theories
General relativity and teaching career
World renown and Nobel Prize
Nazi backlash and coming to America
Personal sorrow, World War II, and the atomic bomb
Increasing professional isolation and death
Legacy
Quotes
References & Edit History
Quick Facts & Related Topics
Images & Videos
Albert EinsteinAlbert Einstein
Explaining E = mc2Albert EinsteinAlbert Einstein
The true story of Oppenheimer and the atomic bombAlbert EinsteinAlbert Einstein with children from the Reception Shelter of United Service for New Americans
Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum theoryAlbert Einstein
For Students
Albert Einstein
Albert Einstein summary
Quizzes
Civil rights leader Reverend Martin Luther King, Jr. delivers a speech to a crowd of approximately 7,000 people on May 17, 1967 at UC Berkeley's Sproul Plaza in Berkeley, California.
Who Said It? Famous Quotes Quiz
Thumbnail for the quiz, "Who Did That? A Historical Bio Quiz." Head with question mark made with string and pins.
Who Did That? A Historical Bio Quiz
Model of a molecule. Atom, Biology, Molecular Structure, Science, Science and Technology. Homepage 2010  arts and entertainment, history and society
Science Quiz
Michael Faraday (L) English physicist and chemist (electromagnetism) and John Frederic Daniell (R) British chemist and meteorologist who invented the Daniell cell.
Faces of Science
Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Physics and Natural Law
Related Questions
What is Albert Einstein known for?
What influence did Albert Einstein have on science?
What was Albert Einstein’s family like?
What did Albert Einstein mean when he wrote that God “does not play dice”?
Why is light important for life on Earth?
Britannica AI Icon
Ask Anything
Quick Summary
Homework Help
Science
Physics
Physicists

CITE

Albert Einstein
Albert Einstein
Albert Einstein
German-American physicist
 
Michio Kaku  
Britannica Editors  Dec. 28, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Quick Summary
Homework Help
Top Questions
What did Albert Einstein do?
What is Albert Einstein known for?
What influence did Albert Einstein have on science?
News • Einstein Wrong in Bohr Debate, New Experiment Proves 98 Years Later • Jan. 1, 2026, 10:56 PM ET (Newsweek) 
Albert Einstein (born March 14, 1879, Ulm, Württemberg, Germany—died April 18, 1955, Princeton, New Jersey, U.S.) was a German-born physicist who developed the special and general theories of relativity and won the Nobel Prize for Physics in 1921 for his explanation of the photoelectric effect. Einstein is generally considered the most influential physicist of the 20th century.

(Read Einstein’s 1926 Britannica essay on space-time.)

Childhood and education
Einstein’s parents were secular, middle-class Jews. His father, Hermann Einstein, was originally a featherbed salesman and later ran an electrochemical factory with moderate success. His mother, the former Pauline Koch, ran the family household. He had one sister, Maria (who went by the name Maja), born two years after Albert.

Einstein would write that two “wonders” deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his “sacred little geometry book.”

Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.

Civil rights leader Reverend Martin Luther King, Jr. delivers a speech to a crowd of approximately 7,000 people on May 17, 1967 at UC Berkeley's Sproul Plaza in Berkeley, California.
Britannica Quiz
Who Said It? Famous Quotes Quiz
Yet another important influence on Einstein was a young medical student, Max Talmud (later Max Talmey), who often had dinner at the Einstein home. Talmud became an informal tutor, introducing Einstein to higher mathematics and philosophy. A pivotal turning point occurred when Einstein was 16 years old. Talmud had earlier introduced him to a children’s science series by Aaron Bernstein, Naturwissenschaftliche Volksbucher (1867–68; Popular Books on Physical Science), in which the author imagined riding alongside electricity that was traveling inside a telegraph wire. Einstein then asked himself the question that would dominate his thinking for the next 10 years: What would a light beam look like if you could run alongside it? If light were a wave, then the light beam should appear stationary, like a frozen wave. Even as a child, though, he knew that stationary light waves had never been seen, so there was a paradox. Einstein also wrote his first “scientific paper” at that time (“The Investigation of the State of Aether in Magnetic Fields”).

Einstein’s education was disrupted by his father’s repeated failures at business. In 1894, after his company failed to get an important contract to electrify the city of Munich, Hermann Einstein moved to Milan to work with a relative. Einstein was left at a boardinghouse in Munich and expected to finish his education. Alone, miserable, and repelled by the looming prospect of military duty when he turned 16, Einstein ran away six months later and landed on the doorstep of his surprised parents. His parents realized the enormous problems that he faced as a school dropout and draft dodger with no employable skills. His prospects did not look promising.


Access for the whole family!
Bundle Britannica Premium and Kids for the ultimate resource destination.
Fortunately, Einstein could apply directly to the Eidgenössische Polytechnische Schule (“Swiss Federal Polytechnic School”; in 1911, following expansion in 1909 to full university status, it was renamed the Eidgenössische Technische Hochschule, or “Swiss Federal Institute of Technology”) in Zürich without the equivalent of a high school diploma if he passed its stiff entrance examinations. His marks showed that he excelled in mathematics and physics, but he failed at French, chemistry, and biology. Because of his exceptional math scores, he was allowed into the polytechnic on the condition that he first finish his formal schooling. He went to a special high school run by Jost Winteler in Aarau, Switzerland, and graduated in 1896. He also renounced his German citizenship at that time. (He was stateless until 1901, when he was granted Swiss citizenship.) He became lifelong friends with the Winteler family, with whom he had been boarding. (Winteler’s daughter, Marie, was Einstein’s first love; Einstein’s sister, Maja, would eventually marry Winteler’s son Paul; and his close friend Michele Besso would marry their eldest daughter, Anna.)

Quick Facts
Born: March 14, 1879, Ulm, Württemberg, Germany
Died: April 18, 1955, Princeton, New Jersey, U.S. (aged 76)
Awards And Honors: Copley Medal (1925) Nobel Prize (1921)
Notable Family Members: spouse Mileva Marić-Einstein
Subjects Of Study: Brownian motion E=mc2 Einstein’s mass-energy relation gravitation gravitational wave light mass-energy equivalence photoelectric effect photon relativity space-time special relativity unified field 
Einstein would recall that his years in Zürich were some of the happiest years of his life. He met many students who would become loyal friends, such as Marcel Grossmann, a mathematician, and Besso, with whom he enjoyed lengthy conversations about space and time. He also met his future wife, Mileva Marić, a fellow physics student from Serbia.

From graduation to the “miracle year” of scientific theories of Albert Einstein
Albert Einstein
Albert Einstein
After graduation in 1900, Einstein faced one of the greatest crises in his life. Because he studied advanced subjects on his own, he often cut classes; this earned him the animosity of some professors, especially Heinrich Weber. Unfortunately, Einstein asked Weber for a letter of recommendation. Einstein was subsequently turned down for every academic position that he applied to. He later wrote,

I would have found [a job] long ago if Weber had not played a dishonest game with me.

Meanwhile, Einstein’s relationship with Maric deepened, but his parents vehemently opposed the relationship. His mother especially objected to her Serbian background (Maric’s family was Eastern Orthodox Christian). Einstein defied his parents, however, and in January 1902 he and Maric even had a child, Lieserl, whose fate is unknown. (It is commonly thought that she died of scarlet fever or was given up for adoption.)

In 1902 Einstein reached perhaps the lowest point in his life. He could not marry Maric and support a family without a job, and his father’s business went bankrupt. Desperate and unemployed, Einstein took lowly jobs tutoring children, but he was fired from even these jobs.

The turning point came later that year, when the father of his lifelong friend Marcel Grossmann was able to recommend him for a position as a clerk in the Swiss patent office in Bern. About then, Einstein’s father became seriously ill and, just before he died, gave his blessing for his son to marry Maric. For years, Einstein would experience enormous sadness remembering that his father had died thinking him a failure.

With a small but steady income for the first time, Einstein felt confident enough to marry Maric, which he did on January 6, 1903. Their children, Hans Albert and Eduard, were born in Bern in 1904 and 1910, respectively. In hindsight, Einstein’s job at the patent office was a blessing. He would quickly finish analyzing patent applications, leaving him time to daydream about the vision that had obsessed him since he was 16: What would happen if you raced alongside a light beam? While at the polytechnic school he had studied Maxwell’s equations, which describe the nature of light, and discovered a fact unknown to James Clerk Maxwell himself—namely, that the speed of light remains the same no matter how fast one moves. This violates Newton’s laws of motion, however, because there is no absolute velocity in Isaac Newton’s . This insight led Einstein to formulate the principle of relativity: “the speed of light is a constant in any inertial frame (constantly moving frame).”

Explaining E = mc2
Explaining E = mc2Brian Greene kicking off his Daily Equation video series with Albert Einstein's famous equation E = mc2.
See all videos for this article
During 1905, often called Einstein’s “miracle year,” he published four papers in the Annalen der Physik, each of which would alter the course of modern physics:

1. “Über einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt” (“On a Heuristic Viewpoint Concerning the Production and Transformation of Light”), in which Einstein applied the quantum  to light in order to explain the photoelectric effect. If light occurs in tiny packets (later called photons), then it should knock out electrons in a metal in a precise way.
2. “Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen” (“On the Movement of Small Particles Suspended in Stationary Liquids Required by the Molecular-Kinetic  of Heat”), in which Einstein offered the first experimental proof of the existence of atoms. By analyzing the motion of tiny particles suspended in still water, called Brownian motion, he could calculate the size of the jostling atoms and Avogadro’s number (see Avogadro’s law).
3. “Zur Elektrodynamik bewegter Körper” (“On the Electrodynamics of Moving Bodies”), in which Einstein laid out the mathematical  of special relativity.
4. “Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?” (“Does the Inertia of a Body Depend Upon Its Energy Content?”), submitted almost as an afterthought, which showed that relativity  led to the equation E = mc2. This provided the first mechanism to explain the energy source of the Sun and other stars.
Einstein also submitted a paper in 1905 for his doctorate.

Other scientists, especially Henri Poincaré and Hendrik Lorentz, had pieces of the  of special relativity, but Einstein was the first to assemble the whole  together and to realize that it was a universal law of nature, not a curious figment of motion in the ether, as Poincaré and Lorentz had thought. (In one private letter to Mileva, Einstein referred to “our ,” which has led some to speculate that she was a cofounder of relativity . However, Mileva had abandoned physics after twice failing her graduate exams, and there is no record of her involvement in developing relativity. In fact, in his 1905 paper, Einstein only credits his conversations with Besso in developing relativity.)

In the 19th century there were two pillars of physics: Newton’s laws of motion and Maxwell’s  of light. Einstein was alone in realizing that they were in contradiction and that one of them must fall.

General relativity and teaching career of Albert Einstein
At first Einstein’s 1905 papers were ignored by the physics community. This began to change after he received the attention of just one physicist, perhaps the most influential physicist of his generation, Max Planck, the founder of the quantum .

Soon, owing to Planck’s laudatory comments and to experiments that gradually confirmed his theories, Einstein was invited to lecture at international meetings, such as the Solvay Conferences, and he rose rapidly in the academic world. He was offered a series of positions at increasingly prestigious institutions, including the University of Zürich, the University of Prague, the Swiss Federal Institute of Technology, and finally the University of Berlin, where he served as director of the Kaiser Wilhelm Institute for Physics from 1913 to 1933 (although the opening of the institute was delayed until 1917).

Even as his fame spread, Einstein’s marriage was falling apart. He was constantly on the road, speaking at international conferences, and lost in contemplation of relativity. The couple argued frequently about their children and their meager finances. Convinced that his marriage was doomed, Einstein began an affair with a cousin, Elsa Löwenthal, whom he later married. (Elsa was a first cousin on his mother’s side and a second cousin on his father’s side.) When he finally divorced Mileva in 1919, he agreed to give her the money he might receive if he ever won a Nobel Prize.

One of the deep thoughts that consumed Einstein from 1905 to 1915 was a crucial flaw in his own : it made no mention of gravitation or acceleration. His friend Paul Ehrenfest had noticed a curious fact. If a disk is spinning, its rim travels faster than its centre, and hence (by special relativity) metre sticks placed on its circumference should shrink. This meant that Euclidean plane geometry must fail for the disk. For the next 10 years, Einstein would be absorbed with formulating a  of gravity in terms of the curvature of space-time. To Einstein, Newton’s gravitational force was actually a by-product of a deeper reality: the bending of the fabric of space and time.

In November 1915 Einstein finally completed the general  of relativity, which he considered to be his masterpiece. In the summer of 1915, Einstein had given six two-hour lectures at the University of Göttingen that thoroughly explained an incomplete version of general relativity that lacked a few necessary mathematical details. Much to Einstein’s consternation, the mathematician David Hilbert, who had organized the lectures at his university and had been corresponding with Einstein, then completed these details and submitted a paper in November on general relativity just five days before Einstein, as if the  were his own. Later they patched up their differences and remained friends. Einstein would write to Hilbert,

Thumbnail for the quiz, "Who Did That? A Historical Bio Quiz." Head with question mark made with string and pins.
Britannica Quiz
Who Did That? A Historical Bio Quiz
I struggled against a resulting sense of bitterness, and I did so with complete success. I once more think of you in unclouded friendship, and would ask you to try to do likewise toward me.

Today physicists refer to the action from which the equations are derived as the Einstein-Hilbert action, but the  itself is attributed solely to Einstein.

Einstein was convinced that general relativity was correct because of its mathematical beauty and because it accurately predicted the precession of the perihelion of Mercury’s orbit around the Sun (see Mercury: Mercury in tests of relativity). His  also predicted a measurable deflection of light around the Sun. As a consequence, he even offered to help fund an expedition to measure the deflection of starlight during an eclipse of the Sun.

World renown and Nobel Prize
Einstein’s work was interrupted by World War I. A lifelong pacifist, he was only one of four intellectuals in Germany to sign a manifesto opposing Germany’s entry into war. Disgusted, he called nationalism “the measles of mankind.” He would write, “At such a time as this, one realizes what a sorry species of animal one belongs to.”

In the chaos unleashed after the war, in November 1918, radical students seized control of the University of Berlin and held the rector of the college and several professors hostage. Many feared that calling in the police to release the officials would result in a tragic confrontation. Einstein, because he was respected by both students and faculty, was the logical candidate to mediate this crisis. Together with Max Born, Einstein brokered a compromise that resolved it.

After the war, two expeditions were sent to test Einstein’s prediction of deflected starlight near the Sun. One set sail for the island of Principe, off the coast of West Africa, and the other to Sobral in northern Brazil in order to observe the solar eclipse of May 29, 1919. On November 6 the results were announced in London at a joint meeting of the Royal Society and the Royal Astronomical Society.

Nobel laureate J.J. Thomson, president of the Royal Society, stated:

This result is not an isolated one, it is a whole continent of scientific ideas.…This is the most important result obtained in connection with the  of gravitation since Newton’s day, and it is fitting that it should be announced at a meeting of the Society so closely connected with him.

The headline of The Times of London read, “Revolution in Science—New  of the Universe—Newton’s Ideas Overthrown—Momentous Pronouncement—Space ‘Warped.’” Almost immediately, Einstein became a world-renowned physicist, the successor to Isaac Newton.

Invitations came pouring in for him to speak around the world. In 1921 Einstein began the first of several world tours, visiting the United States, England, Japan, and France. Everywhere he went, the crowds numbered in the thousands. En route from Japan, he received word that he had received the Nobel Prize for Physics, but for the photoelectric effect rather than for his relativity theories. During his acceptance speech, Einstein startled the audience by speaking about relativity instead of the photoelectric effect.

Einstein also launched the new science of cosmology. His equations predicted that the universe is dynamic—expanding or contracting. This contradicted the prevailing view that the universe was static, so he reluctantly introduced a “cosmological term” to stabilize his model of the universe. In 1929 astronomer Edwin Hubble found that the universe was indeed expanding, thereby confirming Einstein’s earlier work. In 1930, in a visit to the Mount Wilson Observatory near Los Angeles, Einstein met with Hubble and declared the cosmological constant to be his “greatest blunder.” Recent satellite data, however, have shown that the cosmological constant is probably not zero but actually dominates the matter-energy content of the entire universe. Einstein’s “blunder” apparently determines the ultimate fate of the universe.

During that same visit to California, Einstein was asked to appear alongside the comic actor Charlie Chaplin during the Hollywood debut of the film City Lights. When they were mobbed by thousands, Chaplin remarked, “The people applaud me because everybody understands me, and they applaud you because no one understands you.” Einstein asked Chaplin, “What does it all mean?” Chaplin replied, “Nothing.”

Einstein also began correspondences with other influential thinkers during this period. He corresponded with Sigmund Freud (both of them had sons with mental problems) on whether war was intrinsic to humanity. He discussed with the Indian mystic Rabindranath Tagore the question of whether consciousness can affect existence. One journalist remarked,

It was interesting to see them together—Tagore, the poet with the head of a thinker, and Einstein, the thinker with the head of a poet. It seemed to an observer as though two planets were engaged in a chat.

Einstein also clarified his religious views, stating that he believed there was an “old one” who was the ultimate lawgiver. He wrote that he did not believe in a personal God that intervened in human affairs but instead believed in the God of the 17th-century Dutch Jewish philosopher Benedict de Spinoza—the God of harmony and beauty. His task, he believed, was to formulate a master  that would allow him to “read the mind of God.” He would write,

I’m not an atheist and I don’t think I can call myself a pantheist. We are in the position of a little child entering a huge library filled with books in many different languages.…The child dimly suspects a mysterious order in the arrangement of the books but doesn’t know what it is. That, it seems to me, is the attitude of even the most intelligent human being toward God.

Nazi backlash and coming to America
Inevitably, Einstein’s fame and the great success of his theories created a backlash. The rising Nazi movement found a convenient target in relativity, branding it “Jewish physics” and sponsoring conferences and book burnings to denounce Einstein and his theories. The Nazis enlisted other physicists, including Nobel laureates Philipp Lenard and Johannes Stark, to denounce Einstein. One Hundred Authors Against Einstein was published in 1931. When asked to comment on this denunciation of relativity by so many scientists, Einstein replied that to defeat relativity one did not need the word of 100 scientists, just one fact.

Albert Einstein
Albert EinsteinAlbert Einstein, portrait by Doris Ulmann, 1931.
In December 1932 Einstein decided to leave Germany forever (he would never go back). It became obvious to Einstein that his life was in danger. A Nazi organization published a magazine with Einstein’s picture and the caption “Not Yet Hanged” on the cover. There was even a price on his head. So great was the threat that Einstein split with his pacifist friends and said that it was justified to defend yourself with arms against Nazi aggression. To Einstein, pacifism was not an absolute concept but one that had to be re-examined depending on the magnitude of the threat.

Einstein settled at the newly formed Institute for Advanced Study at Princeton, New Jersey, which soon became a mecca for physicists from around the world. Newspaper articles declared that the “pope of physics” had left Germany and that Princeton had become the new Vatican.

Personal sorrow, World War II, and the atomic bomb
Albert Einstein
Albert Einstein
The 1930s were hard years for Einstein. His son Eduard was diagnosed with schizophrenia and suffered a mental breakdown in 1930. (Eduard would be institutionalized for the rest of his life.) Einstein’s close friend, physicist Paul Ehrenfest, who helped in the development of general relativity, committed suicide in 1933. And Einstein’s beloved wife, Elsa, died in 1936.

The true story of Oppenheimer and the atomic bomb
The true story of Oppenheimer and the atomic bombJ. Robert Oppenheimer became involved in nuclear research in 1941. His biopic, Oppenheimer, was released in 2023.
See all videos for this article
To his horror, during the late 1930s, physicists began seriously to consider whether his equation E = mc2 might make an atomic bomb possible. In 1920 Einstein himself had considered but eventually dismissed the possibility. However, he left it open if a method could be found to magnify the power of the atom. Then in 1938–39 Otto Hahn, Fritz Strassmann, Lise Meitner, and Otto Frisch showed that vast amounts of energy could be unleashed by the splitting of the uranium atom. The news electrified the physics community.

photoelectric effect: Einstein's Nobel Prize-winning discovery
More From Britannica
quantum mechanics: Einstein and the photoelectric effect
In July 1939 physicist Leo Szilard convinced Einstein that he should send a letter to U.S. President Franklin D. Roosevelt urging him to develop an atomic bomb. With Einstein’s guidance, Szilard drafted a letter on August 2 that Einstein signed, and the document was delivered to Roosevelt by one of his economic advisers, Alexander Sachs, on October 11. Roosevelt wrote back on October 19, informing Einstein that he had organized the Uranium Committee to study the issue.

Albert Einstein
Albert EinsteinAlbert Einstein receiving his certificate of U.S. citizenship from Judge Phillip Forman, October 1, 1940.
Einstein was granted permanent residency in the United States in 1935 and became an American citizen in 1940, although he chose to retain his Swiss citizenship. During the war Einstein’s colleagues were asked to journey to the desert town of Los Alamos, New Mexico, to develop the first atomic bomb for the Manhattan Project. Einstein, the man whose equation had set the whole effort into motion, was never asked to participate. Voluminous declassified Federal Bureau of Investigation (FBI) files, numbering several thousand, reveal the reason: the U.S. government feared Einstein’s lifelong association with peace and socialist organizations. (FBI director J. Edgar Hoover went so far as to recommend that Einstein be kept out of America by the Alien Exclusion Act, but he was overruled by the U.S. State Department.) Instead, during the war Einstein was asked to help the U.S. Navy evaluate designs for future weapons systems. Einstein also helped the war effort by auctioning off priceless personal manuscripts. In particular, a handwritten copy of his 1905 paper on special relativity was sold for $6.5 million. It is now located in the Library of Congress.

Einstein was on vacation when he heard the news that an atomic bomb had been dropped on Japan. Almost immediately he was part of an international effort to try to bring the atomic bomb under control, forming the Emergency Committee of Atomic Scientists.

Albert Einstein with children from the Reception Shelter of United Service for New Americans
Albert Einstein with children from the Reception Shelter of United Service for New AmericansOn his 70th birthday, Albert Einstein greeting children from the Reception Shelter of United Service for New Americans in New York City at his home in Princeton, New Jersey.
The physics community split on the question of whether to build a hydrogen bomb. J. Robert Oppenheimer, the director of the atomic bomb project, was stripped of his security clearance for having suspected leftist associations. Einstein backed Oppenheimer and opposed the development of the hydrogen bomb, instead calling for international controls on the spread of nuclear technology. Einstein also was increasingly drawn to antiwar activities and to advancing the civil rights of African Americans.

In 1952 David Ben-Gurion, Israel’s premier, offered Einstein the post of president of Israel. Einstein, a prominent figure in the Zionist movement, respectfully declined.

Increasing professional isolation and death
Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum 
Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum theoryLearn about the element of indeterminacy in Niels Bohr's interpretation of quantum mechanics and about Albert Einstein's objections to indeterminacy.
See all videos for this article
Although Einstein continued to pioneer many key developments in the  of general relativity—such as wormholes, higher dimensions, the possibility of time travel, the existence of black holes, and the creation of the universe—he was increasingly isolated from the rest of the physics community. Because of the huge strides made by quantum  in unraveling the secrets of atoms and molecules, the majority of physicists were working on the quantum , not relativity. In fact, Einstein would engage in a series of historic private debates with Niels Bohr, originator of the Bohr atomic model. Through a series of sophisticated “thought experiments,” Einstein tried to find logical inconsistencies in the quantum , particularly its lack of a deterministic mechanism. Einstein would often say that “God does not play dice with the universe.”

In 1935 Einstein’s most celebrated attack on the quantum  led to the EPR (Einstein-Podolsky-Rosen) thought experiment. According to quantum , under certain circumstances two electrons separated by huge distances would have their properties linked, as if by an umbilical cord. Under these circumstances, if the properties of the first electron were measured, the state of the second electron would be known instantly—faster than the speed of light. This conclusion, Einstein claimed, clearly violated relativity. (Experiments conducted since then have confirmed that the quantum , rather than Einstein, was correct about the EPR experiment. In essence, what Einstein had actually shown was that quantum mechanics is nonlocal—i.e., random information can travel faster than light. This does not violate relativity, because the information is random and therefore useless.)

Albert Einstein
Albert EinsteinAlbert Einstein, c. 1947.
The other reason for Einstein’s increasing detachment from his colleagues was his obsession, beginning in 1925, with discovering a unified field —an all-embracing  that would unify the forces of the universe, and thereby the laws of physics, into one framework. In his later years he stopped opposing the quantum  and tried to incorporate it, along with light and gravity, into a larger unified field . Gradually Einstein became set in his ways. He rarely traveled far, confining himself to long walks around Princeton with close associates, whom he engaged in deep conversations about politics, religion, physics, and his unified field . In 1950 he published an article on his  in Scientific American, but because it neglected the still-mysterious strong force, it was necessarily incomplete. When he died five years later of an aortic aneurysm, it was still unfinished.

Legacy of Albert Einstein
Albert Einstein
1 of 2
Albert Einstein
Michio Kaku
2 of 2
Michio KakuMichio Kaku, author of Encycloæpdia Britannica's biography on Albert Einstein.
In some sense, Einstein, instead of being a relic, may have been too far ahead of his time. The strong force, a major piece of any unified field , was still a total mystery in Einstein’s lifetime. Only in the 1970s and ’80s did physicists begin to unravel the secret of the strong force with the quark model. Nevertheless, Einstein’s work continues to win Nobel Prizes for succeeding physicists. In 1993 a Nobel Prize was awarded to the discoverers of gravitation waves, predicted by Einstein. In 1995 a Nobel Prize was awarded to the discoverers of Bose-Einstein condensates (a new form of matter that can occur at extremely low temperatures). Known black holes now number in the thousands. New generations of space satellites have continued to verify the cosmology of Einstein. And many leading physicists are trying to finish Einstein’s ultimate dream of a “ of everything.”

Einstein wrote the space-time entry for the 13th edition of the Encyclopædia Britannica. (See the Britannica Classic: Space-Time.)

Michio Kaku
Britannica AI Icon
Ask Anything
Homework Help
Science
Physics
Matter & Energy
CITE

special relativity
physics
Also known as: special 
 
Britannica Editors  Dec. 17, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Homework Help
special relativity, part of the wide-ranging physical  of relativity formed by the German-born physicist Albert Einstein. It was conceived by Einstein in 1905. Along with quantum mechanics, relativity is central to modern physics.

Key People: Albert Einstein Arthur Eddington Tomonaga Shin’ichirō
Related Topics: relativity quantum field  E = mc2 Einstein’s mass-energy relation
On the Web: Physics LibreTexts - Implications of Special Relativity (Dec. 17, 2025)
relativity of simultaneity
relativity of simultaneityBrian Greene derives the time discrepancy between two people who are moving relative to one another but who disagree on what happens at the same time. This video is an episode in his Daily Equation series.
See all videos for this article
Special relativity is limited to objects that are moving with respect to inertial frames of reference—i.e., in a state of uniform motion with respect to one another such that one cannot, by purely mechanical experiments, distinguish one from the other. Beginning with the behaviour of light (and all other electromagnetic radiation), the  of special relativity draws conclusions that are contrary to everyday experience but fully confirmed by experiments that examine subatomic particles at high speeds or measure small changes between clocks traveling at different speeds. Special relativity revealed that the speed of light is a limit that can be approached but not reached by any material object. It is the origin of the most famous equation in science, E = mc2, which expresses the fact that mass and energy are the same physical entity and can be changed into each other. (For a more-detailed treatment of special relativity, see relativity: Special relativity.)

Keep Learning
How does E=mc² work and why is it important?
What is time dilation and how does it affect astronauts in space?
How did Einstein come up with the  of special relativity?
The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Barbara A. Schreiber.
Britannica AI Icon
Ask Anything
Homework Help
Science
Physics
Matter & Energy
CITE

general relativity
physics
Also known as: general  of relativity
 
Britannica Editors  Dec. 18, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Homework Help
View a demonstration to understand Albert Einstein's general  of relativity
View a demonstration to understand Albert Einstein's general  of relativityExplore general relativity.
See all videos for this article
general relativity, part of the wide-ranging physical  of relativity formed by the German-born physicist Albert Einstein. It was conceived by Einstein in 1916. General relativity is concerned with gravity, one of the fundamental forces in the universe. Gravity defines macroscopic behaviour, and so general relativity describes large-scale physical phenomena.

Key People: Albert Einstein Arthur Eddington Rainer Weiss Kip Thorne Robert H. Dicke
Related Topics: gravitational wave black hole How Albert Einstein Developed the  of General Relativity The Solar Eclipse That Made Albert Einstein a Science Celebrity 100 Years of General Relativity
On the Web: Physics LibreTexts - General  of Relativity (Dec. 18, 2025)
General relativity follows from Einstein’s principle of equivalence: on a local scale it is impossible to distinguish between physical effects due to gravity and those due to acceleration. Gravity is treated as a magnetic phenomenon that arises from the curvature of space-time. The solution of the field equations that describe general relativity can yield answers to different physical situations, such as planetary dynamics, the birth and death of stars, black holes, and the evolution of the universe. General relativity has been experimentally verified by observations of gravitational lenses, the orbit of the planet Mercury, the dilation of time in Earth’s gravitational field, and gravitational waves from merging black holes. (For a more detailed treatment of general relativity, see relativity: General relativity.)

Keep Learning
How did Einstein discover general relativity?
What are black holes and how do they relate to general relativity?
What are gravitational waves and how were they detected?
This article was most recently revised and updated by Erik Gregersen.
Britannica AI Icon
Ask Anything
Quick Summary
Homework Help
Science
Astronomy
CITE

black hole in M87
black hole in M87 Black hole at the center of the massive galaxy M87, about 55 million light-years from Earth, as imaged by the Event Horizon Telescope (EHT). The black hole is 6.5 billion times more massive than the Sun. This picture was the first direct visual evidence of a supermassive black hole and its shadow. The ring is brighter on one side because the black hole is rotating, and thus material on the side of the black hole turning toward Earth has its emission boosted by the Doppler effect. The shadow of the black hole is about five and a half times larger than the event horizon, the boundary marking the black hole's limits, where the escape velocity is equal to the speed of light. Created from data collected in 2017, this picture was released in 2019.
black hole
astronomy
 
Britannica Editors  History
Britannica AI Icon
Britannica AI
Ask Anything
Quick Summary
Homework Help
Top Questions
What is a black hole?
What is the structure of a black hole?
How is a black hole formed?
News • A strange star near a black hole is defying expectations • Dec. 25, 2025, 11:50 PM ET (ScienceDaily) 
black hole
black holeArtist's rendering of matter swirling around a black hole.
black hole, cosmic body of extremely intense gravity from which nothing, not even light, can escape. A black hole can be formed by the death of a massive star. When such a star has exhausted the internal thermonuclear fuels in its core at the end of its life, the core becomes unstable and gravitationally collapses inward upon itself, and the star’s outer layers are blown away. The crushing weight of constituent matter falling in from all sides compresses the dying star to a point of zero volume and infinite density called the singularity.

NASA animation: sizing up the biggest black holes1 of 2
NASA animation: sizing up the biggest black holesThis animation shows 10 supersized black holes that occupy center stage in their host galaxies, including the Milky Way and M87, scaled by the sizes of their shadows.
See all videos for this article
Uncover insight into the black hole2 of 2
Uncover insight into the black holeBlack holes are formed when massive stars die. The intense gravitational force that they exert allows nothing to escape.
See all videos for this article
Details of the structure of a black hole are calculated from Albert Einstein’s general  of relativity. The singularity constitutes the centre of a black hole and is hidden by the object’s “surface,” the event horizon. Inside the event horizon the escape velocity (i.e., the velocity required for matter to escape from the gravitational field of a cosmic object) exceeds the speed of light, so that not even rays of light can escape into space. The radius of the event horizon is called the Schwarzschild radius, after the German astronomer Karl Schwarzschild, who in 1916 predicted the existence of collapsed stellar bodies that emit no radiation. The size of the Schwarzschild radius is proportional to the mass of the collapsing star. For a black hole with a mass 10 times as great as that of the Sun, the radius would be 30 km (18.6 miles).

Only the most massive stars—those of more than three solar masses—become black holes at the end of their lives. Stars with a smaller amount of mass evolve into less compressed bodies, either white dwarfs or neutron stars.

View of the Andromeda Galaxy (Messier 31, M31).
Britannica Quiz
Astronomy and Space Quiz
Black holes usually cannot be observed directly on account of both their small size and the fact that they emit no light. They can be “observed,” however, by the effects of their enormous gravitational fields on nearby matter. For example, if a black hole is a member of a binary star system, matter flowing into it from its companion becomes intensely heated and then radiates X-rays copiously before entering the event horizon of the black hole and disappearing forever. One of the component stars of the binary X-ray system Cygnus X-1 is a black hole. Discovered in 1971 in the constellation Cygnus, this binary consists of a blue supergiant and an invisible companion 14.8 times the mass of the Sun that revolve about one another in a period of 5.6 days.

dust disk around black hole in NGC 4261
dust disk around black hole in NGC 4261Hubble Space Telescope image of an 800-light-year-wide spiral-shaped disk of dust fueling a massive black hole in the centre of galaxy NGC 4261, located 100 million light-years away in the direction of the constellation Virgo.
Some black holes apparently have nonstellar origins. Various astronomers have speculated that large volumes of interstellar gas collect and collapse into supermassive black holes at the centres of quasars and galaxies. A mass of gas falling rapidly into a black hole is estimated to give off more than 100 times as much energy as is released by the identical amount of mass through nuclear fusion. Accordingly, the collapse of millions or billions of solar masses of interstellar gas under gravitational force into a large black hole would account for the enormous energy output of quasars and certain galactic systems.

One such supermassive black hole, Sagittarius A*, exists at the centre of the Milky Way Galaxy. Observations of stars orbiting the position of Sagittarius A* demonstrate the presence of a black hole with a mass equivalent to more than 4,000,000 Suns. (For these observations, American astronomer Andrea Ghez and German astronomer Reinhard Genzel were awarded the 2020 Nobel Prize for Physics.) Supermassive black holes have been detected in other galaxies as well. In 2017 the Event Horizon Telescope obtained an image of the supermassive black hole at the centre of the M87 galaxy. That black hole has a mass equal to six and a half billion Suns but is only 38 billion km (24 billion miles) across. It was the first black hole to be imaged directly. The existence of even larger black holes, each with a mass equal to 10 billion Suns, can be inferred from the energetic effects on gas swirling at extremely high velocities around the centre of NGC 3842 and NGC 4889, galaxies near the Milky Way.

Key People: Kip Thorne Stephen Hawking Subrahmanyan Chandrasekhar Karl Schwarzschild Roger Penrose
Related Topics: accretion disk Penrose diagram Kerr black hole mini black hole singularity
The existence of another kind of nonstellar black hole was proposed by the British astrophysicist Stephen Hawking. According to Hawking’s , numerous tiny primordial black holes, possibly with a mass equal to or less than that of an asteroid, might have been created during the big bang, a state of extremely high temperatures and density in which the universe originated 13.8 billion years ago. These so-called mini black holes, like the more massive variety, lose mass over time through Hawking radiation and disappear. If certain theories of the universe that require extra dimensions are correct, the Large Hadron Collider could produce significant numbers of mini black holes.


Access for the whole family!
Bundle Britannica Premium and Kids for the ultimate resource destination.
The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Encyclopaedia Britannica.
gravitational wave
Introduction
 and sources
Detectors and observations
References & Edit History
Quick Facts & Related Topics
Images & Videos
Laser Interferometer Space Antenna (LISA)
Know about the discovery of gravitational waves and their role in unlocking secrets of the universe
What are gravitational waves and how were they first detected?
How are gravitational waves applied in science and in everyday life?
For Students
default image
gravitational radiation summary
Quizzes
Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Physics and Natural Law
Related Questions
What is Albert Einstein known for?
What influence did Albert Einstein have on science?
What was Albert Einstein’s family like?
What did Albert Einstein mean when he wrote that God “does not play dice”?
Britannica AI Icon
Ask Anything
Homework Help
Science
Physics
Matter & Energy

CITE

Laser Interferometer Space Antenna (LISA)
Laser Interferometer Space Antenna (LISA) Laser Interferometer Space Antenna (LISA), a Beyond Einstein Great Observatory, is scheduled for launch in 2035. Funded by the European Space Agency, LISA will consist of three identical spacecraft that will trail the Earth in its orbit by about 50 million km (30 million miles). The spacecraft will contain thrusters for maneuvering them into an equilateral triangle, with sides of approximately 5 million km (3 million miles), such that the triangle's center will be located along the Earth's orbit. By measuring the transmission of laser signals between the spacecraft (essentially a giant Michelson interferometer in space), scientists hope to detect and accurately measure gravity waves.
gravitational wave
physics
Also known as: gravitational radiation, gravity wave
 
Britannica Editors  History
Britannica AI Icon
Britannica AI
Ask Anything
Homework Help
Top Questions
What is a gravitational wave?
How are gravitational waves created?
How do scientists detect gravitational waves?
What kinds of cosmic events produce strong gravitational waves?
Know about the discovery of gravitational waves and their role in unlocking secrets of the universe
Know about the discovery of gravitational waves and their role in unlocking secrets of the universeAn overview of gravitational waves, including the announcement of their discovery.
See all videos for this article
gravitational wave, the transmission of variations in the gravitational field as waves. According to general relativity, the curvature of space-time is determined by the distribution of masses, while the motion of masses is determined by the curvature. In consequence, variations of the gravitational field are transmitted from place to place as waves, just as variations of an electromagnetic field travel as waves. If the masses that are the source of a field change with time, they radiate energy as waves of curvature of the field. Gravitational waves were first directly detected by the Laser Interferometer Gravitational-Wave Observatory (LIGO) in 2015.

 and sources
Superficially, there are many similarities between gravity and electromagnetism. For example, Newton’s law for the gravitational force between two point masses and Coulomb’s law for the electric force between two point charges indicate that both forces vary as the inverse square of the separation distance. Yet in Scottish physicist James Clerk Maxwell’s  for electromagnetism, accelerated charges emit signals (electromagnetic radiation) that travel at the speed of light, whereas in Newton’s  of gravitation accelerated masses transmit information (action at a distance) that travels at infinite speed. This dichotomy is repaired by Einstein’s  of gravitation, wherein accelerated masses also produce signals (gravitational waves) that travel only at the speed of light. And, just as electromagnetic waves can make their presence known by the pushing to and fro of electrically charged bodies, so too can gravitational waves be detected by the tugging to and fro of massive bodies. However, because the coupling of gravitational forces to masses is intrinsically much weaker than the coupling of electromagnetic forces to charges, the generation and detection of gravitational radiation are much more difficult than those of electromagnetic radiation. Indeed, it was nearly 100 years after Einstein’s discovery of general relativity in 1916 that there was a direct detection of gravitational waves.

Nevertheless, there were strong grounds for believing that such radiation existed. The most convincing concerned radio-timing observations of a pulsar, PSR 1913+16, located in a binary star system with an orbital period of 7.75 hours. This object, discovered in 1974, has a pulse period of about 59 milliseconds that varies by about one part in 1,000 every 7.75 hours. Interpreted as Doppler shifts, these variations imply orbital velocities on the order of 1/1,000 the speed of light. The non-sinusoidal shape of the velocity curve with time allows a deduction that the orbit is quite noncircular (indeed, it is an ellipse of eccentricity 0.62 whose long axis precesses in space by 4.2° per year). The system is composed of two neutron stars, each having a mass of about 1.4 solar masses, with a semimajor axis separation of only 2.8 solar radii. According to Einstein’s  of general relativity, such a system ought to be losing orbital energy through the radiation of gravitational waves at a rate that would cause them to spiral together on a timescale of about 3 × 108 years. The observed decrease in the orbital period in the years since the discovery of the binary pulsar does indeed indicate that the two stars are spiraling toward one another at exactly the predicted rate. Gravitational radiation is the only known means by which that could happen. (American physicists Russell Hulse and Joseph H. Taylor, Jr., won the Nobel Prize for Physics in 1993 for their discovery of PSR 1913+16.)

Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Britannica Quiz
Physics and Natural Law
The implosion of the core of a massive star to form a neutron star prior to a supernova explosion, if it takes place in a nonspherically symmetric way, ought to provide a powerful burst of gravitational radiation. Simple estimates yield the release of a fraction of the mass-energy deficit, roughly 1053 ergs, with the radiation primarily coming out at wave periods between the vibrational period of the neutron star, approximately 0.3 millisecond, and the gravitational radiation damping time, about 300 milliseconds.

Detectors and observations
Three types of detectors have been designed to look for gravitational radiation, which is very weak. The changes of curvature of space-time would correspond to a dilation in one direction and a contraction at right angles to that direction. One scheme, first tried out about 1960, employed a massive cylinder that might be set in mechanical oscillation by a gravitational signal. The authors of this apparatus argued that signals had been detected, but their claim was not substantiated.

In a second scheme an optical interferometer is set up with freely suspended reflectors at the ends of long paths that are at right angles to each other. Shifts of interference fringes corresponding to an increase in length of one arm and a decrease in the other would indicate the passage of gravitational waves. One such interferometer is the Laser Interferometer Gravitational-Wave Observatory (LIGO), which consists of two interferometers with arm lengths of 4 km (2 miles), one in Hanford, Washington, and the other in Livingston, Louisiana. LIGO was the first observatory to directly detect gravitational waves. On September 14, 2015, it observed two black holes 1.3 billion light-years away, which were 36 and 29 times the mass of the Sun, spiralling inward to form a new black hole of 62 solar masses. The remaining three solar masses were converted into gravitational waves.

Also called: gravity wave and gravitational radiation
Key People: Albert Einstein Rainer Weiss Kip Thorne Barry C. Barish Russell Alan Hulse
Related Topics: gravity 6 Amazing Facts About Gravitational Waves and LIGO multimessenger astronomy general relativity gravitation
A third scheme, the Evolved Laser Interferometer Space Antenna (eLISA), is planned that uses three separate, but not independent, interferometers installed in three spacecraft located at the corners of a triangle with sides of some 5 million km (3 million miles). A mission to test the technology for eLISA, LISA Pathfinder, was launched in 2015.


Access for the whole family!
Bundle Britannica Premium and Kids for the ultimate resource destination.
This article was most recently revised and updated by Robert Lewis.
quantum mechanics
Introduction
Historical basis of quantum 
Basic concepts and methods
The interpretation of quantum mechanics
Applications of quantum mechanics
References & Edit History
Related Topics
Images & Videos
photoelectric effect: Einstein's Nobel Prize-winning discovery
Know about quantum tunneling and how it affects the way a particle behaves toward its barriertunnelingmagnet in Stern-Gerlach experimentmeasurement of angular momentum componentslight passing through a slitdouble-slit experiment
Understand why quantum mechanics does not make absolute predictions but only predicts the different outcomes to happen
Learn about Niels Bohr and the difference of opinion between Bohr and Albert Einstein on quantum mechanics
Know about the element of uncertainty of nature in Niels Bohr's interpretation of quantum  and its success despite Albert Einstein's objections
For Students
default image
quantum mechanics summary
Quizzes
Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Physics and Natural Law
Italian physicist Guglielmo Marconi at work in the wireless room of his yacht Electra, c. 1920.
All About Physics Quiz
Related Questions
What is Richard Feynman famous for?
What did Werner Heisenberg do during World War II?
What is Werner Heisenberg best known for?
How did Werner Heisenberg contribute to atomic ?
Britannica AI Icon
Ask Anything
Quick Summary
Homework Help
Science
Physics

CITE

quantum mechanics
physics
 
Gordon Leslie Squires  
Britannica Editors  Dec. 18, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Quick Summary
Homework Help
Top Questions
What is quantum mechanics?
How is quantum mechanics different from classical physics?
What are atoms and subatomic particles?
What is meant by 'quantization' in quantum mechanics?
quantum mechanics, science dealing with the behaviour of matter and light on the atomic and subatomic scale. It attempts to describe and account for the properties of molecules and atoms and their constituents—electrons, protons, neutrons, and other more esoteric particles such as quarks and gluons. These properties include the interactions of the particles with one another and with electromagnetic radiation (i.e., light, X-rays, and gamma rays).

The behaviour of matter and radiation on the atomic scale often seems peculiar, and the consequences of quantum  are accordingly difficult to understand and to believe. Its concepts frequently conflict with common-sense notions derived from observations of the everyday world. There is no reason, however, why the behaviour of the atomic world should conform to that of the familiar, large-scale world. It is important to realize that quantum mechanics is a branch of physics and that the business of physics is to describe and account for the way the world—on both the large and the small scale—actually is and not how one imagines it or would like it to be.

The study of quantum mechanics is rewarding for several reasons. First, it illustrates the essential methodology of physics. Second, it has been enormously successful in giving correct results in practically every situation to which it has been applied. There is, however, an intriguing paradox. In spite of the overwhelming practical success of quantum mechanics, the foundations of the subject contain unresolved problems—in particular, problems concerning the nature of measurement. An essential feature of quantum mechanics is that it is generally impossible, even in principle, to measure a system without disturbing it; the detailed nature of this disturbance and the exact point at which it occurs are obscure and controversial. Thus, quantum mechanics attracted some of the ablest scientists of the 20th century, and they erected what is perhaps the finest intellectual edifice of the period.

Historical basis of quantum 
Basic considerations
At a fundamental level, both radiation and matter have characteristics of particles and waves. The gradual recognition by scientists that radiation has particle-like properties and that matter has wavelike properties provided the impetus for the development of quantum mechanics. Influenced by Newton, most physicists of the 18th century believed that light consisted of particles, which they called corpuscles. From about 1800, evidence began to accumulate for a wave  of light. At about this time Thomas Young showed that, if monochromatic light passes through a pair of slits, the two emerging beams interfere, so that a fringe pattern of alternately bright and dark bands appears on a screen. The bands are readily explained by a wave  of light. According to the , a bright band is produced when the crests (and troughs) of the waves from the two slits arrive together at the screen; a dark band is produced when the crest of one wave arrives at the same time as the trough of the other, and the effects of the two light beams cancel. Beginning in 1815, a series of experiments by Augustin-Jean Fresnel of France and others showed that, when a parallel beam of light passes through a single slit, the emerging beam is no longer parallel but starts to diverge; this phenomenon is known as diffraction. Given the wavelength of the light and the geometry of the apparatus (i.e., the separation and widths of the slits and the distance from the slits to the screen), one can use the wave  to calculate the expected pattern in each case; the  agrees precisely with the experimental data.

Early developments
Planck’s radiation law
By the end of the 19th century, physicists almost universally accepted the wave  of light. However, though the ideas of classical physics explain interference and diffraction phenomena relating to the propagation of light, they do not account for the absorption and emission of light. All bodies radiate electromagnetic energy as heat; in fact, a body emits radiation at all wavelengths. The energy radiated at different wavelengths is a maximum at a wavelength that depends on the temperature of the body; the hotter the body, the shorter the wavelength for maximum radiation. Attempts to calculate the energy distribution for the radiation from a blackbody using classical ideas were unsuccessful. (A blackbody is a hypothetical ideal body or surface that absorbs and reemits all radiant energy falling on it.) One formula, proposed by Wilhelm Wien of Germany, did not agree with observations at long wavelengths, and another, proposed by Lord Rayleigh (John William Strutt) of England, disagreed with those at short wavelengths.

Key People: Werner Heisenberg John von Neumann P.A.M. Dirac Richard Feynman Pascual Jordan
Related Topics: quantum field  matrix mechanics transformation  many-worlds interpretation Copenhagen interpretation
On the Web: CORE - Riemann hypothesis and Quantum Mechanics (PDF) (Dec. 18, 2025)
Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Britannica Quiz
Physics and Natural Law
In 1900 the German measured physicist Max Planck made a bold suggestion. He assumed that the radiation energy is emitted, not continuously, but rather in discrete packets called quanta. The energy E of the quantum is related to the frequency ν by E = hν. The quantity h, now known as Planck’s constant, is a universal constant with the approximate value of 6.62607 × 10−34 joule∙second. Planck showed that the calculated energy spectrum then agreed with observation over the entire wavelength range.

Einstein and the photoelectric effect
photoelectric effect: Einstein's Nobel Prize-winning discovery
photoelectric effect: Einstein's Nobel Prize-winning discoveryBrian Greene discusses the key formula in the photoelectric effect, an insight that helped launch the quantum revolution. This video is an episode in his Daily Equation series.
See all videos for this article
In 1905 Einstein extended Planck’s hypothesis to explain the photoelectric effect, which is the emission of electrons by a metal surface when it is irradiated by light or more-energetic photons. The kinetic energy of the emitted electrons depends on the frequency ν of the radiation, not on its intensity; for a given metal, there is a threshold frequency ν0 below which no electrons are emitted. Furthermore, emission takes place as soon as the light shines on the surface; there is no detectable delay. Einstein showed that these results can be explained by two assumptions: (1) that light is composed of corpuscles or photons, the energy of which is given by Planck’s relationship, and (2) that an atom in the metal can absorb either a whole photon or nothing. Part of the energy of the absorbed photon frees an electron, which requires a fixed energy W, known as the work function of the metal; the rest is converted into the kinetic energy meu2/2 of the emitted electron (me is the mass of the electron and u is its velocity). Thus, the energy relation is
special composition for article "Quantum Mechanics"
If ν is less than ν0, where hν0 = W, no electrons are emitted. Not all the experimental results mentioned above were known in 1905, but all Einstein’s predictions have been verified since.

Bohr’s  of the atom
A major contribution to the subject was made by Niels Bohr of Denmark, who applied the quantum hypothesis to atomic spectra in 1913. The spectra of light emitted by gaseous atoms had been studied extensively since the mid-19th century. It was found that radiation from gaseous atoms at low pressure consists of a set of discrete wavelengths. This is quite unlike the radiation from a solid, which is distributed over a continuous range of wavelengths. The set of discrete wavelengths from gaseous atoms is known as a line spectrum, because the radiation (light) emitted consists of a series of sharp lines. The wavelengths of the lines are characteristic of the element and may form extremely complex patterns. The simplest spectra are those of atomic hydrogen and the alkali atoms (e.g., lithium, sodium, and potassium). For hydrogen, the wavelengths λ are given by the empirical formula
special composition for article "Quantum Mechanics"
where m and n are positive integers with n > m and R∞, known as the Rydberg constant, has the value 1.097373157 × 107 per metre. For a given value of m, the lines for varying n form a series. The lines for m = 1, the Lyman series, lie in the ultraviolet part of the spectrum; those for m = 2, the Balmer series, lie in the visible spectrum; and those for m = 3, the Paschen series, lie in the infrared.

Bohr started with a model suggested by the New Zealand-born British physicist Ernest Rutherford. The model was based on the experiments of Hans Geiger and Ernest Marsden, who in 1909 bombarded gold atoms with massive, fast-moving alpha particles; when some of these particles were deflected backward, Rutherford concluded that the atom has a massive, charged nucleus. In Rutherford’s model, the atom resembles a miniature solar system with the nucleus acting as the Sun and the electrons as the circulating planets. Bohr made three assumptions. First, he postulated that, in contrast to classical mechanics, where an infinite number of orbits is possible, an electron can be in only one of a discrete set of orbits, which he termed stationary states. Second, he postulated that the only orbits allowed are those for which the angular momentum of the electron is a whole number n times ℏ (ℏ = h/2π). Third, Bohr assumed that Newton’s laws of motion, so successful in calculating the paths of the planets around the Sun, also applied to electrons orbiting the nucleus. The force on the electron (the analogue of the gravitational force between the Sun and a planet) is the electrostatic attraction between the positively charged nucleus and the negatively charged electron. With these simple assumptions, he showed that the energy of the orbit has the form
special composition for article "Quantum Mechanics"
where E0 is a constant that may be expressed by a combination of the known constants e, me, and ℏ. While in a stationary state, the atom does not give off energy as light; however, when an electron makes a transition from a state with energy En to one with lower energy Em, a quantum of energy is radiated with frequency ν, given by the equation
special composition for article "Quantum Mechanics"
Inserting the expression for En into this equation and using the relation λν = c, where c is the speed of light, Bohr derived the formula for the wavelengths of the lines in the hydrogen spectrum, with the correct value of the Rydberg constant.

Bohr’s  was a brilliant step forward. Its two most important features have survived in present-day quantum mechanics. They are (1) the existence of stationary, nonradiating states and (2) the relationship of radiation frequency to the energy difference between the initial and final states in a transition. Prior to Bohr, physicists had thought that the radiation frequency would be the same as the electron’s frequency of rotation in an orbit.

Scattering of X-rays
Soon scientists were faced with the fact that another form of radiation, X-rays, also exhibits both wave and particle properties. Max von Laue of Germany had shown in 1912 that crystals can be used as three-dimensional diffraction gratings for X-rays; his technique constituted the fundamental evidence for the wavelike nature of X-rays. The atoms of a crystal, which are arranged in a regular vacuum, scatter the X-rays. For certain directions of scattering, all the crests of the X-rays coincide. (The scattered X-rays are said to be in phase and to give constructive interference.) For these directions, the scattered X-ray beam is very intense. Clearly, this phenomenon demonstrates wave behaviour. In fact, given the interatomic distances in the crystal and the directions of constructive interference, the wavelength of the waves can be calculated.

In 1922 the American physicist Arthur Holly Compton showed that X-rays scatter from electrons as if they are particles. Compton performed a series of experiments on the scattering of monochromatic, high-energy X-rays by graphite. He found that part of the scattered radiation had the same wavelength λ0 as the incident X-rays but that there was an additional component with a longer wavelength λ. To interpret his results, Compton regarded the X-ray photon as a particle that collides and bounces off an electron in the graphite target as though the photon and the electron were a pair of (dissimilar) billiard balls. Application of the laws of conservation of energy and momentum to the collision leads to a specific relation between the amount of energy transferred to the electron and the angle of scattering. For X-rays scattered through an angle θ, the wavelengths λ and λ0 are related by the equation
special composition for article "Quantum Mechanics"
The experimental correctness of Compton’s formula is direct evidence for the corpuscular behaviour of radiation.

De Broglie’s wave hypothesis
Faced with evidence that electromagnetic radiation has both particle and wave characteristics, Louis-Victor de Broglie of France suggested a great unifying hypothesis in 1924. De Broglie proposed that matter has wave as well as particle properties. He suggested that material particles can behave as waves and that their wavelength λ is related to the linear momentum p of the particle by λ = h/p.

In 1927 Clinton Davisson and Lester Germer of the United States confirmed de Broglie’s hypothesis for electrons. Using a crystal of nickel, they diffracted a beam of monoenergetic electrons and showed that the wavelength of the waves is related to the momentum of the electrons by the de Broglie equation. Since Davisson and Germer’s investigation, similar experiments have been performed with atoms, molecules, neutrons, protons, and many other particles. All behave like waves with the same wavelength-momentum relationship.

Basic concepts and methods
Bohr’s , which assumed that electrons moved in circular orbits, was extended by the German physicist Arnold Sommerfeld and others to include elliptic orbits and other refinements. Attempts were made to apply the  to more complicated systems than the hydrogen atom. However, the ad hoc mixture of classical and quantum ideas made the  and calculations increasingly unsatisfactory. Then, in the 12 months started in July 1925, a period of creativity without parallel in the history of physics, there appeared a series of papers by German scientists that set the subject on a firm conceptual foundation. The papers took two approaches: (1) matrix mechanics, proposed by Werner Heisenberg, Max Born, and Pascual Jordan, and (2) wave mechanics, put forward by Erwin Schrödinger. The protagonists were not always polite to each other. Heisenberg found the physical ideas of Schrödinger’s  “disgusting,” and Schrödinger was “discouraged and repelled” by the lack of visualization in Heisenberg’s method. However, Schrödinger, not allowing his emotions to interfere with his scientific endeavours, showed that, in spite of apparent dissimilarities, the two theories are equivalent mathematically. The present discussion follows Schrödinger’s wave mechanics because it is less abstract and easier to understand than Heisenberg’s matrix mechanics.

Schrödinger’s wave mechanics
Schrödinger expressed de Broglie’s hypothesis concerning the wave behaviour of matter in a mathematical form that is adaptable to a variety of physical problems without additional arbitrary assumptions. He was guided by a mathematical formulation of optics, in which the straight-line propagation of light rays can be derived from wave motion when the wavelength is small compared to the dimensions of the apparatus employed. In the same way, Schrödinger set out to find a wave equation for matter that would give particle-like propagation when the wavelength becomes comparatively small. According to classical mechanics, if a particle of mass me is subjected to a force such that its potential energy is V(x, y, z) at position x, y, z, then the sum of V(x, y, z) and the kinetic energy p2/2me is equal to a constant, the total energy E of the particle. Thus,
special composition for article "Quantum Mechanics"

It is assumed that the particle is bound—i.e., confined by the potential to a certain region in space because its energy E is insufficient for it to escape. Since the potential varies with position, two other quantities do also: the momentum and, hence, by extension from the de Broglie relation, the wavelength of the wave. Postulating a wave function Ψ(x, y, z) that varies with position, Schrödinger replaced p in the above energy equation with a differential operator that embodied the de Broglie relation. He then showed that Ψ satisfies the partial differential equation
special composition for article "Quantum Mechanics"

This is the (time-independent) Schrödinger wave equation, which established quantum mechanics in a widely applicable form. An important advantage of Schrödinger’s  is that no further arbitrary quantum conditions need be postulated. The required quantum results follow from certain reasonable restrictions placed on the wave function—for example, that it should not become infinitely large at large distances from the centre of the potential.

Schrödinger applied his equation to the hydrogen atom, for which the potential function, given by classical electrostatics, is proportional to −e2/r, where −e is the charge on the electron. The nucleus (a proton of charge e) is situated at the origin, and r is the distance from the origin to the position of the electron. Schrödinger solved the equation for this particular potential with straightforward, though not elementary, mathematics. Only certain discrete values of E lead to acceptable functions Ψ. These functions are characterized by a trio of integers n, l, m, termed quantum numbers. The values of E depend only on the integers n (1, 2, 3, etc.) and are identical with those given by the Bohr . The quantum numbers l and m are related to the angular momentum of the electron; Square root of√l(l + 1)ℏ is the magnitude of the angular momentum, and mℏ is its component along some physical direction.

The square of the wave function, Ψ2, has a physical interpretation. Schrödinger originally supposed that the electron was spread out in space and that its density at point x, y, z was given by the value of Ψ2 at that point. Almost immediately Born proposed what is now the accepted interpretation—namely, that Ψ2 gives the probability of finding the electron at x, y, z. The distinction between the two interpretations is important. If Ψ2 is small at a particular position, the original interpretation implies that a small fraction of an electron will always be detected there. In Born’s interpretation, nothing will be detected there most of the time, but, when something is observed, it will be a whole electron. Thus, the concept of the electron as a point particle moving in a well-defined path around the nucleus is replaced in wave mechanics by clouds that describe the probable locations of electrons in different states.

Italian physicist Guglielmo Marconi at work in the wireless room of his yacht Electra, c. 1920.
Britannica Quiz
All About Physics Quiz
Electron spin and antiparticles
In 1928 the English physicist Paul A.M. Dirac produced a wave equation for the electron that combined relativity with quantum mechanics. Schrödinger’s wave equation does not satisfy the requirements of the special  of relativity because it is based on a nonrelativistic expression for the kinetic energy (p2/2me). Dirac showed that an electron has an additional quantum number ms. Unlike the first three quantum numbers, ms is not a whole integer and can have only the values +1/2 and −1/2. It corresponds to an additional form of angular momentum ascribed to a spinning motion. (The angular momentum mentioned above is due to the orbital motion of the electron, not its spin.) The concept of spin angular momentum was introduced in 1925 by Samuel A. Goudsmit and George E. Uhlenbeck, two graduate students at the University of Leiden, Neth., to explain the magnetic moment measurements made by Otto Stern and Walther Gerlach of Germany several years earlier. The magnetic moment of a particle is closely related to its angular momentum; if the angular momentum is zero, so is the magnetic moment. Yet Stern and Gerlach had observed a magnetic moment for electrons in silver atoms, which were known to have zero orbital angular momentum. Goudsmit and Uhlenbeck proposed that the observed magnetic moment was attributable to spin angular momentum.

The electron-spin hypothesis not only provided an explanation for the observed magnetic moment but also accounted for many other effects in atomic spectroscopy, including changes in spectral lines in the presence of a magnetic field (Zeeman effect), doublet lines in alkali spectra, and fine structure (close doublets and triplets) in the hydrogen spectrum.

The Dirac equation also predicted additional states of the electron that had not yet been observed. Experimental confirmation was provided in 1932 by the discovery of the positron by the American physicist Carl David Anderson. Every particle described by the Dirac equation has to have a corresponding antiparticle, which differs only in charge. The positron is just such an antiparticle of the negatively charged electron, having the same mass as the latter but a positive charge.

Identical particles and multielectron atoms
Because electrons are identical to (i.e., indistinguishable from) each other, the wave function of an atom with more than one electron must satisfy special conditions. The problem of identical particles does not arise in classical physics, where the objects are large-scale and can always be distinguished, at least in principle. There is no way, however, to differentiate two electrons in the same atom, and the form of the wave function must reflect this fact. The overall wave function Ψ of a system of identical particles depends on the coordinates of all the particles. If the coordinates of two of the particles are interchanged, the wave function must remain unaltered or, at most, undergo a change of sign; the change of sign is permitted because it is Ψ2 that occurs in the physical interpretation of the wave function. If the sign of Ψ remains unchanged, the wave function is said to be symmetric with respect to interchange; if the sign changes, the function is antisymmetric.

The symmetry of the wave function for identical particles is closely related to the spin of the particles. In quantum field  (see below Quantum electrodynamics), it can be shown that particles with half-integral spin (1/2, 3/2, etc.) have antisymmetric wave functions. They are called fermions after the Italian-born physicist Enrico Fermi. Examples of fermions are electrons, protons, and neutrons, all of which have spin 1/2. Particles with zero or integral spin (e.g., mesons, photons) have symmetric wave functions and are called bosons after the Indian mathematician and physicist Satyendra Nath Bose, who first applied the ideas of symmetry to photons in 1924–25.

The requirement of antisymmetric wave functions for fermions leads to a fundamental result, known as the exclusion principle, first proposed in 1925 by the Austrian physicist Wolfgang Pauli. The exclusion principle states that two fermions in the same system cannot be in the same quantum state. If they were, interchanging the two sets of coordinates would not change the wave function at all, which contradicts the result that the wave function must change sign. Thus, two electrons in the same atom cannot have an identical set of values for the four quantum numbers n, l, m, ms. The exclusion principle forms the basis of many properties of matter, including the periodic classification of the elements, the nature of chemical bonds, and the behaviour of electrons in solids; the last determines in turn whether a solid is a metal, an insulator, or a semiconductor (see atom; matter).

The Schrödinger equation cannot be solved precisely for atoms with more than one electron. The principles of the calculation are well understood, but the problems are complicated by the number of particles and the variety of forces involved. The forces include the electrostatic forces between the nucleus and the electrons and between the electrons themselves, as well as weaker magnetic forces arising from the spin and orbital motions of the electrons. Despite these difficulties, approximation methods introduced by the English physicist Douglas R. Hartree, the Russian physicist Vladimir Fock, and others in the 1920s and 1930s have achieved considerable success. Such schemes start by assuming that each electron moves independently in an average electric field because of the nucleus and the other electrons; i.e., correlations between the positions of the electrons are ignored. Each electron has its own wave function, called an orbital. The overall wave function for all the electrons in the atom satisfies the exclusion principle. Corrections to the calculated energies are then made, which depend on the strengths of the electron-electron correlations and the magnetic forces.

Time-dependent Schrödinger equation
At the same time that Schrödinger proposed his time-independent equation to describe the stationary states, he also proposed a time-dependent equation to describe how a system changes from one state to another. By replacing the energy E in Schrödinger’s equation with a time-derivative operator, he generalized his wave equation to determine the time variation of the wave function as well as its spatial variation. The time-dependent Schrödinger equation reads
special composition for article "Quantum Mechanics": Schrodinger equation
The quantity i is the square root of −1. The function Ψ varies with time t as well as with position x, y, z. For a system with constant energy, E, Ψ has the form
special composition for article "Quantum Mechanics"
where exp stands for the exponential function, and the time-dependent Schrödinger equation reduces to the time-independent form.

The probability of a transition between one atomic stationary state and some other state can be calculated with the aid of the time-dependent Schrödinger equation. For example, an atom may change spontaneously from one state to another state with less energy, emitting the difference in energy as a photon with a frequency given by the Bohr relation. If electromagnetic radiation is applied to a set of atoms and if the frequency of the radiation matches the energy difference between two stationary states, transitions can be stimulated. In a stimulated transition, the energy of the atom may increase—i.e., the atom may absorb a photon from the radiation—or the energy of the atom may decrease, with the emission of a photon, which adds to the energy of the radiation. Such stimulated emission processes form the basic mechanism for the operation of lasers. The probability of a transition from one state to another depends on the values of the l, m, ms quantum numbers of the initial and final states. For most values, the transition probability is effectively zero. However, for certain changes in the quantum numbers, summarized as selection rules, there is a finite probability. For example, according to one important selection rule, the l value changes by unity because photons have a spin of 1. The selection rules for radiation relate to the angular momentum properties of the stationary states. The absorbed or emitted photon has its own angular momentum, and the selection rules reflect the conservation of angular momentum between the atoms and the radiation.

Tunneling
Know about quantum tunneling and how it affects the way a particle behaves toward its barrier1 of 2
Know about quantum tunneling and how it affects the way a particle behaves toward its barrierLearn how quantum tunneling affects the way a particle reacts to barriers.
See all videos for this article
tunneling
2 of 2
tunnelingFigure 1: Classically, a particle is bound in the central region C if its energy E is less than V0, but in quantum  the particle may tunnel through the potential barrier and escape.
The phenomenon of tunneling, which has no counterpart in classical physics, is an important consequence of quantum mechanics. Consider a particle with energy E in the inner region of a one-dimensional potential well V(x), as shown in Figure 1. (A potential well is a potential that has a lower value in a certain region of space than in the neighbouring regions.) In classical mechanics, if E < V0 (the maximum height of the potential barrier), the particle remains in the well forever; if E > V0, the particle escapes. In quantum mechanics, the situation is not so simple. The particle can escape even if its energy E is below the height of the barrier V0, although the probability of escape is small unless E is close to V0. In that case, the particle may tunnel through the potential barrier and emerge with the same energy E.

The phenomenon of tunneling has many important applications. For example, it describes a type of radioactive decay in which a nucleus emits an alpha particle (a helium nucleus). According to the quantum explanation given independently by George Gamow and by Ronald W. Gurney and Edward Condon in 1928, the alpha particle is confined before the decay by a potential of the shape shown in Figure 1. For a given nuclear species, it is possible to measure the energy E of the emitted alpha particle and the average lifetime τ of the nucleus before decay. The lifetime of the nucleus is a measure of the probability of tunneling through the barrier—the shorter the lifetime, the higher the probability. With plausible assumptions about the general form of the potential function, it is possible to calculate a relationship between τ and E that is applicable to all alpha emitters. This , which is borne out by experiment, shows that the probability of tunneling, and hence the value of τ, is extremely sensitive to the value of E. For all known alpha-particle emitters, the value of E varies from about 2 to 8 million electron volts, or MeV (1 MeV = 106 electron volts). Thus, the value of E varies only by a factor of 4, whereas the range of τ is from about 1011 years down to about 10−6 second, a factor of 1024. It would be difficult to account for this sensitivity of τ to the value of E by any  other than quantum mechanical tunneling.

Axiomatic approach
Although the two Schrödinger equations form an important part of quantum mechanics, it is possible to present the subject in a more general way. Dirac gave an elegant exposition of an axiomatic approach based on observables and states in a classic textbook entitled The Principles of Quantum Mechanics. (The book, published in 1930, is still in print.) An observable is anything that can be measured—energy, position, a component of angular momentum, and so forth. Every observable has a set of states, each state being represented by an algebraic function. With each state is associated a number that gives the result of a measurement of the observable. Consider an observable with N states, denoted by ψ1, ψ2, . . ., ψN, and corresponding measurement values a1, a2, . . ., aN. A physical system—e.g., an atom in a particular state—is represented by a wave function Ψ, which can be expressed as a linear combination, or mixture, of the states of the observable. Thus, the Ψ may be written as
special composition for article "Quantum Mechanics"
For a given Ψ, the quantities c1, c2, etc., are a set of numbers that can be calculated. In general, the numbers are complex, but, in the present discussion, they are assumed to be real numbers.

The  postulates, first, that the result of a measurement must be an a-value—i.e., a1, a2, or a3, etc. No other value is possible. Second, before the measurement is made, the probability of obtaining the value a1 is c12, and that of obtaining the value a2 is c22, and so on. If the value obtained is, say, a5, the  asserts that after the measurement the state of the system is no longer the original Ψ but has changed to ψ5, the state corresponding to a5.

A number of consequences follow from these assertions. First, the result of a measurement cannot be predicted with certainty. Only the probability of a particular result can be predicted, even though the initial state (represented by the function Ψ) is known exactly. Second, identical measurements made on a large number of identical systems, all in the identical state Ψ, will produce different values for the measurements. This is, of course, quite contrary to classical physics and common sense, which say that the same measurement on the same object in the same state must produce the same result. Moreover, according to the , not only does the act of measurement change the state of the system, but it does so in an indeterminate way. Sometimes it changes the state to ψ1, sometimes to ψ2, and so forth.

There is an important exception to the above statements. Suppose that, before the measurement is made, the state Ψ happens to be one of the ψs—say, Ψ = ψ3. Then c3 = 1 and all the other cs are zero. This means that, before the measurement is made, the probability of obtaining the value a3 is unity and the probability of obtaining any other value of a is zero. In other words, in this particular case, the result of the measurement can be predicted with certainty. Moreover, after the measurement is made, the state will be ψ3, the same as it was before. Thus, in this particular case, measurement does not disturb the system. Whatever the initial state of the system, two measurements made in rapid succession (so that the change in the wave function given by the time-dependent Schrödinger equation is negligible) produce the same result.

The value of one observable can be determined by a single measurement. The value of two observables for a given system may be known at the same time, provided that the two observables have the same set of state functions ψ1, ψ2, . . ., ψN. In this case, measuring the first observable results in a state function that is one of the ψs. Because this is also a state function of the second observable, the result of measuring the latter can be predicted with certainty. Thus the values of both observables are known. (Although the ψs are the same for the two observables, the two sets of a values are, in general, different.) The two observables can be measured repeatedly in any sequence. After the first measurement, none of the measurements disturbs the system, and a unique pair of values for the two observables is obtained.

Incompatible observables
The measurement of two observables with different sets of state functions is a quite different situation. Measurement of one observable gives a certain result. The state function after the measurement is, as always, one of the states of that observable; however, it is not a state function for the second observable. Measuring the second observable disturbs the system, and the state of the system is no longer one of the states of the first observable. In general, measuring the first observable again does not produce the same result as the first time. To sum up, both quantities cannot be known at the same time, and the two observables are said to be incompatible.

magnet in Stern-Gerlach experiment
1 of 2
magnet in Stern-Gerlach experimentFigure 2: N and S are the north and south poles of a magnet. The knife-edge of S results in a much stronger magnetic field at the point P than at Q.
measurement of angular momentum components
2 of 2
measurement of angular momentum componentsFigure 3: Measurements of the x and y components of angular momentum for silver atoms, S, in the ground state. A, B, and C are magnets with inhomogeneous magnetic fields. The arrows show the average direction of each magnetic field.
A specific example of this behaviour is the measurement of the component of angular momentum along two mutually perpendicular directions. The Stern-Gerlach experiment mentioned above involved measuring the angular momentum of a silver atom in the ground state. In reconstructing this experiment, a beam of silver atoms is passed between the poles of a magnet. The poles are shaped so that the magnetic field varies greatly in strength over a very small distance (Figure 2). The apparatus determines the ms quantum number, which can be +1/2 or −1/2. No other values are obtained. Thus in this case the observable has only two states—i.e., N = 2. The inhomogeneous magnetic field produces a force on the silver atoms in a direction that depends on the spin state of the atoms. The result is shown schematically in Figure 3. A beam of silver atoms is passed through magnet A. The atoms in the state with ms = +1/2 are deflected upward and emerge as beam 1, while those with ms = −1/2 are deflected downward and emerge as beam 2. If the direction of the magnetic field is the x-axis, the apparatus measures Sx, which is the x-component of spin angular momentum. The atoms in beam 1 have Sx = +ℏ/2 while those in beam 2 have Sx = −ℏ/2. In a classical picture, these two states represent atoms spinning about the direction of the x-axis with opposite senses of rotation.

The y-component of spin angular momentum Sy also can have only the values +ℏ/2 and −ℏ/2; however, the two states of Sy are not the same as for Sx. In fact, each of the states of Sx is an equal mixture of the states for Sy, and conversely. Again, the two Sy states may be pictured as representing atoms with opposite senses of rotation about the y-axis. These classical pictures of quantum states are helpful, but only up to a certain point. For example, quantum  says that each of the states corresponding to spin about the x-axis is a superposition of the two states with spin about the y-axis. There is no way to visualize this; it has absolutely no classical counterpart. One simply has to accept the result as a consequence of the axioms of the . Suppose that, as in Figure 3, the atoms in beam 1 are passed into a second magnet B, which has a magnetic field along the y-axis perpendicular to x. The atoms emerge from B and go in equal numbers through its two output channels. Classical  says that the two magnets together have measured both the x- and y-components of spin angular momentum and that the atoms in beam 3 have Sx = +ℏ/2, Sy = +ℏ/2, while those in beam 4 have Sx = +ℏ/2, Sy = −ℏ/2. However, classical  is wrong, because if beam 3 is put through still another magnet C, with its magnetic field along x, the atoms divide equally into beams 5 and 6 instead of emerging as a single beam 5 (as they would if they had Sx = +ℏ/2). Thus, the correct statement is that the beam entering B has Sx = +ℏ/2 and is composed of an equal mixture of the states Sy = +ℏ/2 and Sy = −ℏ/2—i.e., the x-component of angular momentum is known but the y-component is not. Correspondingly, beam 3 leaving B has Sy = +ℏ/2 and is an equal mixture of the states Sx = +ℏ/2 and Sx = −ℏ/2; the y-component of angular momentum is known but the x-component is not. The information about Sx is lost because of the disturbance caused by magnet B in the measurement of Sy.

Heisenberg uncertainty principle
The observables discussed so far have had discrete sets of experimental values. For example, the values of the energy of a bound system are always discrete, and angular momentum components have values that take the form mℏ, where m is either an integer or a half-integer, positive or negative. On the other hand, the position of a particle or the linear momentum of a free particle can take continuous values in both quantum and classical . The mathematics of observables with a continuous spectrum of measured values is somewhat more complicated than for the discrete case but presents no problems of principle. An observable with a continuous spectrum of measured values has an infinite number of state functions. The state function Ψ of the system is still regarded as a combination of the state functions of the observable, but the sum in equation (10) must be replaced by an integral.

light passing through a slit
light passing through a slitFigure 4: (A) Parallel monochromatic light incident normally on a slit, (B) variation in the intensity of the light with direction after it has passed through the slit. If the experiment is repeated with electrons instead of light, the same diagram would represent the variation in the intensity (i.e., relative number) of the electrons.
Measurements can be made of position x of a particle and the x-component of its linear momentum, denoted by px. These two observables are incompatible because they have different state functions. The phenomenon of diffraction noted above illustrates the impossibility of measuring position and momentum simultaneously and precisely. If a parallel monochromatic light beam passes through a slit (Figure 4A), its intensity varies with direction, as shown in Figure 4B. The light has zero intensity in certain directions. Wave  shows that the first zero occurs at an angle θ0, given by sin θ0 = λ/b, where λ is the wavelength of the light and b is the width of the slit. If the width of the slit is reduced, θ0 increases—i.e., the diffracted light is more spread out. Thus, θ0 measures the spread of the beam.

The experiment can be repeated with a stream of electrons instead of a beam of light. According to de Broglie, electrons have wavelike properties; therefore, the beam of electrons emerging from the slit should widen and spread out like a beam of light waves. This has been observed in experiments. If the electrons have velocity u in the forward direction (i.e., the y-direction in Figure 4A), their (linear) momentum is p = meu. Consider px, the component of momentum in the x-direction. After the electrons have passed through the aperture, the spread in their directions results in an uncertainty in px by an amount
special composition for article "Quantum Mechanics"
where λ is the wavelength of the electrons and, according to the de Broglie formula, equals h/p. Thus, Δpx ≈ h/b. Exactly where an electron passed through the slit is unknown; it is only certain that an electron went through somewhere. Therefore, immediately after an electron goes through, the uncertainty in its x-position is Δx ≈ b/2. Thus, the product of the uncertainties is of the order of ℏ. More exact analysis shows that the product has a lower limit, given by
special composition for article "Quantum Mechanics": Heisenberg uncertainty principle

This is the well-known Heisenberg uncertainty principle for position and momentum. It states that there is a limit to the precision with which the position and the momentum of an object can be measured at the same time. Depending on the experimental conditions, either quantity can be measured as precisely as desired (at least in principle), but the more precisely one of the quantities is measured, the less precisely the other is known.

The uncertainty principle is significant only on the atomic scale because of the small value of h in everyday units. If the position of a macroscopic object with a mass of, say, one gram is measured with a precision of 10−6 metre, the uncertainty principle states that its velocity cannot be measured to better than about 10−25 metre per second. Such a limitation is hardly worrisome. However, if an electron is located in an atom about 10−10 metre across, the principle gives a minimum uncertainty in the velocity of about 106 metre per second.

The above reasoning leading to the uncertainty principle is based on the wave-particle duality of the electron. When Heisenberg first propounded the principle in 1927 his reasoning was based, however, on the wave-particle duality of the photon. He considered the process of measuring the position of an electron by observing it in a microscope. Diffraction effects due to the wave nature of light result in a blurring of the image; the resulting uncertainty in the position of the electron is approximately equal to the wavelength of the light. To reduce this uncertainty, it is necessary to use light of shorter wavelength—e.g., gamma rays. However, in producing an image of the electron, the gamma-ray photon bounces off the electron, giving the Compton effect (see above Early developments: Scattering of X-rays). As a result of the collision, the electron recoils in a statistically random way. The resulting uncertainty in the momentum of the electron is proportional to the momentum of the photon, which is inversely proportional to the wavelength of the photon. So it is again the case that increased precision in knowledge of the position of the electron is gained only at the expense of decreased precision in knowledge of its momentum. A detailed calculation of the process yields the same result as before (equation [12]). Heisenberg’s reasoning brings out clearly the fact that the smaller the particle being observed, the more significant is the uncertainty principle. When a large body is observed, photons still bounce off it and change its momentum, but, considered as a fraction of the initial momentum of the body, the change is insignificant.

The Schrödinger and Dirac theories give a precise value for the energy of each stationary state, but in reality the states do not have a precise energy. The only exception is in the ground (lowest energy) state. Instead, the energies of the states are spread over a small range. The spread arises from the fact that, because the electron can make a transition to another state, the initial state has a finite lifetime. The transition is a random process, and so different atoms in the same state have different lifetimes. If the mean lifetime is denoted as τ, the  shows that the energy of the initial state has a spread of energy ΔE, given by
special composition for article "Quantum Mechanics"

This energy spread is manifested in a spread in the frequencies of emitted radiation. Therefore, the spectral lines are not infinitely sharp. (Some experimental factors can also broaden a line, but their effects can be reduced; however, the present effect, known as natural broadening, is fundamental and cannot be reduced.) Equation (13) is another type of Heisenberg uncertainty relation; generally, if a measurement with duration τ is made of the energy in a system, the measurement disturbs the system, causing the energy to be uncertain by an amount ΔE, the magnitude of which is given by the above equation.

Quantum electrodynamics
The application of quantum  to the interaction between electrons and radiation requires a quantum treatment of Maxwell’s field equations, which are the foundations of electromagnetism, and the relativistic  of the electron formulated by Dirac (see above Electron spin and antiparticles). The resulting quantum field  is known as quantum electrodynamics, or QED.

QED accounts for the behaviour and interactions of electrons, positrons, and photons. It deals with processes involving the creation of material particles from electromagnetic energy and with the converse processes in which a material particle and its antiparticle annihilate each other and produce energy. Initially the  was beset with formidable mathematical difficulties, because the calculated values of quantities such as the charge and mass of the electron proved to be infinite. However, an ingenious set of techniques developed (in the late 1940s) by Hans Bethe, Julian S. Schwinger, Tomonaga Shin’ichirō, Richard P. Feynman, and others dealt systematically with the infinities to obtain finite values of the physical quantities. Their method is known as renormalization. The  has provided some remarkably accurate predictions.

According to the Dirac , two particular states in hydrogen with different quantum numbers have the same energy. QED, however, predicts a small difference in their energies; the difference may be determined by measuring the frequency of the electromagnetic radiation that produces transitions between the two states. This effect was first measured by Willis E. Lamb, Jr., and Robert Retherford in 1947. Its physical origin lies in the interaction of the electron with the random fluctuations in the surrounding electromagnetic field. These fluctuations, which exist even in the absence of an applied field, are a quantum phenomenon. The accuracy of experiment and  in this area may be gauged by two recent values for the separation of the two states, expressed in terms of the frequency of the radiation that produces the transitions:
Comparison of the experimental and measured values for the separation of two states of hydrogen.

An even more spectacular example of the success of QED is provided by the value for μe, the magnetic dipole moment of the free electron. Because the electron is spinning and has electric charge, it behaves like a tiny magnet, the strength of which is expressed by the value of μe. According to the Dirac , μe is exactly equal to μB = eℏ/2me, a quantity known as the Bohr magneton; however, QED predicts that μe = (1 + a)μB, where a is a small number, approximately 1/860. Again, the physical origin of the QED correction is the interaction of the electron with random oscillations in the surrounding electromagnetic field. The best experimental determination of μe involves measuring not the quantity itself but the small correction term μe − μB. This greatly enhances the sensitivity of the experiment. The most recent results for the value of a are
Comparison of the experimental and measured values of the magnetic dipole moment.

Since a itself represents a small correction term, the magnetic dipole moment of the electron is measured with an accuracy of about one part in 1011. One of the most precisely determined quantities in physics, the magnetic dipole moment of the electron can be calculated correctly from quantum  to within about one part in 1010.

The interpretation of quantum mechanics
Although quantum mechanics has been applied to problems in physics with great success, some of its ideas seem strange. A few of their implications are considered here.

The electron: wave or particle?
double-slit experiment
double-slit experimentFigure 5: (A) Monochromatic light incident on a pair of slits gives interference fringes (alternate light and dark bands) on a screen, (B) variation in the intensity of the light at the screen when both slits are open. With a single slit, there is no interference pattern; the intensity variation is shown by the broken line. As with Figure 4B, the same diagram would give the variation in the intensity of electrons in the corresponding electron experiment.
Young’s aforementioned experiment in which a parallel beam of monochromatic light is passed through a pair of narrow parallel slits (Figure 5A) has an electron counterpart. In Young’s original experiment, the intensity of the light varies with direction after passing through the slits (Figure 5B). The intensity oscillates because of interference between the light waves emerging from the two slits, the rate of oscillation depending on the wavelength of the light and the separation of the slits. The oscillation creates a fringe pattern of alternating light and dark bands that is modulated by the diffraction pattern from each slit. If one of the slits is covered, the interference fringes disappear, and only the diffraction pattern (shown as a broken line in Figure 5B) is observed.

Young’s experiment can be repeated with electrons all with the same momentum. The screen in the optical experiment is replaced by a closely spaced vacuum of electron detectors. There are many devices for detecting electrons; the most common are scintillators. When an electron passes through a scintillating material, such as sodium iodide, the material produces a light flash which gives a voltage pulse that can be amplified and recorded. The pattern of electrons recorded by each detector is the same as that predicted for waves with wavelengths given by the de Broglie formula. Thus, the experiment provides conclusive evidence for the wave behaviour of electrons.

If the experiment is repeated with a very weak source of electrons so that only one electron passes through the slits, a single detector registers the arrival of an electron. This is a well-localized event characteristic of a particle. Each time the experiment is repeated, one electron passes through the slits and is detected. A graph plotted with detector position along one axis and the number of electrons along the other looks exactly like the oscillating interference pattern in Figure 5B. Thus, the intensity function in the figure is proportional to the probability of the electron moving in a particular direction after it has passed through the slits. Apart from its units, the function is identical to Ψ2, where Ψ is the solution of the time-independent Schrödinger equation for this particular experiment.

If one of the slits is covered, the fringe pattern disappears and is replaced by the diffraction pattern for a single slit. Thus, both slits are needed to produce the fringe pattern. However, if the electron is a particle, it seems reasonable to suppose that it passed through only one of the slits. The apparatus can be modified to ascertain which slit by placing a thin wire loop around each slit. When an electron passes through a loop, it generates a small electric signal, showing which slit it passed through. However, the interference fringe pattern then disappears, and the single-slit diffraction pattern returns. Since both slits are needed for the interference pattern to appear and since it is impossible to know which slit the electron passed through without destroying that pattern, one is forced to the conclusion that the electron goes through both slits at the same time.

In summary, the experiment shows both the wave and particle properties of the electron. The wave property predicts the probability of direction of travel before the electron is detected; on the other hand, the fact that the electron is detected in a particular place shows that it has particle properties. Therefore, the answer to the question whether the electron is a wave or a particle is that it is neither. It is an object exhibiting either wave or particle properties, depending on the type of measurement that is made on it. In other words, one cannot talk about the intrinsic properties of an electron; instead, one must consider the properties of the electron and measuring apparatus together.

Hidden variables
Understand why quantum mechanics does not make absolute predictions but only predicts the different outcomes to happen1 of 3
Understand why quantum mechanics does not make absolute predictions but only predicts the different outcomes to happenA review of quantum mechanics and its effect on making factual predictions.
See all videos for this article
Learn about Niels Bohr and the difference of opinion between Bohr and Albert Einstein on quantum mechanics2 of 3
Learn about Niels Bohr and the difference of opinion between Bohr and Albert Einstein on quantum mechanicsHear Abraham Pais, Paul Davies, and other authorities discuss Niels Bohr as well as Albert Einstein's objections to Bohr's interpretation of quantum mechanics.
See all videos for this article
Know about the element of uncertainty of nature in Niels Bohr's interpretation of quantum  and its success despite Albert Einstein's objections3 of 3
Know about the element of uncertainty of nature in Niels Bohr's interpretation of quantum  and its success despite Albert Einstein's objectionsLearn about the element of indeterminacy in Niels Bohr's interpretation of quantum mechanics.
See all videos for this article
A fundamental concept in quantum mechanics is that of randomness, or indeterminacy. In general, the  predicts only the probability of a certain result. Consider the case of radioactivity. Imagine a box of atoms with identical nuclei that can undergo decay with the emission of an alpha particle. In a given time interval, a certain fraction will decay. The  may tell precisely what that fraction will be, but it cannot predict which particular nuclei will decay. The  asserts that, at the beginning of the time interval, all the nuclei are in an identical state and that the decay is a completely random process. Even in classical physics, many processes appear random. For example, one says that, when a roulette wheel is spun, the ball will drop at random into one of the numbered compartments in the wheel. Based on this belief, the casino owner and the players give and accept identical odds against each number for each throw. However, the fact is that the winning number could be predicted if one noted the exact location of the wheel when the croupier released the ball, the initial speed of the wheel, and various other physical parameters. It is only ignorance of the initial conditions and the difficulty of doing the calculations that makes the outcome appear to be random. In quantum mechanics, on the other hand, the randomness is asserted to be absolutely fundamental. The  says that, though one nucleus decayed and the other did not, they were previously in the identical state.

Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum 
Understand Albert Einstein's perspective of disagreement about the element of uncertainty of quantum theoryLearn about the element of indeterminacy in Niels Bohr's interpretation of quantum mechanics and about Albert Einstein's objections to indeterminacy.
See all videos for this article
Many eminent physicists, including Einstein, have not accepted this indeterminacy. They have rejected the notion that the nuclei were initially in the identical state. Instead, they postulated that there must be some other property—presently unknown, but existing nonetheless—that is different for the two nuclei. This type of unknown property is termed a hidden variable; if it existed, it would restore determinacy to physics. If the initial values of the hidden variables were known, it would be possible to predict which nuclei would decay. Such a  would, of course, also have to account for the wealth of experimental data which conventional quantum mechanics explains from a few simple assumptions. Attempts have been made by de Broglie, David Bohm, and others to construct theories based on hidden variables, but the theories are very complicated and contrived. For example, the electron would definitely have to go through only one slit in the two-slit experiment. To explain that interference occurs only when the other slit is open, it is necessary to postulate a special force on the electron which exists only when that slit is open. Such artificial additions make hidden variable theories unattractive, and there is little support for them among physicists.

The orthodox view of quantum mechanics—and the one adopted in the present article—is known as the Copenhagen interpretation because its main protagonist, Niels Bohr, worked in that city. The Copenhagen view of understanding the physical world stresses the importance of basing  on what can be observed and measured experimentally. It therefore rejects the idea of hidden variables as quantities that cannot be measured. The Copenhagen view is that the indeterminacy observed in nature is fundamental and does not reflect an inadequacy in present scientific knowledge. One should therefore accept the indeterminacy without trying to “explain” it and see what consequences come from it.

Attempts have been made to link the existence of free will with the indeterminacy of quantum mechanics, but it is difficult to see how this feature of the  makes free will more plausible. On the contrary, free will presumably implies rational thought and decision, whereas the essence of the indeterminism in quantum mechanics is that it is due to intrinsic randomness.

Paradox of Einstein, Podolsky, and Rosen
Know about Nicolas Gisin and his team's experiment to test the Einstein-Podolsky-Rosen paradox
Know about Nicolas Gisin and his team's experiment to test the Einstein-Podolsky-Rosen paradoxLearn how the Einstein-Podolsky-Rosen paradox was put to the test by Nicolas Gisin's group at the University of Geneva, Switzerland.
See all videos for this article
In 1935 Einstein and two other physicists in the United States, Boris Podolsky and Nathan Rosen, analyzed a thought experiment to measure position and momentum in a pair of interacting systems. Employing conventional quantum mechanics, they obtained some startling results, which led them to conclude that the  does not give a complete description of physical reality. Their results, which are so peculiar as to seem paradoxical, are based on impeccable reasoning, but their conclusion that the  is incomplete does not necessarily follow. Bohm simplified their experiment while retaining the central point of their reasoning; this discussion follows his account.

The proton, like the electron, has spin 1/2; thus, no matter what direction is chosen for measuring the component of its spin angular momentum, the values are always +ℏ/2 or −ℏ/2. (The present discussion relates only to spin angular momentum, and the word spin is omitted from now on.) It is possible to obtain a system consisting of a pair of protons in close proximity and with total angular momentum equal to zero. Thus, if the value of one of the components of angular momentum for one of the protons is +ℏ/2 along any selected direction, the value for the component in the same direction for the other particle must be −ℏ/2. Suppose the two protons move in opposite directions until they are far apart. The total angular momentum of the system remains zero, and if the component of angular momentum along the same direction for each of the two particles is measured, the result is a pair of equal and opposite values. Therefore, after the quantity is measured for one of the protons, it can be predicted for the other proton; the second measurement is unnecessary. As previously noted, measuring a quantity changes the state of the system. Thus, if measuring Sx (the x-component of angular momentum) for proton 1 produces the value +ℏ/2, the state of proton 1 after measurement corresponds to Sx = +ℏ/2, and the state of proton 2 corresponds to Sx = −ℏ/2. Any direction, however, can be chosen for measuring the component of angular momentum. Whichever direction is selected, the state of proton 1 after measurement corresponds to a definite component of angular momentum about that direction. Furthermore, since proton 2 must have the opposite value for the same component, it follows that the measurement on proton 1 results in a definite state for proton 2 relative to the chosen direction, notwithstanding the fact that the two particles may be millions of kilometres apart and are not interacting with each other at the time. Einstein and his two collaborators thought that this conclusion was so obviously false that the quantum mechanical  on which it was based must be incomplete. They concluded that the correct  would contain some hidden variable feature that would restore the determinism of classical physics.

A comparison of how quantum  and classical  describe angular momentum for particle pairs illustrates the essential difference between the two outlooks. In both theories, if a system of two particles has a total angular momentum of zero, then the angular momenta of the two particles are equal and opposite. If the components of angular momentum are measured along the same direction, the two values are numerically equal, one positive and the other negative. Thus, if one component is measured, the other can be predicted. The crucial difference between the two theories is that, in classical physics, the system under investigation is assumed to have possessed the quantity being measured beforehand. The measurement does not disturb the system; it merely reveals the preexisting state. It may be noted that, if a particle were actually to possess components of angular momentum prior to measurement, such quantities would constitute hidden variables.

Understand the concept of teleportation and how quantum mechanics makes photon teleportation possible1 of 2
Understand the concept of teleportation and how quantum mechanics makes photon teleportation possibleHow quantum mechanics makes photon teleportation possible.
See all videos for this article
measuring correlation between photons
2 of 2
measuring correlation between photonsFigure 6: Experiment to determine the correlation in measured angular momentum values for a pair of protons with zero total angular momentum. The two protons are initially at the point 0 and move in opposite directions toward the two magnets.
Does nature behave as quantum mechanics predicts? The answer comes from measuring the components of angular momenta for the two protons along different directions with an angle θ between them. A measurement on one proton can give only the result +ℏ/2 or −ℏ/2. The experiment consists of measuring correlations between the plus and minus values for pairs of protons with a fixed value of θ, and then repeating the measurements for different values of θ, as in Figure 6. The interpretation of the results rests on an important theorem by the Irish-born physicist John Stewart Bell. Bell began by assuming the existence of some form of hidden variable with a value that would determine whether the measured angular momentum gives a plus or minus result. He further assumed locality—namely, that measurement on one proton (i.e., the choice of the measurement direction) cannot affect the result of the measurement on the other proton. Both these assumptions agree with classical, commonsense ideas. He then showed quite generally that these two assumptions lead to a certain relationship, now known as Bell’s inequality, for the correlation values mentioned above. Experiments have been conducted at several laboratories with photons instead of protons (the analysis is similar), and the results show fairly conclusively that Bell’s inequality is violated. That is to say, the observed results agree with those of quantum mechanics and cannot be accounted for by a hidden variable (or deterministic)  based on the concept of locality. One is forced to conclude that the two protons are a correlated pair and that a measurement on one affects the state of both, no matter how far apart they are. This may strike one as highly peculiar, but such is the way nature appears to be.

It may be noted that the effect on the state of proton 2 following a measurement on proton 1 is believed to be instantaneous; the effect happens before a light signal initiated by the measuring event at proton 1 reaches proton 2. Alain Aspect and his coworkers in Paris demonstrated this result in 1982 with an ingenious experiment in which the correlation between the two angular momenta was measured, within a very short time interval, by a high-frequency switching device. The interval was less than the time taken for a light signal to travel from one particle to the other at the two measurement positions. Einstein’s special  of relativity states that no message can travel with a speed greater than that of light. Thus, there is no way that the information concerning the direction of the measurement on the first proton could reach the second proton before the measurement was made on it.

Measurement in quantum mechanics
The way quantum mechanics treats the process of measurement has caused considerable debate. Schrödinger’s time-dependent wave equation (equation [8]) is an exact recipe for determining the way the wave function varies with time for a given physical system in a given physical environment. According to the Schrödinger equation, the wave function varies in a strictly determinate way. On the other hand, in the axiomatic approach to quantum mechanics described above, a measurement changes the wave function abruptly and discontinuously. Before the measurement is made, the wave function Ψ is a mixture of the ψs as indicated in equation (10). The measurement changes Ψ from a mixture of ψs to a single ψ. This change, brought about by the process of measurement, is termed the collapse or reduction of the wave function. The collapse is a discontinuous change in Ψ; it is also unpredictable, because, starting with the same Ψ represented by the right-hand side of equation (10), the end result can be any one of the individual ψs.

The Schrödinger equation, which gives a smooth and predictable variation of Ψ, applies between the measurements. The measurement process itself, however, cannot be described by the Schrödinger equation; it is somehow a thing apart. This appears unsatisfactory, inasmuch as a measurement is a physical process and ought to be the subject of the Schrödinger equation just like any other physical process.

The difficulty is related to the fact that quantum mechanics applies to microscopic systems containing one (or a few) electrons, protons, or photons. Measurements, however, are made with large-scale objects (e.g., detectors, amplifiers, and meters) in the macroscopic world, which obeys the laws of classical physics. Thus, another way of formulating the question of what happens in a measurement is to ask how the microscopic quantum world relates and interacts with the macroscopic classical world. More narrowly, it can be asked how and at what point in the measurement process does the wave function collapse? So far, there are no satisfactory answers to these questions, although there are several schools of thought.

One approach stresses the role of a conscious observer in the measurement process and suggests that the wave function collapses when the observer reads the measuring instrument. Bringing the conscious mind into the measurement problem seems to raise more questions than it answers, however.

As discussed above, the Copenhagen interpretation of the measurement process is essentially pragmatic. It distinguishes between microscopic quantum systems and macroscopic measuring instruments. The initial object or event—e.g., the passage of an electron, photon, or atom—triggers the classical measuring device into giving a reading; somewhere along the chain of events, the result of the measurement becomes fixed (i.e., the wave function collapses). This does not answer the basic question but says, in effect, not to worry about it. This is probably the view of most practicing physicists.

A third school of thought notes that an essential feature of the measuring process is irreversibility. This contrasts with the behaviour of the wave function when it varies according to the Schrödinger equation; in principle, any such variation in the wave function can be reversed by an appropriate experimental arrangement. However, once a classical measuring instrument has given a reading, the process is not reversible. It is possible that the key to the nature of the measurement process lies somewhere here. The Schrödinger equation is known to apply only to relatively simple systems. It is an enormous extrapolation to assume that the same equation applies to the large and complex system of a classical measuring device. It may be that the appropriate equation for such a system has features that produce irreversible effects (e.g., wave-function collapse) which differ in kind from those for a simple system.

One may also mention the so-called many-worlds interpretation, proposed by Hugh Everett III in 1957, which suggests that, when a measurement is made for a system in which the wave function is a mixture of states, the universe branches into a number of noninteracting universes. Each of the possible outcomes of the measurement occurs, but in a different universe. Thus, if Sx = 
1
/
2
 is the result of a Stern-Gerlach measurement on a silver atom (see above Incompatible observables), there is another universe identical to ours in every way (including clones of people), except that the result of the measurement is Sx = −1/2. Although this fanciful model solves some measurement problems, it has few adherents among physicists.

Because the various ways of looking at the measurement process lead to the same experimental consequences, trying to distinguish between them on scientific grounds may be fruitless. One or another may be preferred on the grounds of plausibility, elegance, or economy of hypotheses, but these are matters of individual taste. Whether one day a satisfactory quantum  of measurement will emerge, distinguished from the others by its verifiable predictions, remains an open question.

Applications of quantum mechanics
Know about the future of quantum technology and how quantum devices can be present in everyday technology
Know about the future of quantum technology and how quantum devices can be present in everyday technologyLearn about the future of quantum technology.
See all videos for this article
As has been noted, quantum mechanics has been enormously successful in explaining microscopic phenomena in all branches of physics. The three phenomena described in this section are examples that demonstrate the quintessence of the .

Decay of the kaon
The kaon (also called the K0 meson), discovered in 1947, is produced in high-energy collisions between nuclei and other particles. It has zero electric charge, and its mass is about one-half the mass of the proton. It is unstable and, once formed, rapidly decays into either 2 or 3 pi-mesons. The average lifetime of the kaon is about 10−10 second.

In spite of the fact that the kaon is uncharged, quantum  predicts the existence of an antiparticle with the same mass, decay products, and average lifetime; the antiparticle is denoted by K0. During the early 1950s, several physicists questioned the justification for postulating the existence of two particles with such similar properties. In 1955, however, Murray Gell-Mann and Abraham Pais made an interesting prediction about the decay of the kaon. Their reasoning provides an excellent illustration of the quantum mechanical axiom that the wave function Ψ can be a superposition of states; in this case, there are two states, the K0 and K0 mesons themselves.

A K0 meson may be represented formally by writing the wave function as Ψ = K0; similarly Ψ = K0 represents a K0 meson. From the two states, K0 and K0, the following two new states are constructed:
special composition for article "Quantum Mechanics"
special composition for article "Quantum Mechanics"

From these two equations it follows that
special composition for article "Quantum Mechanics"
special composition for article "Quantum Mechanics"

The reason for defining the two states K1 and K2 is that, according to quantum , when the K0 decays, it does not do so as an isolated particle; instead, it combines with its antiparticle to form the states K1 and K2. The state K1 (called the K-short [K0S]) decays into two pi-mesons with a very short lifetime (about 9 × 10−11 second), while K2 (called the K-long [K0L]) decays into three pi-mesons with a longer lifetime (about 5 × 10−8 second).

decay of K0 meson
decay of K0 mesonFigure 7: Decay of the K0 meson.
The physical consequences of these results may be demonstrated in the following experiment. K0 particles are produced in a nuclear reaction at the point A (Figure 7). They move to the right in the figure and start to decay. At point A, the wave function is Ψ = K0, which, from equation (16), can be expressed as the sum of K1 and K2. As the particles move to the right, the K1 state begins to decay rapidly. If the particles reach point B in about 10−8 second, nearly all the K1 component has decayed, although hardly any of the K2 component has done so. Thus, at point B, the beam has changed from one of pure K0 to one of almost pure K2, which equation (15) shows is an equal mixture of K0 and K0. In other words, K0 particles appear in the beam simply because K1 and K2 decay at different rates. At point B, the beam enters a block of absorbing material. Both the K0 and K0 are absorbed by the nuclei in the block, but the K0 are absorbed more strongly. As a result, even though the beam is an equal mixture of K0 and K0 when it enters the absorber, it is almost pure K0 when it exits at point C. The beam thus begins and ends as K0.

Gell-Mann and Pais predicted all this, and experiments subsequently verified it. The experimental observations are that the decay products are primarily two pi-mesons with a short decay time near A, three pi-mesons with longer decay time near B, and two pi-mesons again near C. (This account exaggerates the changes in the K1 and K2 components between A and B and in the K0 and K0 components between B and C; the argument, however, is unchanged.) The phenomenon of generating the K0 and regenerating the K1 decay is purely quantum. It rests on the quantum axiom of the superposition of states and has no classical counterpart.

Cesium clock
The cesium clock is the most accurate type of clock yet developed. This device makes use of transitions between the spin states of the cesium nucleus and produces a frequency which is so regular that it has been adopted for establishing the time standard.

Like electrons, many atomic nuclei have spin. The spin of these nuclei produces a set of small effects in the spectra, known as hyperfine structure. (The effects are small because, though the angular momentum of a spinning nucleus is of the same magnitude as that of an electron, its magnetic moment, which governs the energies of the atomic levels, is relatively small.) The nucleus of the cesium atom has spin quantum number 7/2. The total angular momentum of the lowest energy states of the cesium atom is obtained by combining the spin angular momentum of the nucleus with that of the single valence electron in the atom. (Only the valence electron contributes to the angular momentum because the angular momenta of all the other electrons total zero. Another simplifying feature is that the ground states have zero orbital momenta, so only spin angular momenta need to be considered.) When nuclear spin is taken into account, the total angular momentum of the atom is characterized by a quantum number, conventionally denoted by F, which for cesium is 4 or 3. These values come from the spin value 7/2 for the nucleus and 1/2 for the electron. If the nucleus and the electron are visualized as tiny spinning tops, the value F = 4 (7/2 + 1/2) corresponds to the tops spinning in the same sense, and F = 3 (7/2 − 1/2) corresponds to spins in opposite senses. The energy difference ΔE of the states with the two F values is a precise quantity. If electromagnetic radiation of frequency ν0, where
special composition for article "Quantum Mechanics"
is applied to a system of cesium atoms, transitions will occur between the two states. An apparatus that can detect the occurrence of transitions thus provides an extremely precise frequency standard. This is the principle of the cesium clock.

cesium clock
cesium clockFigure 8: Cesium clock.
The apparatus is shown schematically in Figure 8. A beam of cesium atoms emerges from an oven at a temperature of about 100 °C. The atoms pass through an inhomogeneous magnet A, which deflects the atoms in state F = 4 downward and those in state F = 3 by an equal amount upward. The atoms pass through slit S and continue into a second inhomogeneous magnet B. Magnet B is arranged so that it deflects atoms with an unchanged state in the same direction that magnet A deflected them. The atoms follow the paths indicated by the broken lines in the figure and are lost to the beam. However, if an alternating electromagnetic field of frequency ν0 is applied to the beam as it traverses the centre region C, transitions between states will occur. Some atoms in state F = 4 will change to F = 3, and vice versa. For such atoms, the deflections in magnet B are reversed. The atoms follow the whole lines in the diagram and strike a tungsten wire, which gives electric signals in proportion to the number of cesium atoms striking the wire. As the frequency ν of the alternating field is varied, the signal has a sharp maximum for ν = ν0. The length of the apparatus from the oven to the tungsten detector is about one metre.

cesium-133 states
cesium-133 statesFigure 9: Variation of energy with magnetic-field strength for the F = 4 and F = 3 states in cesium-133.
Each atomic state is characterized not only by the quantum number F but also by a second quantum number mF. For F = 4, mF can take integral values from 4 to −4. In the absence of a magnetic field, these states have the same energy. A magnetic field, however, causes a small change in energy proportional to the magnitude of the field and to the mF value. Similarly, a magnetic field changes the energy for the F = 3 states according to the mF value which, in this case, may vary from 3 to −3. The energy changes are indicated in Figure 9. In the cesium clock, a weak constant magnetic field is superposed on the alternating electromagnetic field in region C. The  shows that the alternating field can bring about a transition only between pairs of states with mF values that are the same or that differ by unity. However, as can be seen from the figure, the only transitions occurring at the frequency ν0 are those between the two states with mF = 0. The apparatus is so sensitive that it can discriminate easily between such transitions and all the others.

If the frequency of the oscillator drifts slightly so that it does not quite equal ν0, the detector output drops. The change in signal strength produces a signal to the oscillator to bring the frequency back to the correct value. This feedback system keeps the oscillator frequency automatically locked to ν0.

The cesium clock is exceedingly stable. The frequency of the oscillator remains constant to about one part in 1013. For this reason, the device is used to redefine the second. This base unit of time in the SI system is defined as equal to 9,192,631,770 cycles of the radiation corresponding to the transition between the levels F = 4, mF = 0 and F = 3, mF = 0 of the ground state of the cesium-133 atom. Prior to 1967, the second was defined in terms of the motion of Earth. The latter, however, is not nearly as stable as the cesium clock. Specifically, the fractional variation of Earth’s rotation period is a few hundred times larger than that of the frequency of the cesium clock.

A quantum voltage standard
Quantum  has been used to establish a voltage standard, and this standard has proven to be extraordinarily accurate and consistent from laboratory to laboratory.

If two layers of superconducting material are separated by a thin insulating barrier, a supercurrent (i.e., a current of paired electrons) can pass from one superconductor to the other. This is another example of the tunneling process described earlier. Several effects based on this phenomenon were predicted in 1962 by the British physicist Brian D. Josephson. Demonstrated experimentally soon afterwards, they are now referred to as the Josephson effects.

If a DC (direct-current) voltage V is applied across the two superconductors, the energy of an electron pair changes by an amount of 2eV as it crosses the junction. As a result, the supercurrent oscillates with frequency ν given by the Planck relationship (E = hν). Thus,
special composition for article "Quantum Mechanics"

This oscillatory behaviour of the supercurrent is known as the AC (alternating-current) Josephson effect. Measurement of V and ν permits a direct verification of the Planck relationship. Although the oscillating supercurrent has been detected directly, it is extremely weak. A more sensitive method of investigating equation (19) is to study effects resulting from the interaction of microwave radiation with the supercurrent.

Several carefully conducted experiments have verified equation (19) to such a high degree of precision that it has been used to determine the value of 2e/h. This value can in fact be determined more precisely by the AC Josephson effect than by any other method. The result is so reliable that laboratories now employ the AC Josephson effect to set a voltage standard. The numerical relationship between V and ν is
Special composition for "Quantum Mechanics" article. equation 20

In this way, measuring a frequency, which can be done with great precision, gives the value of the voltage. Before the Josephson method was used, the voltage standard in metrological laboratories devoted to the maintenance of physical units was based on high-stability Weston cadmium cells. These cells, however, tend to drift and so caused inconsistencies between standards in different laboratories. The Josephson method has provided a standard giving agreement to within a few parts in 108 for measurements made at different times and in different laboratories.

The experiments described in the preceding two sections are only two examples of high-precision measurements in physics. The values of the fundamental constants, such as c, h, e, and me, are determined from a wide variety of experiments based on quantum phenomena. The results are so consistent that the values of the constants are thought to be known in most cases to better than one part in 108. Physicists may not know what they are doing when they make a measurement, but they do it extremely well.

Gordon Leslie Squires
Britannica AI Icon
Ask Anything
Homework Help
Science
Astronomy
CITE

space-time
physics
Also known as: four-dimensional space, space-time continuum
 
Britannica Editors  Dec. 18, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Homework Help
Top Questions
What is space-time?
How are space and time connected in space-time?
Why did Albert Einstein think space and time should be combined into one idea?
How does gravity affect space-time?
space-time, in physical science, single concept that recognizes the union of space and time, first proposed by the mathematician Hermann Minkowski in 1908 as a way to reformulate Albert Einstein’s special  of relativity (1905).

(Read Einstein’s 1926 Britannica essay on space-time.)

Common intuition previously supposed no connection between space and time. Physical space was held to be a flat, three-dimensional continuum—i.e., an arrangement of all possible point locations—to which Euclidean postulates would apply. To such a spatial manifold, Cartesian coordinates seemed most naturally adapted, and straight lines could be conveniently accommodated. Time was viewed independent of space—as a separate, one-dimensional continuum, completely homogeneous along its infinite extent. Any “now” in time could be regarded as an origin from which to take duration past or future to any other time instant. Uniformly moving spatial coordinate systems attached to uniform time continua represented all unaccelerated motions, the special class of so-called inertial reference frames. The universe according to this convention was called Newtonian. In a Newtonian universe, the laws of physics would be the same in all inertial frames, so that one could not single out one as representing an absolute state of rest.

Nicolaus Copernicus. Nicolas Copernicus (1473-1543) Polish astronomer. In 1543 he published, forward proof of a Heliocentric (sun centered) universe. Coloured stipple engraving published London 1802. De revolutionibus orbium coelestium libri vi.
Britannica Quiz
All About Astronomy
In the Minkowski universe, the time coordinate of one coordinate system depends on both the time and space coordinates of another relatively moving system according to a rule that forms the essential alteration required for Einstein’s special  of relativity; according to Einstein’s  there is no such thing as “simultaneity” at two different points of space, hence no absolute time as in the Newtonian universe. The Minkowski universe, like its predecessor, contains a distinct class of inertial reference frames, but now spatial dimensions, mass, and velocities are all relative to the inertial frame of the observer, following specific laws first formulated by H.A. Lorentz, and later forming the central rules of Einstein’s  and its Minkowski interpretation. Only the speed of light is the same in all inertial frames. Every set of coordinates, or particular space-time event, in such a universe is described as a “here-now” or a world point. In every inertial reference frame, all physical laws remain unchanged.

Einstein’s general  of relativity (1916) again makes use of a four-dimensional space-time, but incorporates gravitational effects. Gravity is no longer thought of as a force, as in the Newtonian system, but as a cause of a “warping” of space-time, an effect described explicitly by a set of equations formulated by Einstein. The result is a “curved” space-time, as opposed to the “flat” Minkowski space-time, where trajectories of particles are straight lines in an inertial coordinate system. In Einstein’s curved space-time, a direct extension of Riemann’s notion of curved space (1854), a particle follows a world line, or geodesic, somewhat analogous to the way a billiard ball on a warped surface would follow a path determined by the warping or curving of the surface. One of the basic tenets of general relativity is that inside a container following a geodesic of space-time, such as an elevator in free-fall, or a satellite orbiting the Earth, the effect would be the same as a total absence of gravity. The paths of light rays are also geodesics of space-time, of a special sort, called “null geodesics.” The speed of light again has the same constant velocity c.

In both Newton’s and Einstein’s theories, the route from gravitational masses to the paths of particles is rather roundabout. In the Newtonian formulation, the masses determine the total gravitational force at any point, which by Newton’s third law determines the acceleration of the particle. The actual path, as in the orbit of a planet, is found by solving a differential equation. In general relativity, one must solve Einstein’s equations for a given situation to determine the corresponding structure of space-time, and then solve a second set of equations to find the path of a particle. However, by invoking the general principle of equivalence between the effects of gravity and of uniform acceleration, Einstein was able to deduce certain effects, such as the deflection of light when passing a massive object, such as a star.

The first exact solution of Einstein’s equations, for a single spherical mass, was carried out by a German astronomer, Karl Schwarzschild (1916). For so-called small masses, the solution does not differ too much from that afforded by Newton’s gravitational law, but enough to account for the previously unexplained size of the advance of the perihelion of Mercury. For “large” masses the Schwarzschild solution predicts unusual properties. Astronomical observations of dwarf stars eventually led the American physicists J. Robert Oppenheimer and H. Snyder (1939) to postulate super-dense states of matter. These, and other hypothetical conditions of gravitational collapse, were borne out in later discoveries of pulsars, neutron stars, and black holes.

Key People: Albert Einstein Stephen Hawking Roger Penrose Hermann Minkowski Samuel Alexander
Related Topics: physical science time space Minkowski universe world point
On the Web: Physics LibreTexts - Geometry of Space-time (Dec. 18, 2025)

Access for the whole family!
Bundle Britannica Premium and Kids for the ultimate resource destination.
A subsequent paper of Einstein (1917) applies the  of general relativity to cosmology, and in fact represents the birth of modern cosmology. In it, Einstein looks for models of the entire universe that satisfy his equations under suitable assumptions about the large-scale structure of the universe, such as its “homogeneity,” meaning that space-time looks the same in any part as any other part (the “cosmological principle”). Under those assumptions, the solutions seemed to imply that space-time was either expanding or contracting, and in order to construct a universe that did neither, Einstein added an extra term to his equations, the so-called “cosmological constant.” When observational evidence later revealed that the universe did in fact seem to be expanding, Einstein withdrew that suggestion. However, closer analysis of the expansion of the universe during the late 1990s once more led astronomers to believe that a cosmological constant should indeed be included in Einstein’s equations.

The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Adam Augustyn.
6 Amazing Facts About Gravitational Waves and LIGO
Introduction
Gravitational Waves Are Ripples in Space-Time
Gravitational Waves Come from Really Heavy Objects
The Effect of Gravitational Waves Is Very, Very Small
Measuring Gravitational Waves Is Tricky
LIGO Is Very Sensitive
Gravitational-Wave Astronomy Can See an Entirely New Side of the Universe
References & Edit History
Related Topics
Images
Laser Interferometer Gravitational-Wave Observatory (LIGO)
Britannica AI Icon
Ask Anything
Homework Help
Science
Physics
Matter & Energy

CITE

Laser Interferometer Gravitational-Wave Observatory (LIGO)
Laser Interferometer Gravitational-Wave Observatory (LIGO) The Laser Interferometer Gravitational-Wave Observatory (LIGO) near Hanford, Washington, U.S. There are two LIGO installations; the other is near Livingston, Louisiana.
6 Amazing Facts About Gravitational Waves and LIGO
Catch a gravitational wave or two.
 
Erik Gregersen  
Britannica Editors
Britannica AI Icon
Britannica AI
Ask Anything
Homework Help
Nearly everything we know about the universe comes from electromagnetic radiation—that is, light. Astronomy began with visible light and then expanded to the rest of the electromagnetic spectrum. By using the spectrum, from the short wavelengths of gamma rays to the long wavelengths of radio waves, astronomers have discovered strange and wondrous things. Now a new form of astronomy, gravitational-wave astronomy, has come. It watches not light but movements in space-time. Along with the new astronomy has come a new kind of observatory, the Laser Interferometer Gravitational-Wave Observatory (LIGO).

Gravitational Waves Are Ripples in Space-Time
According to Albert Einstein’s  of general relativity, gravity is not a force reaching out through the universe. It’s a bending of space-time. When an object accelerates, it distorts the space-time around it, and that distortion travels away from the source at the speed of light.

Gravitational Waves Come from Really Heavy Objects
So how massive an object are we talking about? The first proof that gravitational waves actually exist came from a binary pulsar—two neutron stars, each about the mass of the Sun, that orbit each other. The pulsars’ orbit is gradually shrinking, so the pulsars are losing energy. That energy is exactly the amount that general relativity predicts that the pulsars would give off in gravitational waves.

The Effect of Gravitational Waves Is Very, Very Small
Since gravitational waves are a ripple in space-time, they cause the distance between two points to change ever so slightly. How slightly? LIGO must be able to measure distances as small as 10−19 meter. The proton has a radius of about 0.85 × 10−15 meter, or 10,000 times larger.

Measuring Gravitational Waves Is Tricky
To detect a change in distance much smaller than the proton requires great precision. Each LIGO installation is a laser interferometer made up of two underground pipes, each 1.3 meters (4.3 feet) wide and 4 km (2.5 miles) long, set in an L-shape. The inside of the pipes is a vacuum. When a gravitational wave passes through LIGO, one arm of the instrument gets longer and the other gets shorter. A laser beam is split in half, sent down the two pipes, reflected back, and then recombined so the two beams cancel each other out in destructive interference if there is no gravitational wave. If there is a gravitational wave, the beams won’t cancel each other out. A 4-km long beam is still not enough to detect a gravitational wave, so the beams are bounced back and forth about 400 times so the light travels a distance of 1,600 km (1,000 miles).

LIGO Is Very Sensitive
LIGO detects such a small change in distance that it can detect a lot of other vibrations too. For example, the speed limit at LIGO is 16 km (10 miles) per hour to minimize vibrations from nearby cars. One source of noise is gravity gradient noise, which is the minute change in Earth’s gravitational field when a vibration passes through the ground near the mirrors. The mirrors that reflect the light weigh 40 kg (88 pounds) and hang by silica fibers in a complex suspension system. To make sure that LIGO does detect gravitational waves and not just passing cars, there are two LIGO installations—one in Livingston, Louisiana, and the other in Hanford, Washington. A gravitational wave would show up at both installations.

Related Topics: gravitational wave
Gravitational-Wave Astronomy Can See an Entirely New Side of the Universe
If supermassive black holes (black holes one million times more massive than the Sun) merged in a distant galaxy, LIGO could observe it. Scientists also expect that if a neutron star is slightly nonspherical, the gravitational waves could be observed and thus reveal much about the star’s structure. Every time astronomers have been able to look at the universe in a new way, they’ve always observed something unexpected, and gravitational-wave astronomy will likely show something not yet thought of.


Access for the whole family!
Bundle Britannica Premium and Kids for the ultimate resource destination.
Erik Gregersen
cosmology
Introduction
The cosmological expansion
The nature of space and time
Relativistic cosmologies
The hot big bang
The very early universe
Steady state  and other alternative cosmologies
References & Edit History
Related Topics
Images & Videos
Examine the observable universe's place within the whole universeAndromeda Galaxybig bang modelcurved space-timeexperimental evidence for general relativityintrinsic curvature of a surfacerelative size of the universeWilkinson Microwave Anisotropy Probeevolution of the universesupernova 1987A in the Large Magellanic Cloud
For Students
Andromeda Galaxy
cosmology summary
Quizzes
The orbits of the planets and other elements of the solar system, including asteroids, Kuiper belt, Oort cloud, comet
Space Odyssey
Ursa major constellation illustration art.  (Big Dipper) stars, space, night sky)
Stars: Explosions in Space
Nicolaus Copernicus. Nicolas Copernicus (1473-1543) Polish astronomer. In 1543 he published, forward proof of a Heliocentric (sun centered) universe. Coloured stipple engraving published London 1802. De revolutionibus orbium coelestium libri vi.
All About Astronomy
Horologist Roman Piekarski starts the time consuming task of adjusting the 600 antique clocks at Cuckooland Museum in readiness for this weekends change to British summer time on March 23, 2009 in Knutsford, England.
Ologies Quiz
Britannica AI Icon
Ask Anything
Quick Summary
Homework Help
Science
Astronomy

CITE

cosmology
astronomy
 
Frank H. Shu  
Britannica Editors  Nov. 14, 2025 •History
Britannica AI Icon
Britannica AI
Ask Anything
Quick Summary
Homework Help
Examine the observable universe's place within the whole universe
Examine the observable universe's place within the whole universeLearn about defining and measuring the observable universe within the “whole” universe.
See all videos for this article
cosmology, field of study that brings together the natural sciences, particularly astronomy and physics, in a joint effort to understand the physical universe as a unified whole. The “observable universe” is the region of space that humans can actually or theoretically observe with the aid of technology. It can be thought of as a bubble with Earth at its centre. It is differentiated from the entirety of the universe, which is the whole cosmic system of matter and energy, including the human race. Unlike the observable universe, the  universe is possibly infinite and without spatial edges.

If one looks up on a clear night, one will see that the sky is full of stars. During the summer months in the Northern Hemisphere, a faint band of light stretches from horizon to horizon, a swath of pale white cutting across a background of deepest black. For the early Egyptians, this was the heavenly Nile, flowing through the land of the dead ruled by Osiris. The ancient Greeks likened it to a river of milk. Astronomers now know that the band is actually composed of countless stars in a flattened disk seen edge on. The stars are so close to one another along the line of sight that the unaided eye has difficulty discerning the individual members. Through a large telescope, astronomers find myriads of like systems sprinkled throughout the depths of space. They call such vast collections of stars galaxies, after the Greek word for milk, and call the local galaxy to which the Sun belongs the Milky Way Galaxy or simply the Galaxy.

The Sun is a star around which Earth and the other planets revolve, and by extension every visible star in the sky is a sun in its own right. Some stars are intrinsically brighter than the Sun; others, fainter. Much less light is received from the stars than from the Sun because the stars are all much farther away. Indeed, they appear densely packed in the Milky Way only because there are so many of them. The actual separations of the stars are enormous, so large that it is conventional to measure their distances in units of how far light can travel in a given amount of time. The speed of light (in a vacuum) equals 3 × 1010 cm/sec (centimetres per second); at such a speed, it is possible to circle the Earth seven times in a single second. Thus in terrestrial terms the Sun, which lies 500 light-seconds from the Earth, is very far away; however, even the next closest star, Proxima Centauri, at a distance of 4.3 light-years (4.1 × 1018 cm), is 270,000 times farther yet. The stars that lie on the opposite side of the Milky Way from the Sun have distances that are on the order of 100,000 light-years, which is the typical diameter of a large spiral galaxy.

Andromeda Galaxy
Andromeda GalaxyThe Andromeda Galaxy, also known as the Andromeda Nebula or M31. It is the closest spiral galaxy to Earth, at a distance of 2.48 million light-years.
If the kingdom of the stars seems vast, the realm of the galaxies is larger still. The nearest galaxies to the Milky Way system are the Large and Small Magellanic Clouds, two irregular satellites of the Galaxy visible to the naked eye in the Southern Hemisphere. The Magellanic Clouds are relatively small (containing roughly 109 stars) compared to the Galaxy (with some 1011 stars), and they lie at a distance of about 200,000 light-years. The nearest large galaxy comparable to the Galaxy is the Andromeda Galaxy (also called M31 because it was the 31st entry in a catalog of astronomical objects compiled by the French astronomer Charles Messier in 1781), and it lies at a distance of about 2,000,000 light-years. The Magellanic Clouds, the Andromeda Galaxy, and the Milky Way system all are part of an aggregation of two dozen or so neighbouring galaxies known as the Local Group. The Galaxy and M31 are the largest members of this group.

Key People: Emanuel Swedenborg Giordano Bruno George Gamow Pascual Jordan James Peebles
Related Topics: multiverse cosmography matter-antimatter asymmetry massive-neutrino hypothesis inhomogeneous nucleosynthesis hypothesis
The Galaxy and M31 are both spiral galaxies, and they are among the brighter and more massive of all spiral galaxies. The most luminous and brightest galaxies, however, are not spirals but rather supergiant ellipticals (also called cD galaxies by astronomers for historical reasons that are not particularly illuminating). Elliptical galaxies have roundish shapes rather than the flattened distributions that characterize spiral galaxies, and they tend to occur in rich clusters (those containing thousands of members) rather than in the loose groups favoured by spirals. The brightest member galaxies of rich clusters have been detected at distances exceeding several thousand million light-years from the Earth. The branch of learning that deals with phenomena at the scale of many millions of light-years is called cosmology—a term derived from combining two Greek words, kosmos, meaning “order,” “harmony,” and “the world,” and logos, signifying “word” or “discourse.” Cosmology is, in effect, the study of the universe at large.

Horologist Roman Piekarski starts the time consuming task of adjusting the 600 antique clocks at Cuckooland Museum in readiness for this weekends change to British summer time on March 23, 2009 in Knutsford, England.
Britannica Quiz
Ologies Quiz
The cosmological expansion
big bang model
big bang modelAccording to the evolutionary, or big bang,  of the universe, the universe is expanding while the total energy and matter it contains remain constant. Therefore, as the universe expands, the density of its energy and matter must become progressively thinner. At left is a two-dimensional representation of the universe as it appears now, with galaxies occupying a typical section of space. At right, billions of years later the same amount of matter will fill a larger volume of space.
When the universe is viewed in the large, a dramatic new feature, not present on small scales, emerges—namely, the cosmological expansion. On cosmological scales, galaxies (or, at least, clusters of galaxies) appear to be racing away from one another with the apparent velocity of recession being linearly proportional to the distance of the object. This relation is known as the Hubble law (after its discoverer, the American astronomer Edwin Powell Hubble). Interpreted in the simplest fashion, the Hubble law implies that 13.8 billion years ago all of the matter in the universe was closely packed together in an incredibly dense state and that everything then exploded in a “big bang,” the signature of the explosion being written eventually in the galaxies of stars that formed out of the expanding debris of matter. Strong scientific support for this interpretation of a big bang origin of the universe comes from the detection by radio telescopes of a steady and uniform background of microwave radiation. The cosmic microwave background is believed to be a ghostly remnant of the fierce light of the primeval fireball reduced by cosmic expansion to a shadow of its former splendour but still pervading every corner of the known universe.

The simple (and most common) interpretation of the Hubble law as a recession of the galaxies over time through space, however, contains a misleading notion. In a sense, as will be made more precise later in the article, the expansion of the universe represents not so much a fundamental motion of galaxies within a framework of absolute time and absolute space, but an expansion of time and space themselves. On cosmological scales, the use of light-travel times to measure distances assumes a special significance because the lengths become so vast that even light, traveling at the fastest speed attainable by any physical entity, takes a significant fraction of the age of the universe (13.8 billion years old) to travel from an object to an observer. Thus, when astronomers measure objects at cosmological distances from the Local Group, they are seeing the objects as they existed during a time when the universe was much younger than it is today. Under these circumstances, Albert Einstein taught in his  of general relativity that the gravitational field of everything in the universe so warps space and time as to require a very careful reevaluation of quantities whose seemingly elementary natures are normally taken for granted.

The nature of space and time
Finite or infinite?
An issue that arises when one contemplates the universe at large is whether space and time are infinite or finite. After many centuries of thought by some of the best minds, humanity has still not arrived at conclusive answers to these questions. Aristotle’s answer was that the material universe must be spatially finite, for if stars extended to infinity, they could not perform a complete rotation around Earth in 24 hours. Space must then itself also be finite because it is merely a receptacle for material bodies. On the other hand, the heavens must be temporally infinite, without beginning or end, since they are imperishable and cannot be created or destroyed.

Except for the infinity of time, these views came to be accepted religious teachings in Europe before the period of modern science. The most notable person to publicly express doubts about restricted space was the Italian philosopher-mathematician Giordano Bruno, who asked the obvious question that, if there is a boundary or edge to space, what is on the other side? For his advocacy of an infinity of suns and earths, he was burned at the stake in 1600.

In 1610 the German astronomer Johannes Kepler provided a profound reason for believing that the number of stars in the universe had to be finite. If there were an infinity of stars, he argued, then the sky would be completely filled with them and night would not be dark! This point was rediscussed by the astronomers Edmond Halley of England and Jean-Philippe-Loys de Chéseaux of Switzerland in the 18th century, but it was not popularized as a paradox until Wilhelm Olbers of Germany took up the problem in the 19th century. The difficulty became potentially very real with American astronomer Edwin Hubble’s measurement of the enormous extent of the universe of galaxies with its large-scale homogeneity and isotropy. His discovery of the systematic recession of the galaxies provided an escape, however. At first people thought that the redshift effect alone would suffice to explain why the sky is dark at night—namely, that the light from the stars in distant galaxies would be redshifted to long wavelengths beyond the visible regime. The modern consensus is, however, that a finite age for the universe is a far more important effect. Even if the universe is spatially infinite, photons from very distant galaxies simply do not have the time to travel to Earth because of the finite speed of light. There is a spherical surface, the cosmic event horizon (13.8 billion light-years in radial distance from Earth at the current epoch), beyond which nothing can be seen even in principle; and the number (roughly 1010) of galaxies within this cosmic horizon, the observable universe, are too few to make the night sky bright.

When one looks to great distances, one is seeing things as they were a long time ago, again because light takes a finite time to travel to Earth. Over such great spans, do the classical notions of Euclid concerning the properties of space necessarily continue to hold? The answer given by Einstein was: No, the gravitation of the mass contained in cosmologically large regions may warp one’s usual perceptions of space and time; in particular, the Euclidean postulate that parallel lines never cross need not be a correct description of the geometry of the actual universe. And in 1917 Einstein presented a mathematical model of the universe in which the total volume of space was finite yet had no boundary or edge. The model was based on his  of general relativity that utilized a more generalized approach to geometry devised in the 19th century by the German mathematician Bernhard Riemann.

Gravitation and the geometry of space-time
The physical foundation of Einstein’s view of gravitation, general relativity, lies on two empirical findings that he elevated to the status of basic postulates. The first postulate is the relativity principle: local physics is governed by the  of special relativity. The second postulate is the equivalence principle: there is no way for an observer to distinguish locally between gravity and acceleration. The motivation for the second postulate comes from Galileo’s observation that all objects—independent of mass, shape, colour, or any other property—accelerate at the same rate in a (uniform) gravitational field.

The orbits of the planets and other elements of the solar system, including asteroids, Kuiper belt, Oort cloud, comet
Britannica Quiz
Space Odyssey
Einstein’s  of special relativity, which he developed in 1905, had as its basic premises (1) the notion (also dating back to Galileo) that the laws of physics are the same for all inertial observers and (2) the constancy of the speed of light in a vacuum—namely, that the speed of light has the same value (3 × 1010 centimetres per second [cm/sec], or 2 × 105 miles per second [miles/sec]) for all inertial observers independent of their motion relative to the source of the light. Clearly, this second premise is incompatible with Euclidean and Newtonian precepts of absolute space and absolute time, resulting in a program that merged space and time into a single structure, with well-known consequences. The space-time structure of special relativity is often called “flat” because, among other things, the propagation of photons is easily represented on a flat sheet of graph paper with equal-sized squares. Let each tick on the vertical axis represent one light-year (9.46 × 1017 cm [5.88 × 1012 miles]) of distance in the direction of the flight of the photon, and each tick on the horizontal axis represent the passage of one year (3.16 × 107 seconds) of time. The propagation path of the photon is then a 45° line because it flies one light-year in one year (with respect to the space and time measurements of all inertial observers no matter how fast they move relative to the photon).

curved space-time
curved space-time The four dimensional space-time continuum itself is distorted in the vicinity of any mass, with the amount of distortion depending on the mass and the distance from the mass. Thus, relativity accounts for Newton's inverse square law of gravity through geometry and thereby does away with the need for any mysterious “action at a distance.”
The principle of equivalence in general relativity allows the locally flat space-time structure of special relativity to be warped by gravitation, so that (in the cosmological case) the propagation of the photon over thousands of millions of light-years can no longer be plotted on a globally flat sheet of paper. To be sure, the curvature of the paper may not be apparent when only a small piece is examined, thereby giving the local impression that space-time is flat (i.e., satisfies special relativity). It is only when the graph paper is examined globally that one realizes it is curved (i.e., satisfies general relativity).

In Einstein’s 1917 model of the universe, the curvature occurs only in space, with the graph paper being rolled up into a cylinder on its side, a loop around the cylinder at constant time having a circumference of 2πR—the total spatial extent of the universe. Notice that the “radius of the universe” is measured in a “direction” perpendicular to the space-time surface of the graph paper. Since the ringed space axis corresponds to one of three dimensions of the actual world (any will do since all directions are equivalent in an isotropic model), the radius of the universe exists in a fourth spatial dimension (not time) which is not part of the real world. This fourth spatial dimension is a mathematical artifice introduced to represent diagrammatically the solution (in this case) of equations for curved three-dimensional space that need not refer to any dimensions other than the three physical ones. Photons traveling in a straight line in any physical direction have trajectories that go diagonally (at 45° angles to the space and time axes) from corner to corner of each little square cell of the space-time vacuum; thus, they describe helical paths on the cylindrical surface of the graph paper, making one turn after traveling a spatial distance 2πR. In other words, always flying dead ahead, photons would return to where they started from after going a finite distance without ever coming to an edge or boundary. The distance to the “other side” of the universe is therefore πR, and it would lie in any and every direction; space would be closed on itself.

Now, except by analogy with the closed two-dimensional surface of a sphere that is uniformly curved toward a centre in a third dimension lying nowhere on the two-dimensional surface, no three-dimensional creature can visualize a closed three-dimensional volume that is uniformly curved toward a centre in a fourth dimension lying nowhere in the three-dimensional volume. Nevertheless, three-dimensional creatures could discover the curvature of their three-dimensional world by performing surveying experiments of sufficient spatial scope. They could draw circles, for example, by tacking down one end of a string and tracing along a single plane the locus described by the other end when the string is always kept taut in between (a straight line) and walked around by a surveyor. In Einstein’s universe, if the string were short compared to the quantity R, the circumference of the circle divided by the length of the string (the circle’s radius) would nearly equal 2π = 6.2837853…, thereby fooling the three-dimensional creatures into thinking that Euclidean geometry gives a correct description of their world. However, the ratio of circumference to length of string would become less than 2π when the length of string became comparable to R. Indeed, if a string of length πR could be pulled taut to the antipode of a positively curved universe, the ratio would go to zero. In short, at the tacked-down end the string could be seen to sweep out a great arc in the sky from horizon to horizon and back again; yet, to make the string do this, the surveyor at the other end need only walk around a circle of vanishingly small circumference.

To understand why gravitation can curve space (or more generally, space-time) in such startling ways, consider the following thought experiment that was originally conceived by Einstein. Imagine an elevator in free space accelerating upward, from the viewpoint of a woman in inertial space, at a rate numerically equal to g, the gravitational field at the surface of Earth. Let this elevator have parallel windows on two sides, and let the woman shine a brief pulse of light toward the windows. She will see the photons enter close to the top of the near window and exit near the bottom of the far window because the elevator has accelerated upward in the interval it takes light to travel across the elevator. For her, photons travel in a straight line, and it is merely the acceleration of the elevator that has caused the windows and floor of the elevator to curve up to the flight path of the photons.

Let there now be a man standing inside the elevator. Because the floor of the elevator accelerates him upward at a rate g, he may—if he chooses to regard himself as stationary—think that he is standing still on the surface of Earth and is being pulled to the ground by its gravitational field g. Indeed, in accordance with the equivalence principle, without looking out the windows (the outside is not part of his local environment), he cannot perform any local experiment that would inform him otherwise. Let the woman shine her pulse of light. The man sees, just like the woman, that the photons enter near the top edge of one window and exit near the bottom of the other. And just like the woman, he knows that photons propagate in straight lines in free space. (By the relativity principle, they must agree on the laws of physics if they are both inertial observers.) However, since he actually sees the photons follow a curved path relative to himself, he concludes that they must be bent by the force of gravity. The woman tries to tell him there is no such force at work; he is not an inertial observer. Nonetheless, he has the solidity of Earth beneath him, so he insists on attributing his acceleration to the force of gravity. According to Einstein, they are both right. There is no need to distinguish locally between acceleration and gravity—the two are in some sense equivalent. But if that is the case, then it must be true that gravity—“real” gravity—can actually bend light. And indeed it can, as many experiments have shown since Einstein’s first discussion of the phenomenon.

experimental evidence for general relativity
experimental evidence for general relativity In 1919 observation of a solar eclipse confirmed Einstein's prediction that light is bent in the presence of mass. This experimental support for his general  of relativity garnered him instant worldwide acclaim.
It was the genius of Einstein to go even further. Rather than speak of the force of gravitation having bent the photons into a curved path, might it not be more fruitful to think of photons as always flying in straight lines—in the sense that a straight line is the shortest distance between two points—and that what really happens is that gravitation bends space-time? In other words, perhaps gravitation is curved space-time, and photons fly along the shortest paths possible in this curved space-time, thus giving the appearance of being bent by a “force” when one insists on thinking that space-time is flat. The utility of taking this approach is that it becomes automatic that all test bodies fall at the same rate under the “force” of gravitation, for they are merely producing their natural trajectories in a background space-time that is curved in a certain fashion independent of the test bodies. What was a minor miracle for Galileo and Newton becomes the most natural thing in the world for Einstein.

To complete the program and to conform with Newton’s  of gravitation in the limit of weak curvature (weak field), the source of space-time curvature would have to be ascribed to mass (and energy). The mathematical expression of these ideas constitutes Einstein’s  of general relativity, one of the most beautiful artifacts of pure thought ever produced. The American physicist John Archibald Wheeler and his colleagues summarized Einstein’s view of the universe in these terms:

Curved spacetime tells mass-energy how to move;

mass-energy tells spacetime how to curve.

Contrast this with Newton’s view of the mechanics of the heavens:

Force tells mass how to accelerate;

mass tells gravity how to exert force.

Notice therefore that Einstein’s worldview is not merely a quantitative modification of Newton’s picture (which is also possible via an equivalent route using the methods of quantum field ) but represents a qualitative change of perspective. And modern experiments have amply justified the fruitfulness of Einstein’s alternative interpretation of gravitation as geometry rather than as force. His  would have undoubtedly delighted the Greeks.

Relativistic cosmologies
Einstein’s model
To derive his 1917 cosmological model, Einstein made three assumptions that lay outside the scope of his equations. The first was to suppose that the universe is homogeneous and isotropic in the large (i.e., the same everywhere on average at any instant in time), an assumption that the English astrophysicist Edward A. Milne later elevated to an entire philosophical outlook by naming it the cosmological principle. Given the success of the Copernican revolution, this outlook is a natural one. Newton himself had it implicitly in mind when he took the initial state of the universe to be everywhere the same before it developed “ye Sun and Fixt stars.”

The second assumption was to suppose that this homogeneous and isotropic universe had a closed spatial geometry. As described above, the total volume of a three-dimensional space with uniform positive curvature would be finite but possess no edges or boundaries (to be consistent with the first assumption).

The third assumption made by Einstein was that the universe as a whole is static—i.e., its large-scale properties do not vary with time. This assumption, made before Hubble’s observational discovery of the expansion of the universe, was also natural; it was the simplest approach, as Aristotle had discovered, if one wishes to avoid a discussion of a creation event. Indeed, the philosophical attraction of the notion that the universe on average is not only homogeneous and isotropic in space but also constant in time was so appealing that a school of English cosmologists—Hermann Bondi, Fred Hoyle, and Thomas Gold—would call it the perfect cosmological principle and carry its implications in the 1950s to the ultimate refinement in the so-called steady-state .

To his great chagrin Einstein found in 1917 that with his three adopted assumptions, his equations of general relativity—as originally written down—had no meaningful solutions. To obtain a solution, Einstein realized that he had to add to his equations an extra term, which came to be called the cosmological constant. If one speaks in Newtonian terms, the cosmological constant could be interpreted as a repulsive force of unknown origin that could exactly balance the attraction of gravitation of all the matter in Einstein’s closed universe and keep it from moving. The inclusion of such a term in a more general context, however, meant that the universe in the absence of any mass-energy (i.e., consisting of a vacuum) would not have a space-time structure that was flat (i.e., would not have satisfied the dictates of special relativity exactly). Einstein was prepared to make such a sacrifice only very reluctantly, and, when he later learned of Hubble’s discovery of the expansion of the universe and realized that he could have predicted it had he only had more faith in the original form of his equations, he regretted the introduction of the cosmological constant as the “biggest blunder” of his life. Ironically, observations of distant supernovas have shown the existence of dark energy, a repulsive force that is the dominant component of the universe.

De Sitter’s model
It was also in 1917 that the Dutch astronomer Willem de Sitter recognized that he could obtain a static cosmological model differing from Einstein’s simply by removing all matter. The solution remains stationary essentially because there is no matter to move about. If some test particles are reintroduced into the model, the cosmological term would propel them away from each other. Astronomers now began to wonder if this effect might not underlie the recession of the spiral galaxies.

Hubble Space Telescope
More From Britannica
astronomy: Cosmology
Friedmann-Lemaître models
intrinsic curvature of a surface
intrinsic curvature of a surface
In 1922 Aleksandr A. Friedmann, a Russian meteorologist and mathematician, and in 1927 Georges Lemaître, a Belgian cleric, independently discovered solutions to Einstein’s equations that contained realistic amounts of matter. These evolutionary models correspond to big bang cosmologies. Friedmann and Lemaître adopted Einstein’s assumption of spatial homogeneity and isotropy (the cosmological principle). They rejected, however, his assumption of time independence and considered both positively curved spaces (“closed” universes) as well as negatively curved spaces (“open” universes). The difference between the approaches of Friedmann and Lemaître is that the former set the cosmological constant equal to zero, whereas the latter retained the possibility that it might have a nonzero value. To simplify the discussion, only the Friedmann models are considered here.

The decision to abandon a static model meant that the Friedmann models evolve with time. As such, neighbouring pieces of matter have recessional (or contractional) phases when they separate from (or approach) one another with an apparent velocity that increases linearly with increasing distance. Friedmann’s models thus anticipated Hubble’s law before it had been formulated on an observational basis. It was Lemaître, however, who had the good fortune of deriving the results at the time when the recession of the galaxies was being recognized as a fundamental cosmological observation, and it was he who clarified the measured basis for the phenomenon.

The geometry of space in Friedmann’s closed models is similar to that of Einstein’s original model; however, there is a curvature to time as well as one to space. Unlike Einstein’s model, where time runs eternally at each spatial point on an uninterrupted horizontal line that extends infinitely into the past and future, there is a beginning and end to time in Friedmann’s version of a closed universe when material expands from or is recompressed to infinite densities. These instants are called the instants of the “big bang” and the “big squeeze,” respectively. The global space-time diagram for the middle half of the expansion-compression phases can be depicted as a barrel lying on its side. The space axis corresponds again to any one direction in the universe, and it wraps around the barrel. Through each spatial point runs a time axis that extends along the length of the barrel on its (space-time) surface. Because the barrel is curved in both space and time, the little squares in the vacuum of the curved sheet of graph paper marking the space-time surface are of nonuniform size, stretching to become bigger when the barrel broadens (universe expands) and shrinking to become smaller when the barrel narrows (universe contracts).

It should be remembered that only the surface of the barrel has physical significance; the dimension off the surface toward the axle of the barrel represents the fourth spatial dimension, which is not part of the real three-dimensional world. The space axis circles the barrel and closes upon itself after traversing a circumference equal to 2πR, where R, the radius of the universe (in the fourth dimension), is now a function of the time t. In a closed Friedmann model, R starts equal to zero at time t = 0 (not shown in barrel diagram), expands to a maximum value at time t = tm (the middle of the barrel), and recontracts to zero (not shown) at time t = 2tm, with the value of tm dependent on the total amount of mass that exists in the universe.

Imagine now that galaxies reside on equally spaced tick marks along the space axis. Each galaxy on average does not move spatially with respect to its tick mark in the spatial (ringed) direction but is carried forward horizontally by the march of time. The total number of galaxies on the spatial ring is conserved as time changes, and therefore their average spacing increases or decreases as the total circumference 2πR on the ring increases or decreases (during the expansion or contraction phases). Thus, without in a sense actually moving in the spatial direction, galaxies can be carried apart by the expansion of space itself. From this point of view, the recession of galaxies is not a “velocity” in the usual sense of the word. For example, in a closed Friedmann model, there could be galaxies that started, when R was small, very close to the Milky Way system on the opposite side of the universe. Now, 1010 years later, they are still on the opposite side of the universe but at a distance much greater than 1010 light-years away. They reached those distances without ever having had to move (relative to any local observer) at speeds faster than light—indeed, in a sense without having had to move at all. The separation rate of nearby galaxies can be thought of as a velocity without confusion in the sense of Hubble’s law, if one wants, but only if the inferred velocity is much less than the speed of light.

On the other hand, if the recession of the galaxies is not viewed in terms of a velocity, then the cosmological redshift cannot be viewed as a Doppler shift. How, then, does it arise? The answer is contained in the barrel diagram when one notices that, as the universe expands, each small cell in the space-time vacuum also expands. Consider the propagation of electromagnetic radiation whose wavelength initially spans exactly one cell length (for simplicity of discussion), so that its head lies at a vertex and its tail at one vertex back. Suppose an elliptical galaxy emits such a wave at some time t1. The head of the wave propagates from corner to corner on the little square grids that look locally flat, and the tail propagates from corner to corner one vertex back. At a later time t2, a spiral galaxy begins to intercept the head of the wave. At time t2, the tail is still one vertex back, and therefore the wave train, still containing one wavelength, now spans one current spatial vacuum spacing. In other words, the wavelength has grown in direct proportion to the linear expansion factor of the universe. Since the same conclusion would have held if n wavelengths had been involved instead of one, all electromagnetic radiation from a given object will show the same cosmological redshift if the universe (or, equivalently, the average spacing between galaxies) was smaller at the epoch of transmission than at the epoch of reception. Each wavelength will have been stretched in direct proportion to the expansion of the universe in between.

A nonzero peculiar velocity for an emitting galaxy with respect to its local cosmological frame can be taken into account by Doppler-shifting the emitted photons before applying the cosmological redshift factor; i.e., the observed redshift would be a product of two factors. When the observed redshift is large, one usually assumes that the dominant contribution is of cosmological origin. When this assumption is valid, the redshift is a monotonic function of both distance and time during the expansional phase of any cosmological model. Thus, astronomers often use the redshift z as a shorthand indicator of both distance and elapsed time. Following from this, the statement “object X lies at z = a” means that “object X lies at a distance associated with redshift a”; the statement “event Y occurred at redshift z = b” means that “event Y occurred a time ago associated with redshift b.”

The open Friedmann models differ from the closed models in both spatial and temporal behaviour. In an open universe the total volume of space and the number of galaxies contained in it are infinite. The three-dimensional spatial geometry is one of uniform negative curvature in the sense that, if circles are drawn with very large lengths of string, the ratio of circumferences to lengths of string are greater than 2π. The temporal history begins again with expansion from a big bang of infinite density, but now the expansion continues indefinitely, and the average density of matter and radiation in the universe would eventually become vanishingly small. Time in such a model has a beginning but no end.

The Einstein–de Sitter universe
In 1932 Einstein and de Sitter proposed that the cosmological constant should be set equal to zero, and they derived a homogeneous and isotropic model that provides the separating case between the closed and open Friedmann models; i.e., Einstein and de Sitter assumed that the spatial curvature of the universe is neither positive nor negative but rather zero. The spatial geometry of the Einstein–de Sitter universe is Euclidean (infinite total volume), but space-time is not globally flat (i.e., not exactly the space-time of special relativity). Time again commences with a big bang and the galaxies recede forever, but the recession rate (Hubble’s “constant”) asymptotically coasts to zero as time advances to infinity. Because the geometry of space and the gross evolutionary properties are uniquely defined in the Einstein–de Sitter model, many people with a philosophical bent long considered it the most fitting candidate to describe the actual universe.

Bound and unbound universes and the closure density
relative size of the universe
relative size of the universeHow the relative size of the universe changes with time in four different models. The red line shows a universe devoid of matter, with constant expansion. Pink shows a collapsing universe, with six times the critical density of matter. Green shows a model favoured until 1998, with exactly the critical density and a universe 100 percent matter. Blue shows the currently favoured scenario, with exactly the critical density, of which 27 percent is visible and dark matter and 73 percent is dark energy.
The different separation behaviours of galaxies at large timescales in the Friedmann closed and open models and the Einstein–de Sitter model allow a different classification scheme than one based on the global structure of space-time. The alternative way of looking at things is in terms of gravitationally bound and unbound systems: closed models where galaxies initially separate but later come back together again represent bound universes; open models where galaxies continue to separate forever represent unbound universes; the Einstein–de Sitter model where galaxies separate forever but slow to a halt at infinite time represents the critical case.

The advantage of this alternative view is that it focuses attention on local quantities where it is possible to think in the simpler terms of Newtonian physics—attractive forces, for example. In this picture it is intuitively clear that the feature that should distinguish whether or not gravity is capable of bringing a given expansion rate to a halt depends on the amount of mass (per unit volume) present. This is indeed the case; the Newtonian and relativistic formalisms give the same criterion for the critical, or closure, density (in mass equivalent of matter and radiation) that separates closed or bound universes from open or unbound ones. If Hubble’s constant at the present epoch is denoted as H0, then the closure density (corresponding to an Einstein–de Sitter model) equals 3H02/8πG, where G is the universal gravitational constant in both Newton’s and Einstein’s theories of gravity. The numerical value of Hubble’s constant H0 is 22 kilometres per second per million light-years; the closure density then equals 10−29 gram per cubic centimetre, the equivalent of about six hydrogen atoms on average per cubic metre of cosmic space. If the actual cosmic average is greater than this value, the universe is bound (closed) and, though currently expanding, will end in a crush of unimaginable proportion. If it is less, the universe is unbound (open) and will expand forever. The result is intuitively plausible since the smaller the mass density, the smaller the role for gravitation, so the more the universe will approach free expansion (assuming that the cosmological constant is zero).

The mass in galaxies observed directly, when averaged over cosmological distances, is estimated to be only a few percent of the amount required to close the universe. The amount contained in the radiation field (most of which is in the cosmic microwave background) contributes negligibly to the total at present. If this were all, the universe would be open and unbound. However, the dark matter that has been deduced from various dynamic arguments is about 23 percent of the universe, and dark energy supplies the remaining amount, bringing the total average mass density up to 100 percent of the closure density.

The hot big bang
Wilkinson Microwave Anisotropy Probe
Wilkinson Microwave Anisotropy ProbeA full-sky map produced by the Wilkinson Microwave Anisotropy Probe (WMAP) showing cosmic background radiation, a very uniform glow of microwaves emitted by the infant universe more than 13 billion years ago. Colour differences indicate tiny fluctuations in the intensity of the radiation, a result of tiny variations in the density of matter in the early universe. According to inflation , these irregularities were the “seeds” that became the galaxies. WMAP's data support the big bang and inflation models, and cosmic microwave background is at the farthest limits of the observable universe.
Given the measured radiation temperature of 2.735 kelvins (K), the energy density of the cosmic microwave background can be shown to be about 1,000 times smaller than the average rest-energy density of ordinary matter in the universe. Thus, the current universe is matter-dominated. If one goes back in time to redshift z, the average number densities of particles and photons were both bigger by the same factor (1 + z)3 because the universe was more compressed by this factor, and the ratio of these two numbers would have maintained its current value of about one hydrogen nucleus, or proton, for every 109 photons. The wavelength of each photon, however, was shorter by the factor 1 + z in the past than it is now; therefore, the energy density of radiation increases faster by one factor of 1 + z than the rest-energy density of matter. Thus, the radiation energy density becomes comparable to the energy density of ordinary matter at a redshift of about 1,000. At redshifts larger than 10,000, radiation would have dominated even over the dark matter of the universe. Between these two values radiation would have decoupled from matter when hydrogen recombined. It is not possible to use photons to observe redshifts larger than about 1,090, because the cosmic plasma at temperatures above 4,000 K is essentially opaque before recombination. One can think of the spherical surface as an inverted “photosphere” of the observable universe. This spherical surface of last scattering probably has slight ripples in it that account for the slight anisotropies observed in the cosmic microwave background today. In any case, the earliest stages of the universe’s history—for example, when temperatures were 109 K and higher—cannot be examined by light received through any telescope. Clues must be sought by comparing the matter content with measured calculations.

For this purpose, fortunately, the cosmological evolution of model universes is especially simple and amenable to computation at redshifts much larger than 10,000 (or temperatures substantially above 30,000 K) because the physical properties of the dominant component, photons, then are completely known. In a radiation-dominated early universe, for example, the radiation temperature T is very precisely known as a function of the age of the universe, the time t after the big bang.

Primordial nucleosynthesis
evolution of the universe
evolution of the universeImmediately after the big bang (1), the universe was filled with a dense “soup” of subatomic particles (2), called quarks and leptons (such as electrons), and their antiparticle equivalents. By 0.01 second after the big bang (3), some of the quarks had united to form neutrons and protons. (After another 2 seconds, the only leptons remaining were electrons; the antiparticles had been annihilated.) After 3.5 minutes (4), hydrogen and helium nuclei had formed. After a million years (5), the universe was populated with hydrogen and helium atoms, the raw material of stars and galaxies. The initial radiation from the big bang had grown less energetic.
According to the considerations outlined above, at a time t less than 10-4 seconds, the creation of matter-antimatter pairs would have been in thermodynamic equilibrium with the ambient radiation field at a temperature T of about 1012 K. Nevertheless, there was a slight excess of matter particles (e.g., protons) compared to antimatter particles (e.g., antiprotons) of roughly a few parts in 109. This is known because, as the universe aged and expanded, the radiation temperature would have dropped and each antiproton and each antineutron would have annihilated with a proton and a neutron to yield two gamma rays; and later each antielectron would have done the same with an electron to give two more gamma rays. After annihilation, however, the ratio of the number of remaining protons to photons would be conserved in the subsequent expansion to the present day. Since that ratio is known to be one part in 109, it is easy to work out that the original matter-antimatter asymmetry must have been a few parts per 109.

In any case, after proton-antiproton and neutron-antineutron annihilation but before electron-antielectron annihilation, it is possible to calculate that for every excess neutron there were about five excess protons in thermodynamic equilibrium with one another through neutrino and antineutrino interactions at a temperature of about 1010 K. When the universe reached an age of a few seconds, the temperature would have dropped significantly below 1010 K, and electron-antielectron annihilation would have occurred, liberating the neutrinos and antineutrinos to stream freely through the universe. With no neutrino-antineutrino reactions to replenish their supply, the neutrons would have started to decay with a half-life of 10.6 minutes to protons and electrons (and antineutrinos). However, at an age of 1.5 minutes, well before neutron decay went to completion, the temperature would have dropped to 109 K, low enough to allow neutrons to be captured by protons to form a nucleus of heavy hydrogen, or deuterium. (Before that time, the reaction could still have taken place, but the deuterium nucleus would immediately have broken up under the prevailing high temperatures.) Once deuterium had formed, a very fast chain of reactions set in, quickly assembling most of the neutrons and deuterium nuclei with protons to yield helium nuclei. If the decay of neutrons is ignored, an original mix of 10 protons and two neutrons (one neutron for every five protons) would have assembled into one helium nucleus (two protons plus two neutrons), leaving more than eight protons (eight hydrogen nuclei). This amounts to a helium-mass fraction of 4/12 = 1/3—i.e., 33 percent. A more sophisticated calculation that takes into account the concurrent decay of neutrons and other complications yields a helium-mass fraction in the neighbourhood of 25 percent and a hydrogen-mass fraction of 75 percent, which are close to the deduced primordial values from astronomical observations. This agreement provides one of the primary successes of hot big bang .

The deuterium abundance
Not all of the deuterium formed by the capture of neutrons by protons would be further reacted to produce helium. A small residual can be expected to remain, the exact fraction depending sensitively on the density of ordinary matter existing in the universe when the universe was a few minutes old. The problem can be turned around: given measured values of the deuterium abundance (corrected for various effects), what density of ordinary matter needs to be present at a temperature of 109 K so that the nuclear reaction calculations will reproduce the measured deuterium abundance? The answer is known, and this density of ordinary matter can be expanded by simple scaling relations from a radiation temperature of 109 K to one of 2.735 K. This yields a predicted present density of ordinary matter and can be compared with the density inferred to exist in galaxies when averaged over large regions. The two numbers are within a factor of a few of each other. In other words, the deuterium calculation implies much of the ordinary matter in the universe has already been seen in observable galaxies. Ordinary matter cannot be the hidden mass of the universe.

The very early universe
Inhomogeneous nucleosynthesis
One possible modification concerns models of so-called inhomogeneous nucleosynthesis. The idea is that in the very early universe (the first microsecond) the subnuclear particles that later made up the protons and neutrons existed in a free state as a quark-gluon plasma. As the universe expanded and cooled, this quark-gluon plasma would undergo a phase transition and become confined to protons and neutrons (three quarks each). In laboratory experiments of similar phase transitions—for example, the solidification of a liquid into a solid—involving two or more substances, the final state may contain a very uneven distribution of the constituent substances, a fact exploited by industry to purify certain materials. Some astrophysicists have proposed that a similar partial separation of neutrons and protons may have occurred in the very early universe. Local pockets where protons abounded may have few neutrons and vice versa for where neutrons abounded. Nuclear reactions may then have occurred much less efficiently per proton and neutron nucleus than accounted for by standard calculations, and the average density of matter may be correspondingly increased—perhaps even to the point where ordinary matter can close the present-day universe. Unfortunately, calculations carried out under the inhomogeneous hypothesis seem to indicate that conditions leading to the correct proportions of deuterium and helium-4 produce too much primordial lithium-7 to be compatible with measurements of the atmospheric compositions of the oldest stars.

Matter-antimatter asymmetry
A curious number that appeared in the above discussion was the few parts in 109 asymmetry initially between matter and antimatter (or equivalently, the ratio 10−9 of protons to photons in the present universe). What is the origin of such a number—so close to zero yet not exactly zero?

At one time the question posed above would have been considered beyond the ken of physics, because the net “baryon” number (for present purposes, protons and neutrons minus antiprotons and antineutrons) was thought to be a conserved quantity. Therefore, once it exists, it always exists, into the indefinite past and future. Developments in particle physics during the 1970s, however, suggested that the net baryon number may in fact undergo alteration. It is certainly very nearly maintained at the relatively low energies accessible in terrestrial experiments, but it may not be conserved at the almost arbitrarily high energies with which particles may have been endowed in the very early universe.

An analogy can be made with the chemical elements. In the 19th century most chemists believed the elements to be strictly conserved quantities; although oxygen and hydrogen atoms can be combined to form water molecules, the original oxygen and hydrogen atoms can always be recovered by chemical or physical means. However, in the 20th century with the discovery and elucidation of nuclear forces, chemists came to realize that the elements are conserved if they are subjected only to chemical forces (basically electromagnetic in origin); they can be transmuted by the introduction of nuclear forces, which enter characteristically only when much higher energies per particle are available than in chemical reactions.

In a similar manner it turns out that at very high energies new forces of nature may enter to transmute the net baryon number. One hint that such a transmutation may be possible lies in the remarkable fact that a proton and an electron seem at first sight to be completely different entities, yet they have, as far as one can tell to very high experimental precision, exactly equal but opposite electric charges. Is this a fantastic coincidence, or does it represent a deep physical connection? A connection would obviously exist if it can be shown, for example, that a proton is capable of decaying into a positron (an antielectron) plus electrically neutral particles. Should this be possible, the proton would necessarily have the same charge as the positron, for charge is exactly conserved in all reactions. In turn, the positron would necessarily have the opposite charge of the electron, as it is its antiparticle. Indeed, in some sense the proton (a baryon) can even be said to be merely the “excited” version of an antielectron (an “antilepton”).

supernova 1987A in the Large Magellanic Cloud
supernova 1987A in the Large Magellanic CloudThis picture shows the faint outer rings and bright inner ring characteristic of an hourglass nebula.
Motivated by this line of reasoning, experimental physicists searched hard during the 1980s for evidence of proton decay. They found none and set a lower limit of 1032 years for the lifetime of the proton if it is unstable. This value is greater than what measured physicists had originally predicted on the basis of early unification schemes for the forces of nature. Later versions can accommodate the data and still allow the proton to be unstable. Despite the inconclusiveness of the proton-decay experiments, some of the apparatuses were eventually put to good astronomical use. They were converted to neutrino detectors and provided valuable information on the solar neutrino problem, as well as giving the first positive recordings of neutrinos from a supernova explosion (namely, supernova 1987A).

With respect to the cosmological problem of the matter-antimatter asymmetry, one measured approach is founded on the idea of a grand unified  (GUT), which seeks to explain the electromagnetic, weak nuclear, and strong nuclear forces as a single grand force of nature. This approach suggests that an initial collection of very heavy particles, with zero baryon and lepton number, may decay into many lighter particles (baryons and leptons) with the desired average for the net baryon number (and net lepton number) of a few parts per 109. This event is supposed to have occurred at a time when the universe was perhaps 10−35 second old.

Another approach to explaining the asymmetry relies on the process of CP violation, or violation of the combined conservation laws associated with charge conjugation (C) and parity (P) by the weak force, which is responsible for reactions such as the radioactive decay of atomic nuclei. Charge conjugation implies that every charged particle has an oppositely charged antimatter counterpart, or antiparticle. Parity conservation means that left and right and up and down are indistinguishable in the sense that an atomic nucleus emits decay products up as often as down and left as often as right. With a series of debatable but plausible assumptions, it can be demonstrated that the observed imbalance or asymmetry in the matter-antimatter ratio may have been produced by the occurrence of CP violation in the first seconds after the big bang. CP violation is expected to be more prominent in the decay of particles known as B-mesons. In 2010, scientists at the Fermi National Accelerator Laboratory in Batavia, Illinois, finally detected a slight preference for B-mesons to decay into muons rather than anti-muons.

Superunification and the Planck era
Planck length: why string  is hard to test
Planck length: why string  is hard to testThe Planck scale is described as the arena in which both quantum mechanical and gravitational effects come into play. Brian Greene explains where the Planck values come from. This video is an episode in his Daily Equation series.
See all videos for this article
Why should a net baryon fraction initially of zero be more appealing aesthetically than 10−9? The underlying motivation here is perhaps the most ambitious undertaking ever attempted in the history of science—the attempt to explain the creation of truly everything from literally nothing. In other words, is the creation of the entire universe from a vacuum possible?

The evidence for such an event lies in another remarkable fact. It can be estimated that the total number of protons in the observable universe is an integer 80 digits long. No one of course knows all 80 digits, but for the argument about to be presented, it suffices only to know that they exist. The total number of electrons in the observable universe is also an integer 80 digits long. In all likelihood these two integers are equal, digit by digit—if not exactly, then very nearly so. This inference comes from the fact that, as far as astronomers can tell, the total electric charge in the universe is zero (otherwise electrostatic forces would overwhelm gravitational forces). Is this another coincidence, or does it represent a deeper connection? The apparent coincidence becomes trivial if the entire universe was created from a vacuum since a vacuum has by definition zero electric charge. It is a truism that one cannot get something for nothing. The interesting question is whether one can get everything for nothing. Clearly, this is a very speculative topic for scientific investigation, and the ultimate answer depends on a sophisticated interpretation of what “nothing” means.

The words “nothing,” “void,” and “vacuum” usually suggest uninteresting empty space. To modern quantum physicists, however, the vacuum has turned out to be rich with complex and unexpected behaviour. They envisage it as a state of minimum energy where quantum fluctuations, consistent with the uncertainty principle of the German physicist Werner Heisenberg, can lead to the temporary formation of particle-antiparticle pairs. In flat space-time, destruction follows closely upon creation (the pairs are said to be virtual) because there is no source of energy to give the pair permanent existence. All the known forces of nature acting between a particle and antiparticle are attractive and will pull the pair together to annihilate one another. In the expanding space-time of the very early universe, however, particles and antiparticles may separate and become part of the observable world. In other words, sharply curved space-time can give rise to the creation of real pairs with positive mass-energy, a fact first demonstrated in the context of black holes by the English astrophysicist Stephen W. Hawking.

Yet Einstein’s picture of gravitation is that the curvature of space-time itself is a consequence of mass-energy. Now, if curved space-time is needed to give birth to mass-energy and if mass-energy is needed to give birth to curved space-time, which came first, space-time or mass-energy? The suggestion that they both rose from something still more fundamental raises a new question: What is more fundamental than space-time and mass-energy? What can give rise to both mass-energy and space-time? No one knows the answer to this question, and perhaps some would argue that the answer is not to be sought within the boundaries of natural science.

Hawking and the American cosmologist James B. Hartle have proposed that it may be possible to avert a beginning to time by making it go imaginary (in the sense of the mathematics of complex numbers) instead of letting it suddenly appear or disappear. Beyond a certain point in their scheme, time may acquire the characteristic of another spatial dimension rather than refer to some sort of inner clock. Another proposal states that, when space and time approach small enough values (the Planck values; see below), quantum effects make it meaningless to ascribe any classical notions to their properties. The most promising approach to describe the situation comes from the  of “superstrings.”

Superstrings represent one example of a class of attempts, generically classified as superunification , to explain the four known forces of nature—gravitational, electromagnetic, weak, and strong—on a single unifying basis. Common to all such schemes are the postulates that quantum mechanics and special relativity underlie the measured framework. Another common feature is supersymmetry, the notion that particles with half-integer values of the spin angular momentum (fermions) can be transformed into particles with integer spins (bosons).

The distinguishing feature of superstring  is the postulate that elementary particles are not mere points in space but have linear extension. The characteristic linear dimension is given as a certain combination of the three most fundamental constants of nature: (1) Planck’s constant h (named after the German physicist Max Planck, the founder of quantum physics), (2) the speed of light c, and (3) the universal gravitational constant G. The combination, called the Planck length (Gh/c3)1/2, equals roughly 10−33 cm, far smaller than the distances to which elementary particles can be probed in particle accelerators on Earth.

The energies needed to smash particles to within a Planck length of each other were available to the universe at a time equal to the Planck length divided by the speed of light. This time, called the Planck time (Gh/c5)1/2, equals approximately 10−43 second. At the Planck time, the mass density of the universe is thought to approach the Planck density, c5/hG2, roughly 1093 grams per cubic centimetre. Contained within a Planck volume is a Planck mass (hc/G)1/2, roughly 10−5 gram. An object of such mass would be a quantum black hole, with an event horizon close to both its own Compton length (distance over which a particle is quantum mechanically “fuzzy”) and the size of the cosmic horizon at the Planck time. Under such extreme conditions, space-time cannot be treated as a classical continuum and must be given a quantum interpretation.

The latter is the goal of the superstring , which has as one of its features the curious notion that the four space-time dimensions (three space dimensions plus one time dimension) of the familiar world may be an illusion. Real space-time, in accordance with this picture, has 26 or 10 space-time dimensions, but all of these dimensions except the usual four are somehow compacted or curled up to a size comparable to the Planck scale. Thus has the existence of these other dimensions escaped detection. It is presumably only during the Planck era, when the usual four space-time dimensions acquire their natural Planck scales, that the existence of what is more fundamental than the usual ideas of mass-energy and space-time becomes fully revealed. Unfortunately, attempts to deduce anything more quantitative or physically illuminating from the  have bogged down in the intractable mathematics of this difficult subject. At the present time superstring  remains more of an enigma than a solution.

Inflation
One of the more enduring contributions of particle physics to cosmology is the prediction of inflation by the American physicist Alan Guth and others. The basic idea is that at high energies matter is better described by fields than by classical means. The contribution of a field to the energy density (and therefore the mass density) and the pressure of the vacuum state need not have been zero in the past, even if it is today. During the time of superunification (Planck era, 10−43 second) or grand unification (GUT era, 10−35 second), the lowest-energy state for this field may have corresponded to a “false vacuum,” with a combination of mass density and negative pressure that results gravitationally in a large repulsive force. In the context of Einstein’s  of general relativity, the false vacuum may be thought of alternatively as contributing a cosmological constant about 10100 times larger than it can possibly be today. The corresponding repulsive force causes the universe to inflate exponentially, doubling its size roughly once every 10−43 or 10−35 second. After at least 85 doublings, the temperature, which started out at 1032 or 1028 K, would have dropped to very low values near absolute zero. At low temperatures the true vacuum state may have lower energy than the false vacuum state, in an analogous fashion to how solid ice has lower energy than liquid water. The supercooling of the universe may therefore have induced a rapid phase transition from the false vacuum state to the true vacuum state, in which the cosmological constant is essentially zero. The transition would have released the energy differential (akin to the “latent heat” released by water when it freezes), which reheats the universe to high temperatures. From this temperature bath and the gravitational energy of expansion would then have emerged the particles and antiparticles of noninflationary big bang cosmologies.

Cosmic inflation serves a number of useful purposes. First, the drastic stretching during inflation flattens any initial space curvature, and so the universe after inflation will look exceedingly like an Einstein–de Sitter universe. Second, inflation so dilutes the concentration of any magnetic monopoles appearing as “topological knots” during the GUT era that their cosmological density will drop to negligibly small and acceptable values. Finally, inflation provides a mechanism for understanding the overall isotropy of the cosmic microwave background because the matter and radiation of the entire observable universe were in good thermal contact (within the cosmic event horizon) before inflation and therefore acquired the same thermodynamic characteristics. Rapid inflation carried different portions outside their individual event horizons. When inflation ended and the universe reheated and resumed normal expansion, these different portions, through the natural passage of time, reappeared on our horizon. And through the observed isotropy of the cosmic microwave background, they are inferred still to have the same temperatures. Finally, slight anisotropies in the cosmic microwave background occurred because of quantum fluctuations in the mass density. The amplitudes of these small (adiabatic) fluctuations remained independent of comoving scale during the period of inflation. Afterward they grew gravitationally by a constant factor until the recombination era. Cosmic microwave photons seen from the last scattering surface should therefore exhibit a scale-invariant spectrum of fluctuations, which is exactly what the Cosmic Background Explorer satellite observed.

As influential as inflation has been in guiding modern cosmological thought, it has not resolved all internal difficulties. The most serious concerns the problem of a “graceful exit.” Unless the effective potential describing the effects of the inflationary field during the GUT era corresponds to an extremely gently rounded hill (from whose top the universe rolls slowly in the transition from the false vacuum to the true vacuum), the exit to normal expansion will generate so much turbulence and inhomogeneity (via violent collisions of “domain walls” that separate bubbles of true vacuum from regions of false vacuum) as to make inexplicable the small observed amplitudes for the anisotropy of the cosmic microwave background radiation. Arranging a tiny enough slope for the effective potential requires a degree of fine-tuning that most cosmologists find philosophically objectionable.

Steady state  and other alternative cosmologies
Big bang cosmology, augmented by the ideas of inflation, remains the  of choice among nearly all astronomers, but, apart from the difficulties discussed above, no consensus has been reached concerning the origin in the cosmic gas of fluctuations thought to produce the observed galaxies, clusters, and superclusters. Most astronomers would interpret these shortcomings as indications of the incompleteness of the development of the , but it is conceivable that major modifications are needed.

An early problem encountered by big bang theorists was an apparent large discrepancy between the Hubble time and other indicators of cosmic age. This discrepancy was resolved by revision of Hubble’s original estimate for H0, which was about an order of magnitude too large owing to confusion between Population I and II variable stars and between H II regions and bright stars. However, the apparent difficulty motivated Bondi, Hoyle, and Gold to offer the alternative  of steady state cosmology in 1948.

By that year, of course, the universe was known to be expanding; therefore, the only way to explain a constant (steady state) matter density was to postulate the continuous creation of matter to offset the attenuation caused by the cosmic expansion. This aspect was physically very unappealing to many people, who consciously or unconsciously preferred to have all creation completed in virtually one instant in the big bang. In the steady state  the average age of matter in the universe is one-third the Hubble time, but any given galaxy could be older or younger than this mean value. Thus, the steady state  had the virtue of making very specific predictions, and for this reason it was vulnerable to observational disproof.

The first blow was delivered by British astronomer Martin Ryle’s counts of extragalactic radio sources during the 1950s and ’60s. These counts involved the same methods discussed above for the star counts by Dutch astronomer Jacobus Kapteyn and the galaxy counts by Hubble except that radio telescopes were used. Ryle found more radio galaxies at large distances from Earth than can be explained under the assumption of a uniform spatial distribution no matter which cosmological model was assumed, including that of steady state. This seemed to imply that radio galaxies must evolve over time in the sense that there were more powerful sources in the past (and therefore observable at large distances) than there are at present. Such a situation contradicts a basic tenet of the steady state , which holds that all large-scale properties of the universe, including the population of any subclass of objects like radio galaxies, must be constant in time.

The second blow came in 1965 with the announcement of the discovery of the cosmic microwave background radiation. Though it has few adherents today, the steady state  is credited as having been a useful idea for the development of modern cosmological thought as it stimulated much work in the field.

At various times, other alternative theories have also been offered as challenges to the prevailing view of the origin of the universe in a hot big bang: the cold big bang  (to account for galaxy formation), symmetric matter-antimatter cosmology (to avoid an asymmetry between matter and antimatter), variable G cosmology (to explain why the gravitational constant is so small), tired-light cosmology (to explain redshift), and the notion of shrinking atoms in a nonexpanding universe (to avoid the singularity of the big bang). The motivation behind these suggestions is, as indicated in the parenthetical comments, to remedy some perceived problem in the standard picture. Yet, in most cases, the cure offered is worse than the disease, and none of the mentioned alternatives has gained much of a following. The hot big bang  has ascended to primacy because, unlike its many rivals, it attempts to address not isolated individual facts but a whole panoply of cosmological issues. And, although some sought-after results remain elusive, no glaring weakness has yet been uncovered.

Frank H. Shu   Project Data Management Plan 
THM-SYS-012 
07/15/2004 
Timothy Quinn, THEMIS Science Operations Manager 
Dr. Tai Phan, THEMIS Data Analysis Software Lead 
Dr. Manfred Bester, THEMIS Mission Operations Manager 
Dr. Ellen Taylor, THEMIS Mission Systems Engineer 
Peter Harvey, THEMIS Project Manager 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Document Revision Record 
Rev. 
Date 
1 
12/03/2003 
Description of Change 
Preliminary Draft 
Approved By 
2 
06/07/2004 
Second Draft 
3 
06/13/2004 
Signature Version --
See Signatories 
Distribution List 
Name 
Timothy Quinn, U.C. Berkeley 
Dr. Tai Phan, U.C. Berkeley 
Dr. Manfred Bester, U.C. Berkeley 
Dr. Ellen Taylor, U.C. Berkeley 
Peter Harvey, U.C. Berkeley 
Dr. Vassilis Angelopoulos, U.C. Berkeley 
Dr. Dave Sibeck, NASA GSFC 
Dr. William Peterson, NASA Headquarters 
Email 
teq@ssl.berkeley.edu 
phan@ssl.berkeley.edu 
manfred@ssl.berkeley.edu 
ertaylor@ssl.berkeley.edu 
prh@ssl.berkeley.edu 
vassilis@ssl.berkeley.edu 
david.g.sibeck@nasa.gov 
william.k.Peterson@nasa.gov 
TBD List 
Identifier 
Description 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
   
    
   
    
    
      
     
     
     
     
     
    
     
        
        
         
    
      
       
        
     
    
     
     
    
    
     
      
     
    
    
      
       
Table of Contents 
DOCUMENT REVISION RECORD...............................................................................................2 
DISTRIBUTION LIST.....................................................................................................................2 
TBD LIST.........................................................................................................................................2 
1. INTRODUCTION....................................................................................................................7 
1.1 Purpose and Scope................................................................................................................7 
1.2 Applicable Documents. ........................................................................................................7 
2. PROJECT OVERVIEW...........................................................................................................8 
2.1 Science Objectives................................................................................................................8 
2.2 Mission Summary.................................................................................................................8 
3. PROBE DESCRIPTION........................................................................................................10 
3.1 Overview ............................................................................................................................10 
3.2 Subsystem Descriptions......................................................................................................11 
3.2.1 RF and Communications Subsystem (RFCS)............................................................................ 11 
3.2.2 Guidance Navigation and Control (GN&C) .............................................................................. 11 
3.2.3 Command and Data Handling Subsystem (CDHS)................................................................... 11 
3.2.4 Power. ........................................................................................................................................ 12 
3.2.5 Structural/Mechanical & Thermal. ............................................................................................ 12 
3.2.6 Bus Avionics Unit (BAU). ........................................................................................................ 13 
3.2.7 Probe Carrier Configuration and Launch. ................................................................................. 13 
4. INSTRUMENT DESCRIPTIONS.........................................................................................14 
4.1 Overview ............................................................................................................................14 
4.2 Fluxgate Magnetometer......................................................................................................14 
4.2.1 Science Requirements................................................................................................................ 14 
4.2.2 Specification .............................................................................................................................. 15 
4.2.3 Calibration ................................................................................................................................. 15 
4.2.4 Boom Deployment..................................................................................................................... 15 
4.3 Electrostatic Analyzers (ESA)............................................................................................16 
4.3.1 Science Requirements................................................................................................................ 16 
4.3.2 Specifications............................................................................................................................. 16 
4.3.3 Calibration ................................................................................................................................. 16 
4.3.4 Aperture Cover Release............................................................................................................. 17 
4.4 Solid State Telescope (SST)...............................................................................................17 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
    
    
    
     
      
     
    
    
     
       
     
    
    
     
        
     
       
       
      
    
     
       
        
     
       
     
       
    
     
    
       
       
    
      
4.4.1 Science Requirements................................................................................................................ 17 
4.4.2 Specifications............................................................................................................................. 17 
4.4.3 Calibration ................................................................................................................................. 17 
4.4.4 Attenuator Operation ................................................................................................................. 18 
4.5 Search Coil Magnetometer.................................................................................................18 
4.5.1 Science Requirements................................................................................................................ 18 
4.5.2 Specifications............................................................................................................................. 18 
4.5.3 Calibration ................................................................................................................................. 18 
4.5.4 Boom Deployment..................................................................................................................... 18 
4.6 Electric Field Instrument (EFI) ..........................................................................................19 
4.6.1 Science Requirements................................................................................................................ 19 
4.6.2 Specifications............................................................................................................................. 19 
4.6.3 Calibration ................................................................................................................................. 20 
4.6.4 Deployment Operations ............................................................................................................. 20 
4.7 Instrument Data Processing Unit (IDPU)...........................................................................20 
4.8 Ground Observations..........................................................................................................21 
4.8.1 Ground Based Observatories (GBO) ......................................................................................... 21 
4.8.2 E/PO Ground Magnetometers (E/PO-GMAGS) ....................................................................... 27 
5. GROUND DATA SYSTEM (GDS) DESCRIPTION ...........................................................28 
5.1 Overview ............................................................................................................................28 
5.2 Ground Stations..................................................................................................................28 
5.2.1 Berkeley Ground Station (BGS)................................................................................................ 28 
5.2.2 Secondary and Backup Ground Stations ................................................................................... 28 
5.2.3 Telemetry Files .......................................................................................................................... 29 
5.3 Mission Operations Center (MOC)....................................................................................29 
5.3.1 Mission Operations.................................................................................................................. 29 
5.4 Flight Dynamics Center (FDC)..........................................................................................31 
5.4.1 Overview.................................................................................................................................... 31 
5.4.2 Software Tools........................................................................................................................... 31 
5.4.3 Operations.................................................................................................................................. 31 
5.5 Flight Operations Team (FOT)...........................................................................................32 
5.6 Science Operations Center (SOC)......................................................................................32 
5.6.1 Overview.................................................................................................................................... 32 
6. PROJECT DATA FLOW.......................................................................................................33 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
    
      
       
          
        
         
        
        
          
        
        
    
       
          
           
          
             
  
        
         
         
            
    
       
       
         
     
      
    
     
     
        
      
      
6.1 Overview ............................................................................................................................33 
6.2 Probe Instrument Data........................................................................................................33 
6.2.1 Collection – Time T0................................................................................................................. 34 
6.2.2 Recovery – Time T1 (T0+8 Days Maximum)........................................................................... 34 
6.2.3 Delivery to SOC – T1+1Hr ....................................................................................................... 34 
6.2.4 Level Zero Processing – Time T1+2Hrs ................................................................................... 35 
6.2.5 CDF Processing – Time T1+3Hrs ............................................................................................. 35 
6.2.6 Diagnostic Plot Creation – T1+4Hrs ......................................................................................... 35 
6.2.7 Browse/Key Parameter (K0) Data Creation – T1+24Hrs.......................................................... 35 
6.2.8 K1 Data Creation – T1+1Month................................................................................................ 35 
6.2.9 K2 Data Creation – T1+6Months .............................................................................................. 36 
6.3 GBO Data...........................................................................................................................37 
6.3.1 Collection – Time T0................................................................................................................. 37 
6.3.2 Thumbnail Image Recovery by UC – T0+1min ........................................................................ 38 
6.3.3	 Health and Safety (H&S) Data Recovery – T0+1min ............................................................... 38 
6.3.4	 Thumbnail Image Copied to UCB – T0+5mins ........................................................................ 38 
6.3.5	 Raw Magnetometer Data Recovered by UC and Copied to UCLA, UA, and UCB -T0+27hrs 
39 
6.3.6	 UCLA Produces Processed GMAG Data – T0+28hrs .............................................................. 39 
6.3.7	 Keogram Recovery and Distribution – T0+30Hrs..................................................................... 39 
6.3.8	 Inclusion in Key Parameter Data – T0+30Hrs .......................................................................... 39 
6.3.9	 Recovery and Distribution of Full Resolution Images – T0+6Months ..................................... 39 
6.4 E/PO GMAG Data..............................................................................................................39 
6.4.1 Collection – Time T0................................................................................................................. 39 
6.4.2 Recovery – Time T0+27Hrs ...................................................................................................... 39 
6.4.3 Processed GMAG Data Produced and Distributed.................................................................... 39 
6.5 SPASE Collaboration.........................................................................................................39 
7. INSTRUMENT COMMAND AND CONTROL ..................................................................40 
7.1.1 Overview.................................................................................................................................... 40 
7.1.2 Instrument Commissioning........................................................................................................ 40 
7.1.3 Normal Operations..................................................................................................................... 40 
7.2 GBO Installation, Monitoring, Control, and Maintenance.................................................41 
7.3 E/PO GMAG Control.........................................................................................................41 
8. SCIENCE DATA PRODUCTS .............................................................................................42 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
       
       
      
      
    
    
       
    
           
    
     
    
      
       
       
    
      
      
      
 
8.1 Instrument Data CDF Files.................................................................................................42 
8.2 Instrument Data Calibration Files ......................................................................................42 
8.3 Key Parameter Data............................................................................................................43 
8.4 GBO Data and Products .....................................................................................................44 
8.4.1 ASI............................................................................................................................................. 44 
8.4.2 GMAG ....................................................................................................................................... 44 
8.5 E/PO GMAG Data Products...............................................................................................44 
9. DATA ACCESS.....................................................................................................................46 
9.1 THEMIS Data Analysis Software Package File Search Tool ............................................46 
9.2 Website...............................................................................................................................46 
10. DATA ANALYSIS SOFTWARE......................................................................................46 
10.1 Overview......................................................................................................................46 
10.2 File Search Tool ...........................................................................................................47 
10.3 Moments & Fields Tool ...............................................................................................47 
10.4 Reading and Writing Tools ..........................................................................................47 
11. DATA ARCHIVING AND DISTRIBUTION...................................................................47 
12. APPENDIX A. INSTRUMENT DATA QUANTITIES....................................................49 
13. APPENDIX B. INSTRUMENT DATA RATES...............................................................50 
14. APPENDIX C. INSTRUMENT DATA VOLUMES ........................................................51 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
1. Introduction. 
1.1 Purpose and Scope. 
This document provides the Project Data Management Plan (PDMP) for the Time History of Events and 
Macroscale Interactions during Substorms (THEMIS) Explorer Mission. The PDMP describes all of the 
activities associated with the flow of THEMIS scientific data from collection on the spacecraft through 
production, distribution and access, and archiving of data and data products. This also includes extensive 
ground based imager and magnetometer measurements taken by 20 Ground Based Observatories (GBO's) 
spread across Alaska and Canada, and 10 Education and Public Outreach (E/PO) magnetometers spread 
across the northern continental United States. 
1.2 Applicable Documents. 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
THM-SYS-102 
THM-SYS-115 
THM-SYS-116 
THM-SYS-114 
THM-SYS-013 
THM-SYS-018 
THM-SYS-019 
THEMIS Command Format Specification 
THEMIS Telemetry Data Format Specification 
THEMIS Telemetry Data Packet Format Specification 
THEMIS Radio Frequency Interface Control Document 
THEMIS Mission Operations Plan 
THEMIS Launch and Early Orbit Operations Plan 
THEMIS Contingency Plan 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
2. Project Overview. 
2.1 Science Objectives. 
The primary objective for the THEMIS project is to understand the onset and macroscale evolution of 
magnetospheric substorms. A substorm is an instability in the circulation of magnetic flux and plasma 
through the solar wind magnetospheric system ultimately linked to the familiar auroral eruptions on the 
Earth's polar ionosphere. Understanding the substorm instability is crucial for space science, basic plasma 
physics, and space weather, and has been identified by the National Research Council (NRC) as one of the 
main strategic questions in space physics. THEMIS will determine for the first time when and where in the 
magnetosphere substorms start, and how they evolve macroscopically. It will do so by timing well-known 
plasma particle and field signatures at several locations in the Earth’s magnetotail while simultaneously 
determining the time and location of substorm onset at Earth using a dense network of ground 
observatories. 
? 
Rarefaction wave 
? 
Flows 
GBO 
2.2 Mission Summary. 
P3 
P4
P5 
Figure 1. Science Objectives 
P2
P1
The THEMIS science objectives are achieved by five space probes, P1 – P5, in High Earth Orbits (HEO) 
with similar perigee altitudes (1.16 to 1.5 earth radii, Re) and varying apogee altitudes. P1 has an apogee of 
~30 Re, P2 at ~20 Re, and P3 - P5 at ~12 Re, with corresponding orbital periods of ~4, 2, and 1 days, 
respectively. This choice of periods results in multi-point conjunctions at apogee, allowing the probes to 
simultaneously measure substorm signatures over long distances along the magneto tail, while simplifying 
ground communications and scheduling. The probe conjunctions are tightly coordinated with the ground
based observatories within a 4-month primary observing season per year, centered on mid-February and 
carried out each year during a 2-year baseline mission. A store-and-forward data flow scheme retrieves 
prime conjunction plasma and fields data during substorm events with simple, automated science 
operations. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
The ground observations will be carried out by 20 Ground Based Observatories (GBO) spread across 
Alaska and Canada. Each GBO will use an All Sky Imager (ASI - camera) and ground magnetometer 
(GMAG) to monitor the auroral light and ionospheric currents in order to localize the time, location, and 
evolution of the auroral manifestation of the substorm. A second ground network will include 10 Education 
and Public Outreach (E/PO) Ground Magnetometers located in schools at sub-auroral latitudes in the U.S. 
Launch 
Vehicle: Delta II Eastern Range 
Injection: 1.1x12Re, 9 degrees inclination 
Date: October, 2006 
Space Segment 
Spacecraft: 5 spinning probes with fuel for orbit/attitude adjust 
Instruments: 3-Axis E-Field and B-Field, 3-D Ion and electron particle 
detectors 
Orbit Periods: 1, 2, and 4 days 
Spin Axis Orientation: Ecliptic normal 
Ground Segment 
Ground Based Observatories (GBO): 20 sites in Alaska (4) and Canada (16) 
containing All Sky Imagers (ASI) and Ground Magnetometers (GMAG) 
E/PO GMAGS: 10 GMAGS placed in schools located in Northern Latitude U.S. 
Operations 
Phases: I&T, L&EO (2 mo), Campaigns (December-March), De-orbit 
Lifetime: 2 years 
Table 1. THEMIS Mission Summary 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
3. Probe Description. 
3.1 Overview 
THEMIS employs 5 simple, identical, high heritage space probes (P1, P2, P3, P4, & P5) in coordinated 
orbits. Each probe consists of the probe bus (probe) and the instrument suite. The probe bus subsystems 
include Structural/Mechanical, Thermal, Power, RF and Communications (RFCS), Command and Data 
Handling (CDHS), and Guidance Navigation & Control (GN&C). The GNCS consists of the Attitude 
Control Subsystem (ACS) and the Reaction Control (propulsion) Subsystem (RCS). The electronics 
associated with the Power, CDHS, ACS, and RCS reside in the Bus Avionics Unit (BAU). The probe bus 
has a simple, low-rate S-band communications system with a store-and-forward (near perigee) strategy. It 
is supported by the General Dynamics ColdFire processor, hosting heritage software to perform data 
handling and minor fault detection activities. The power system is comprised of simple body-mounted solar 
panels and a small battery charged by a direct energy transfer controller. The probes are spin-stabilized and 
the Attitude Control  System (ACS) uses a fault-tolerant cross-strapped monopropellant hydrazine blow
down system to control orbit, spin rate, and spin axis attitude. ACS is simplified by ground based attitude 
determination performed at UCB by the Flight Dynamics Center (FDC). All maneuver sequences are 
planned, checked (via a spacecraft simulator), uploaded, and executed during real-time ground 
communications. 
Figure 2. THEMIS Probe 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
   
     
             
    
               
               
          
               
                   
         
 
    
   
     
        
                         
   
     
          
        
    
    
        
  
      
            
  
 
         
         
             
       
               
     
    
 
   
 
                   
             
               
          
       
             
            
          
3.2 Subsystem Descriptions. 
3.2.1 RF and Communications Subsystem (RFCS). 
The RFCS utilizes a NASA standard 5-Watt S-Band transponder for Command and Telemetry 
communications with a single cylindrical FAST-like antenna with a toroidal gain pattern. The transponder 
allows two-way Doppler ranging for accurate orbit determination. All probes use the same frequency pair 
for telemetry and commanding. Communications are established with one probe at a time. Command and 
telemetry protocols for the probes follow standard CCSDS procedures [1]. Downlink telemetry rates are 
selectable and available to optimize probe monitoring and telemetry recovery as a function of probe range. 
The nominal rate is 524.288 kbps, and the expected data volume during a science dump is 480 -640 Mbits. 
The command uplink rate is fixed at 1 kbps. 
Frequency Downlink – 2282.5 MHz 
Uplink – 2101.8 MHz 
Polarization Left-Hand Circular Polarized (LHCP) 
Modulation Downlink – BPSK (4 highest data rates) 
PCM/PSK/PM (6 lowest data rates) 
Uplink -PCM/PSK/PM 
Encoding Downlink – Reed-Solomon + Rate-1/2 Convolution 
Compression Scheme (VC3 only) Differencing and truncation or Huffman (TBR) 
Bit Rates 1.024, 4.096, 8.192, 16.384, 32.768, 65.536, 
131.072, 262.144, 524.288, 1048.576 kbps 
Virtual Channels 0-3, 6 
Data Volume per Orbit per Probe 480 -640 Mbits 
Table 2. RF and Communications Subsystem Summary 
3.2.2 Guidance Navigation and Control (GN&C) 
The GN&C subsystem includes the Attitude Control Subsystem (ACS) and the Reaction Control 
Subsystem (RCS). 
The ACS utilizes a thruster interface driven by ground-processed estimation and command algorithms with 
on-board limit and time-out protection. Attitude data collected from a Miniature Spinning Sun Sensor 
(MSSS) and the science Fluxgate Magnetometer (FGM) are sampled at 10 Hz and telemetered to the 
ground for standard, 3-axis, post-processing estimation. Ground generated thruster command sequences are 
tested in a high-fidelity probe simulator (I&T test bed migrated to MOC) prior to any upload. Also, two 
single-axis gyros, transverse to the spin plane, provide short-term attitude verification (prior to orbit 
maneuvers). The on-board protection logic monitors real-time sun aspect angle and spin period, comparing 
them to a ground commanded reference uploaded for each maneuver. If thresholds are exceeded, the 
maneuver is terminated. 
The RCS includes two fuel tanks, a pressurization tank, a pyro valve, two latch valves, fuel line and fuel 
filters, and four 5-N thrusters: 2 oriented axially, (Both along +Z) for primary orbit placement ΔV and 
attitude control; and 2 oriented tangentially for spin up/down control and minor ΔV side thrusting for orbit 
fine tuning. The minimum thruster pulse duration is 50 milliseconds. 
3.2.3 Command and Data Handling Subsystem (CDHS). 
The CDHS provides real-time and stored command capability for the bus subsystems and instruments, 
collects, formats, and transmits to the ground data from the bus subsystems and instruments, provides 
engineering data storage, distributes time to the IDPU, and implements autonomous fault protection 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
                
        
 
                
        
           
           
         
              
      
 
     
               
 
 
           
          
              
        
 
   
  
       
  
    
   
   
  
  
                 
           
                  
 
      
 
          
      
   
                     
            
                 
                
                  
              
          
features to ensure the health and safety of the probe. The CDHS functions are implemented in flight 
software and hardware that reside in the BAU. 
CDHS receives uplink commands from the RFCS at a fixed rate of 100 bps using CCSDS telecommand 
protocols that guarantee correct, in-sequence delivery of variable-length command packets (embedded in 
command transfer frames) to the probe. Command transfer frames are authenticated. The CDHS is capable 
of accepting hardware commands (commands that do not require processor involvement) to perform critical 
operations such as hardware reconfiguration from the ground. Stored command capability in the form of 
Absolute Time Sequence (ATS) and Relative Time Sequence (RTS) loads is available for controlling the 
probe and instruments outside of a ground station contact. 
CDHS data may involve real-time engineering, playback engineering, and real-time or playback science 
(from the IDPU). The CDHS collects and packetizes engineering data from the bus subsystem and 
instruments and either delivers these data to the RFCS in real-time for downlink (VC0), or stores the data 
locally in the BAU for playback at a later time (VC1).The CDHS will also route real-time (VC2) and stored 
(VC3) science data to the RFCS for downlink. The THEMIS telemetry format is based on CCSDS 
standards and data structures. The telemetry link is encoded using concatenated rate-1/2 (K=7) 
convolutional and Reed-Solomon (255,223,I=5) coding to allow for error correction. Also, the VC3 packet 
data are compressed, as described in [3]. 
VC ID Description 
0 Real-time Engineering Data (Probe and Instruments) 
1 Stored Engineering Data (Probe and Instruments) 
2 Real-time Science Data 
3 Stored Science Data 
6 Event Data 
7 Fill Data 
Table 3. Virtual Channel Summary 
3.2.4 Power. 
The Power Subsystem is a Direct Energy Transfer (DET) system with the battery and solar array connected 
directly to the power bus. The solar array consists of eight panels, four on each side, and two on each deck. 
At nominal attitudes, approximately 59 Watts EOL are provided by the side panels and 21 Watts EOL by 
the top and bottom panels. Accounting for battery recharging, increased eclipse heater power, and power 
control efficiencies, the minimum load power available is 41.7 Watts, easily achieving energy balance for 
the required load power of 29.2 Watts. Eclipse and peak transient loads (i.e., transmitter operation) are 
balanced with an 11.8 A-hr, 28V Lithium-Ion battery. Thermal management using heaters and thermistors 
keeps the battery temperature at -5 to +25 degrees C. 
3.2.5 Structural/Mechanical & Thermal. 
The probe consists of a lower deck, an upper deck, and four corner and side panels. The lower deck is the 
primary mounting surface for most of the instruments and probe components. The upper deck, corner, and 
side panels close out the probe internal cavity. The FGM and SCM mount to the upper deck; solar cells 
utilize the exterior surface of the probe side panels as their substrate. The ESA and SST instruments, the 
sun sensor, and thruster brackets mount to two of the corner panels for a clear Field of View (FOV). The 
mechanical and thermal designs provide a low conductance composite structure for isolation of the body
mounted solar panels, minimizing thermal energy effects between full-sun and shadow operations. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
3.2.6 Bus Avionics Unit (BAU). 
The BAU includes a SMEX-Lite heritage uplink/downlink communications card, a processor card 
(identical to the IDPU processor card), and a Direct Energy Transfer (DET) power control card with 
SMEX-Lite and EO-1 heritage. The flight software is derived from prior SMEX mission modules (in C
language) and is hosted by the heritage CMX-RTX Real-Time Operating System (RTOS). Instrument and 
bus housekeeping data are stored in the local bus memory, while science data are stored in the IDPU. 
During a ground station contact, housekeeping data are transmitted directly by the BAU while science data 
stored in the IDPU memory are sent to the BAU. The latter in turn merges these into the telemetry stream 
(bent pipe flow), similar to the FAST implementation. 
3.2.7 Probe Carrier Configuration and Launch. 
THEMIS will use a standard Delta sequence to directly inject the Probe Carrier Assembly (PCA) into the 
target insertion orbit. The PCA does not separate from the third stage. The probes separate from the probe 
carrier immediately after third stage burnout and yo-yo despin. The probes are electrically independent; 
each imitates separation based on built-in sequence timers and ELV separation signals, thereby eliminating 
any credible single point failure. For contingency, the separation can also be initiated by ground command. 
Multiple timers (hardware and software) are provided to protect against premature probe separation. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4. Instrument Descriptions. 
4.1 Overview 
Each probe contains an instrument complement that will measure DC and AC electric and magnetic fields 
as well as electron and ion energies and distributions. A detailed list of the instrument data quantities, data 
rates, and data volume is given in Appendix A, B, and C, respectively. The instruments and their probe 
placement and configuration are detailed in Figure 3 below. 
Figure 3. Instrument/Probe Configuration 
4.2 Fluxgate Magnetometer 
A tri-axial fluxgate magnetometer will measure the 3D ambient magnetic field in the frequency bandwidth 
from DC to 64 Hz (Nyquist). 
4.2.1 Science Requirements 
1) Measure DC and low frequency perturbations of the magnetic field 
2) Time wave and structure propagation between probes 
3) Provide information on plasma currents based on instantaneous magnetic field differences on two or 
more probes, separated by >0.2 Re. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4.2.2 Specification 
The unit consists of two orthogonal ring core elements of different diameter, fixed with a bobbin. The unit 
is mounted on a 2-meter double-hinge carbon epoxy boom. The electronics consist of the driver and control 
circuits on a board within the IDPU. The controller controls digital excitation, data acquisition, feedback, 
and compensation, making the device low power. Its low noise permits easy inter-calibration with the 
search-coil magnetometer at frequencies of approximately 10 Hz. 
Figure 4. Fluxgate Magnetometer 
4.2.3 Calibration 
Although a 1 nT absolute accuracy requirement is achievable with independent sensor calibration, it is 
important to ascertain that two separate probes provide identical values when properties of the medium are 
steady. As required (near each apogee, perigee or both), calibration data will be collected at 32 Hz to 
determine (on individual probes) zero levels, gains, and sensor orientation. The magnetometers on all 5 
probes will also be inter-calibrated during the early part of the mission (L&EO) using traversals of current
free (or low current density) regions of the magnetosphere. Also, as required during the second year of the 
mission, magnetometer data from probes P3, P4 and P5 will be collected at high rates outside of burst
mode triggers, in order to perform inter-calibration of their relative orientation and offsets in current-free 
regions. The validity of a divergence-free assumption (a measured necessity) will be used to ascertain the 
validity of the current-free approximation. If the divergence-free approximation cannot be easily met then 
time-tagged data from the probes traversing the same region will be compared for trend-recognition after 
long-term averaging. 
4.2.4 Boom Deployment 
During L&EO, once the FGM and SCM are operating, the magnetometer booms will be deployed by 
ground command. First the FGM data rate is set to 32 Hz and FGM data are monitored in the real-time 
telemetry stream (VC2). The prime and secondary boom release mechanisms are then commanded in 
succession. The FGM axis rotation is verified during deployment. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4.3 Electrostatic Analyzers (ESA). 
The Electrostatic Analyzers will measure thermal ions and electrons in the range 5 eV - 30 keV. 
4.3.1 Science Requirements 
1) Plasma moments to within 10%, at high time resolution (10s or better) for inter-probe timing studies. 
2) Instantaneous differences in velocity and ion pressure between probes, to estimate the scale size of 
transport, the size and strength of flow vortices and the pressure gradient. 
3) Distribution functions of ions and electrons, to ascertain the presence of free energy sources. 
4.3.2 Specifications 
Both the ion and electron ESA have a look direction of 180 degrees in elevation, split in eight 22.5-degree 
bins (one per anode). Measurements over 4π steradian are made once per spin. The particles are selected in 
E/q (where q is the charge) by a sweeping potential applied in 32 steps, 32 times/spin (32 azimuths) 
between the outer (0 kV and the inner (~3 kV) concentric spheres in a Chevron configuration. On-board 
moment, pitch angle, and averaging computations are implemented at the IDPU. These operations routinely 
utilize FGM and SST data (to ensure correct values when the peak flux extends beyond the plasma 
instrument energy range). Even with onboard averaging, the ESAs generate nearly 3 kbytes of data each 
spin and thus require onboard moment calculations to obtain spin period data. 3-D distributions will be 
transmitted at a much lower cadence except during event bursts that will contain spin period distributions. 
Figure 5. ESA 
4.3.3 Calibration 
The science requirement of 10% accuracy on moment computation can be met by independent calibration 
of the ESAs. However, by inter-calibrating hour-long averages of routinely collected particle distributions 
during quiet-time probe-conjunctions it is expected to surpass the accuracy obtained from independent ESA 
calibration. 
An automated calibration procedure performs a complete angle/energy calibration of an instrument stack in 
less than 1 day. Calibration determines: 
1) Analyzer constant, uniformity of energy/angle response 
2) Hemisphere concentricity 
3) Optimum MCP voltage 
4) Sweep voltage verification 
5) Relative magnetic factors 
6) Flight mode validation 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Absolute magnetic factor values are determined from computer simulations and calibrations with a Ni63 
beta source. 
4.3.4 Aperture Cover Release 
During L&EO the ESA entrance aperture covers will be removed. This process will be commanded from 
the ground and the cover release is performed using a Shaped Metal Alloy (SMA) device. 
4.4 Solid State Telescope (SST). 
The Solid State Telescope (SST) measures the angular distribution (~3π steradian coverage) of super 
thermal ions and electrons. The detectors are identical to the SST pairs flown on the WIND spacecraft. 
Each probe carries two telescope pairs. 
4.4.1 Science Requirements. 
1) Perform remote sensing of the tail-ward moving current distribution boundary (at P3, P4, P5) 
2) Measure the time-of-arrival of super thermal ions and electrons (30-300 keV, at 10s resolution or better) 
during injections, and ascertain the Rx onset time (P1, P2). 
4.4.2 Specifications 
Each of the two SSTs consists of a telescope pair with double-ended sensors, as shown in Figure 6. 
Individual sensors comprise three stacked, fully depleted, passivated, ion-implanted, 1.5 cm2 silicon 
detectors. The center (T) detector is 600 µm thick, while the outside (O & F) detectors are 300 µm thick. 
Each of the four sensors measures ions and electrons via opposing sides of its detector. The two telescope 
pairs are mounted such that one pair of sensor aperatures points above the spin plane at 25 and 55 deg, and 
the other pair below the spin plane at –25 and –55 deg, respectively. 
4.4.3 Calibration 
Figure 6. The Solid State Telescope (SST) 
Absolute calibration points are determined by monitoring the highest energy of protons stopped and by 
placing the pairs (or triplets) of detectors in coincidence and monitoring the minimum ionizing energy for 
penetrating particles. Such practices have led to superb agreement between SST and ESA fluxes on WIND, 
and result to <10% absolute flux uncertainty. Inter-probe calibration will also be performed at times of low 
plasma sheet activity, when the flux anisotropy is low. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4.4.4 Attenuator Operation 
The SST attenuator is controlled by the IDPU. It will be operated when a probe approaches the radiation 
belts based on measured flux levels. Sufficient (~10min) hysteresis is built into the design. 
4.5 Search Coil Magnetometer 
The SCM measures the 3D magnetic field in the frequency bandwidth from 1 Hz to 4 kHz. It will extend 
with appropriate sensitivity the measurements of the FGM beyond the 1 Hz range. 
4.5.1 Science Requirements 
The science requirements derive from the need to measure with appropriate sensitivity (<1 pT/√Hz @ 10 
Hz): the cross-field current disruption waves (~0.1 fLH) at least as close to Earth as 8Re (fLH-60Hz). 
4.5.2 Specifications 
The SCM measures the variation of the magnetic flux threading three orthogonal high permeability 
µ−metal rods. The unit sensitivity is 0.5 pT/√Hz @ 10 Hz. A flux feedback loop is employed to ensure 
phase stability. 
The signals from the three sensors are pre-amplified and then processed together with the EFI data at the 
IDPU. 
Figure 7. The Search Coil Magnetometer (SCM) 
4.5.3 Calibration 
Absolute amplitude and phase calibration takes place with calibration coils that create a known AC pseudo
random noise consisting of a series of discrete frequencies covering most of the bandwidth (10 Hz – 4 
kHz). Calibration switch-on is commanded by the IDPU according to a pre-scheduled sequence. 
4.5.4 Boom Deployment 
See FGM boom deployment. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4.6 Electric Field Instrument (EFI) 
The EFI measures the 3D electric field in the frequency range from DC to 300 KHz. The three-dimensional 
EFI experiment consists of 4 spin-plane spherical sensors each suspended on its own 20 mm deployable 
cable 20 meters away from the probe center. Two axial tubular sensors, each 1 m long, are mounted on a 4 
m-long stacer element. 
4.6.1 Science Requirements 
Determine the time of onset at 8-10 Re by measuring: 
1) The plasma pure convection motion, i.e., without the effects of diamagnetic drifts that ESA 
measurements are subject to. 
2) The low frequency (T~1min) wave mode and pointing flux. 
The inner probes will determine the axial component independently from the axial boom measurement and 
provide both a method for calibration of the axial measurement and a backup solution. 
4.6.2 Specifications 
Boom electronics located at the EFI housing perform stub and guard voltage control and sphere biasing. 
Signal processing takes place in the IDPU, together with the SCM. Routine waveforms (32 samples/s) or 
burst waveforms (128-8192 samples/s) are captured and processed just as for the SCM data. Spectral 
processing of the low frequency (< 8 kHz) data occurs in the DSP in a fashion identical to the SCM. The 
wire booms will be deployed with near real-time monitoring of a release and spin-up sequence, each lasting 
1-2 hours/probe. Alternating between different THEMIS probes in science and sphere-release phase, 
mission-total EFI deployment lasts < 10 days. 
Figure 8. The Electric Field Instrument (EFI) 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4.6.3 Calibration 
The aforementioned individual probe calibration results in absolute DC measurement accuracy of 0.1 
mV/m, i.e. <10% of the field value anticipated during fast flows. Increased confidence in the measurements 
will be obtained from inter-spacecraft calibration during quiet times. 
4.6.4 Deployment Operations 
The EFI requires operational commands to govern boom deployment and adjustment as well as science 
commands to control sensor bias voltages, data sample rates, filter settings, and spectral resolution control. 
As in previous missions, a typical mode can be specified with ~200 commands valid over a typical 
operational period of ~1 month once deployment and checkout phases have been completed. 
Three phases: 
1) Deployment alternatively extending the wire boom pair in predetermined increments. During radial 
wire boom deployment and at each stop, sphere potentials are monitored in order to characterize probe 
charging affects, plasma environment, and EFI status. After the radial boom deployment, the axial 
booms are each deployed to their final lengths using one initiator event per boom. 
2) Checkout: Assuming nominal potential measurements and probe spin rate, the checkout phase begins 
with final adjustments in wire boom lengths to verify that each pair deployed symmetrically relative to 
the probe body. These occur in near real-time sessions, monitoring the release and spin-up sequence, 
each lasting 1-2 hours/probe. Alternating between different THEMIS probes in science data collection 
and sphere-release phase, mission-total EFI deployment lasts < 10 days. After boom deployment, an 
EFI early-checkout phase begins in which the photo-currents are characterized and the guards, stubs, 
and bias adjusted accordingly, requiring a new command load roughly once per week, per probe. 
Science quality data are returned during this phase which lasts ~1 month. 
3) During the nominal science phase, the EFI is configured roughly once each month through a command 
sequence. 
4.7 Instrument Data Processing Unit (IDPU) 
This unit is the heart of the instrument package: it provides instrument power, controls instrument 
functions, receives instrument commands and obtains housekeeping and science data, stores and processes 
the data and transmits data to the probe bus electronics. It is the interface between the instrument sensors 
and the probe BAU. The IDPU uses an 8085 processor and 256 Mbytes of memory to store science data. 
The IDPU collects, compresses, and stores instrument data and transmits the data to the ground upon 
command with a nominal downlink rate of 524.288 kbps or 1,048.576 kbps. The command uplink protocol 
is a 1 kbps, COP-1 compliant system, with commands relayed by the bus processor. Instrument data are 
yielded to the IDPU at continuous rates governed by the overall system mode (survey, particle burst, wave 
burst I or II). The data format is 24-bits consisting of an 8-bit application process identifier (APID) 
followed by 16 bits of data. Data compression and complete packetization is performed by the processor, 
prior to storage in the IDPU memory. The IDPU-to-bus C&DH telemetry requirement is a 1 Mbps serial 
data stream. The IDPU can mix and prioritize engineering and science frames according to operational 
preferences at downlink time. 
During nominal operation, the IDPU provides instrument housekeeping packets to the probe-BAU, which 
are combined with its data into CCSDS frames for downlink. Stored science data are transmitted separately 
after engineering data over the high-speed link to the BAU when commanded from the ground. 
The IDPU is responsible for monitoring instrument science data and using pre-defined measurement 
quantities as criteria for the overall instrument data rates. Using a command upload table, the processor 
steers instrument quantities into a trigger buffer section of memory based on a trigger APID list. A real
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
time evaluation of a single measurement level or weighted linear combinations of several measurements are 
compared to pre-set thresholds as criteria for survey, particle burst, or wave burst instrument rates. 
Mode definition tables are large macros used by the IDPU to configure instruments appropriate to the 
region of space being examined. The IDPU will be programmed with a number of mode definitions that are 
selected by an ATS command or on-board triggering logic. Assuming 32 macros of 512 bytes each, a full 
reload would require 16 kbytes. ESA and SST Moment Tables are calculated by IDPU FSW at system 
startup and loaded into the ESA and SST moment circuitry. For contingency operations, these tables are 
also directly loadable from the ground. EFI biasing, FGM and SCM parameter modes are expected to be 
small and are included in the mode definitions. 
4.8 Ground Observations 
Ground observations and measurements will be made of the aurora and earth's magnetic field by two 
networks of instruments. The first will be 20 Ground Based Observatories (GBO's) spread across Alaska 
(4) and Canada (16). The GBO's will contain All Sky Imagers (ASI) and Ground Magnetometers 
(GMAGs). The second network will be 10 Education and Public Outreach (E/PO) GMAGs located in 
schools at sub-auroral latitudes in the U.S. The locations of both networks are shown below in Figure 9. 
4.8.1 Ground Based Observatories (GBO) 
The THEMIS mission times substorm signatures on the ground and in space with a time resolution of better 
than 30 seconds. The comprehensive THEMIS approach to solving the substorm problem calls for 
monitoring the night side auroral oval with fast (<1s exposures), low cost, and robust white-light All Sky 
Imagers (ASI) and high-time resolution (1s) Ground Magnetometers (GMAGS). These instruments will 
produce auroral images and Earth magnetic field measurements. In addition to the ASI and GMAG data, 
the GBO will produce health and safety data for evaluating the status of the site. The ASI's are provided by 
UCB and the GMAGs are supplied by UCLA. GBO locations are shown below in Figure 9. 
Figure 9. Ground Based Observatory (GBO) Locations 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Some of the GBO’s will take advantage of infrastructure and instrumentation which are part of existing 
ground based networks. Each of the 20 sites will house a UCB ASI, however, only 10 sites will house a 
UCLA GMAG. Two of the Alaska sites will rely on magnetometers supplied and operated by the 
Geophysical Institute at the University of Alaska, Fairbanks. The remaining 8 sites in Canada will rely on 
magnetometers supplied and operated by Canadian Geospace Monitoring (CGSM) and Natural Resources 
Canada (NRCAN). Table 2 lists the type of magnetometer at each site as well as the geographic 
coordinates. 
Geographic 
No. Site Abbrev. Location Latitude Longitude GMAG type 
1 Gakona GAK USA 62.4 214.8 GI & GPS5 
2 Fort Yukon FYU USA 67 199.6 GI & GPS1 
3 Mcgrath MCGR USA 63 204.4 GMAG1 
4 Kiana KIA USA 66.6 214.7 GMAG4 
5 Inuvik INUV Canada 68.3 226.7 CGSM&GPS4 
6 White Horse WHOR Canada 60.7 224.9 GMAG7 
7 Lac de Gras LGRA Canada 64.6 250 GMAG2 
8 Fort Simpson FSIM Canada 61.8 238.8 CGSM&GPS3 
9 Prince George PGEO Canada 53.9 237.4 GMAG-Proto 
10 Rankin Inlet RANK Canada 62.8 267.9 CGSM&GPS6 
11 Fort Smith FSMI Canada 60 248.1 CGSM&GPS6 
12 Athabasca ATHA Canada 54.7 246.7 NRCan&GPS2 
13 Gillam GILL Canada 56.4 265.4 CGSM&GPS9 
14 The Pas TPAS Canada 54 259 GMAG3 
15 Pinawa PINA Canada 50.3 264 CGSM&GPS7 
16 PBQ PBQ Canada 55.3 292.3 NRCan&GPS10 
17 Kapuskasing KAPU Canada 49.4 277.6 GMAG9 
18 Nain NAIN Canada 56.5 298.3 GMAG5 
19 Gangon GANG Canada 51.9 291.8 GMAG6 
20 Goose Bay GBAY Canada 53.3 299.6 GMAG8 
Table 4. GBO Information 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Figure 10 shows the major components of the GBO. The Computer System Enclosure (CSE) consists of an 
external insulated environmental enclosure and an internal rack mount. The rack mount will contain the 
system computer, hot swappable hard drive, GMAG interface electronics, Power Control Unit (PCU), 
CD10X Datalogger and battery, ASI power supply, and an Uninterruptible Power Supply (UPS). 
Figure 10. GBO Components 
The CSE will operate under external ambient temperatures of –50 degrees C to 40 degrees C. It provides a 
dust free method of cooling via a solid state air conditioner and heating via small space heaters (see Figure 
11). It provides access for external cables, maintenance, hot swapping of hard drives. 
Figure 11. GBO CSE Heating and Cooling 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
 
                
         
              
 
 
  
              
 
             
             
      
 
                  
 
       
        
      
      
       
  
 
 
  
The PCU provides control of both temperature and instrument power. The temperature in the CSE and ASI 
will be maintained at 20 degrees Celsius +/-10 degrees Celsius. Also, the PCU provides a graceful 
shutdown of the system computer in the event of a loss of power or temperature. 
Figure 12. GBO PCU 
The CR10X Datalogger is a “smart” controller with simple programming and data logging capability, and 
takes the place of a regular thermostat. It provides analog and digital I/O for the system computer, and is 
always operating and always accessible for re-programming via the internet or Iridium modem. It has an 
extended operating range of –55 degrees Celsius to 85 degrees Celsius. Due to lower power consumption, it 
can operate for months on battery power. 
Remote access to the GBO’s will typically be through an internet connection made possible by one of the 
following: 
• Hardwired using a local LAN connection 
• Telesat HSi (Canada) or Starband 480 (Alaska) 
o Can provide fixed IP address 
o Minimum 10 kbps uplink rate 
• Backup connection via an Iridium modem 
o 2400 bps 
Figure 13. Prime Data Communications Link 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Figure 14. Backup Data Communications Link 
4.8.1.1 All Sky Imagers (ASI) 
The ASI design uses commercially available components, and is based on heritage ASI’s used at AGO sites 
in Antarctica. The basic components include a Charged Couple Device (CCD) camera and an all sky (fish
eye) lens (Figure 15). 
Figure 15. All Sky Imager (ASI) 
The ASI is housed in a heated environmental enclosure topped with a polycarbonate/acrylic dome (Figure 
16). The enclosure is hermetically sealed with a nitrogen purge. The enclosure is designed to operate with 
external ambient temperatures in the range of –50 degrees Celsius to 40 degrees Celsius. The internal 
temperature is maintained at 20 degrees Celsius, +/- 10 degrees Celsius 
. 
Figure 16. ASI Mounting and Enclosure 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
An internal sun shield will protect the ASI UV radiation damage during non-operation periods (figure 17). 
Figure 17. ASI Sun Shield 
4.8.1.2 Ground Magnetometers (GBO-GMAGs) 
There are 10 GBO's (2 in Alaska, 8 in Canada) that will house Ground Magnetometers (GMAGs) 
developed by UCLA. These units are also referred to as Fluxgate Magnetometers. The GMAGs are small, 
low power units, and have a ruggedized all weather sensor design. The GMAG processor card is located in 
the CSE and has a USB interface for data retrieval and firmware uploads. 
Figure 18. GMAG Sensor 
The sensor has a +/-72KnT dynamic range @ 0.01nT resolution, and will produce 2 vectors per second. 
Each vector consists of three quantities: Bx, By, and Bz, which are measurements of the magnetic field 
strength along each axis. The data output is expected to be 86.4 kbytes/hour with approximately 100 
bytes/hour for housekeeping and log data. The raw magnetometer data are recorded in raw format, and 
calibration is applied when products are compiled for distribution. Data are recovered with the ASI data. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
       
                  
                  
 
    
            
           
   
 
 
  
             
          
           
                   
        
 
4.8.1.3 Health & Safety -System Status 
The GBO will log approximately 10 kbytes of health and safety data every 5 minutes, or 100 kbytes/hour. 
Essential information is recorded at a much lower rate, approximately 20 bps, yielding a data volume of 10 
Kbytes/hour. 
4.8.2 E/PO Ground Magnetometers (E/PO-GMAGS) 
UCLA will also build and install 10 additional GMAGs in selected K-12 schools located in sub-auroral 
latitudes in the U.S. (blue dots in figure 9). The UCB Education and Public Outreach (E/PO) group will 
manage this effort in order to promote inquiry-based and theme-based instruction as well as allow hands-on 
student participation. 
Figure 19. E/PO GMAG Locations 
ASCII conversion routines will process the raw magnetometer data at the sites. Working with E/PO 
personnel, students and teachers will use standard Windows software packages (Excel import of ASCII 
data) to view and analyze data. Web-based download functions will make the data accessible to other 
schools and the general public. Data will also be copied to UCLA and UCB. At UCB, the data will be 
integrated into the data access and retrieval system as well as folded into the summary plot production 
system. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
5. Ground Data System (GDS) Description 
5.1 Overview 
The THEMIS GDS consists of several functional segments: The Ground Stations (GS) to communicate 
with the probes on-orbit, the Mission Operations Center (MOC) for probe telemetry and command control, 
the Flight Dynamics Center (FDC) for probe orbit and attitude determination, and the Science Operations 
Center (SOC) for instrument data collection, processing, archiving, and distribution functions as well as 
planning and generating commands for instrument operations. The Berkeley Ground Station (BGS), MOC, 
FDC, and SOC are all co-located at the Space Sciences Laboratory on the UCB campus. A general 
description of each GDS segment is given below as well as their operational responsibility. 
5.2 Ground Stations 
5.2.1 Berkeley Ground Station (BGS) 
The primary ground station for THEMIS is the Berkeley Ground Station (BGS). The BGS employs front
end processors for bit synchronization, Viterbi decoding, frame synchronization, Reed-Solomon decoding, 
and CCSDS channel routing. Data streams that carry real-time engineering and science data are routed 
directly into the MOC for real-time state-of-health monitoring and control functions. In addition, all 
received telemetry data are stored locally at the ground station in separate files for each virtual channel and 
are automatically transferred to the MOC and SOC via FTP once the support is complete. 
Commanding of the probes is initiated from the ITOS workstations in the MOC and follows standard 
CCSDS procedures [1]. Individual commands or entire command loads are divided up into CLTUs and are 
forwarded to the front-end processors via secure TCP/IP network socket connections. The command data 
stream is then transmitted in real-time at a rate of 1 kbps and BPSK modulated onto a 16-kHz subcarrier. 
The subcarrier is in turn PM modulated onto the RF carrier with a modulation index of 1.0 rad. The CCSDS 
COP-1 protocol is used to verify command reception on the probe. Once a command is transmitted to the 
probe, ITOS monitors the Command Link Control Word (CLCW) that is attached to each telemetry frame, 
indicating the command verification status on-board the probe. ITOS automatically initiates retransmission 
of commands that are not verified. 
Each probe contains a coherent STDN compatible transponder, thus allowing two-way Doppler ranging for 
accurate orbit determination. All probes use the same frequency pair for telemetry and commanding. 
Communications are established with one probe at a time. 
5.2.2 Secondary and Backup Ground Stations 
Secondary ground station support will be provided by NASA/GN stations WGS 11-m, MILA 9-m, AGO 
9-m, and HBK 10-m. TDRSS support is baselined for the Launch and Early Orbit (L&EO) phase of the 
mission to monitor probe release, aid in maneuver operations, and recovery from anomalous conditions. 
Real-time telemetry and command data are carried between the ground station and MOC via a T1 line. 
Telemetry data stored on the ground are transferred to the MOC and SOC via the open Internet. During 
Launch and Early Orbit (L&EO) operations, TDRSS S-Band Single Access (SSA) mode allows 
communications with each of the probes at a low data rate at times when the individual probes are within 
communications range of a TDRS spacecraft. 
Pass schedule requests generated by the MOC are submitted to the respective scheduling office associated 
with each ground station network. Confirmed pass schedules are used to perform mission planning and 
build command loads. All real-time command and telemetry connections between the MOC and ground 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
stations are carried over secure network links. Tracking data are transferred from all ground stations to the 
FDC to perform orbit determination in order to generate updated ephemeris products. Attitude sensor data 
received from the probes through the MOC are routed to the FDC to obtain ground based attitude solutions. 
Once verified, ephemeris products and attitude solutions are used to plan orbit maneuvers. 
5.2.3 Telemetry Files 
The ground station telemetry file transfer protocols and file formats are listed in reference document [4]. 
The file naming convention is listed below: 
Telemetry File Naming Conventions: 
Format: 
• FACILITY.PROBE_BUS_NAME.TLM_VCN.YYYY_DDD_HHMMSS.dat 
Examples: 
• BGS.THEMIS_A.TLM_VC0.2007_028_060312.dat 
• WGS.THEMIS_B.TLM_VC1.2007_029_012031.dat 
• MIL.THEMIS_C.TLM_VC2.2007_038_102319.dat 
• AGO.THEMIS_D.TLM_VC3.2007_032_151745.dat 
• HBK.THEMIS_E.TLM_VC6.2007_034_234512.dat 
• WSC.THEMIS_A.TLM_VC0.2006_301_001834.dat 
5.3 Mission Operations Center (MOC) 
The THEMIS Mission Operations Center (MOC) will perform mission planning functions, commanding, 
and state of health monitoring of the 5 probes, recovery of science and engineering data, data trending and 
anomaly resolution. The UCB Flight Operations Team (FOT) will carry out these activities. 
5.3.1 Mission Operations 
5.3.1.1 Overview 
The main operational modes or phases include Pre-Launch, Launch and Early Orbit (L&EO), Nominal 
Science Operations, Maneuvers, and End-of-Mission. The probes are powered with the receiver on during 
Pre-Launch and L&EO modes with all deployable appendages stowed during the Probe Carrier (PC) 
dispense operation. The probes are passively spin-stabilized upon release (even under dispense fault 
conditions) and full command and telemetry operations commence, initiated by ground command, to each 
probe using unique identification codes. Deployment of all booms, checkout, and power-up of each 
instrument is accomplished at the appropriate stages of the In-orbit Checkout (IOC) phase (part of L&EO) 
to verify key instrument functions and extract body (FGM), sheath (EFI), and field (SCM) calibration data 
for each unique configuration during these deployments. This allows for independent decoupling and 
characterization of the probe body effects for use in subsequent science data analysis. While all probes are 
self-sufficient the FDC will carry out orbit and attitude determination using the Berkeley Flight Dynamics 
System (BFDS). All maneuvers will take place during a ground station contact. A passively spin-stabilized 
control scheme, 4π steradian power positive body-mounted solar panels and a near omni directional 
communications coverage allow any probe to fail-safe with no required maneuvers. 
5.3.1.2 Pre-launch, Launch and Early Orbit (L&EO) Operations 
Pre-launch operations include end-to-end data flow tests, rehearsals, and full mission simulations, 
integrating and operating all of the GDS elements. During the launch sequence the Delta II injects the PCA 
into the target insertion orbit, initiating the release of the probes from the PCA. At this time operational 
command and control authority transitions from the LV controllers to the MOC at UCB. Subsequently each 
probe is polled via ground station contacts in a round-robin scheme to evaluate state-of-health and to obtain 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
telemetry and tracking data for initial orbit and attitude determination. Once the orbits are well established, 
the MOC generates the first set of command loads that are uplinked to each probe. Further on-orbit 
checkout commences with deployment of the FGM/SCM magnetometer booms, which undergo a 
simultaneous deploy, followed by the power up sequence of all science instruments. As soon as all probes 
are checked out, the final designation of probe constellation IDs, i.e. P1, P2, P3, P4 and P5 is performed 
based on functional test results and magnetic signature levels. This scheme allows for implementation of 
mission redundancy and a probe replacement strategy that minimizes impact from off-nominal science 
instrument performance. 
Final orbit injection begins after all probes are re-spun to a spin rate of 20 rpm. Calibration of the tangential 
thrusters is part of the re-spin sequence. The orbits of each probe are then adjusted in one (P3-P5) or two 
(P1-P2) discrete pairs of apogee and perigee maneuvers, using the axial thrusters. Each individual 
maneuver is followed by accurate orbit and attitude determination, allowing for a calibration of the axial 
thrusters. Proper thruster firing is verified in real-time by monitoring telemetry data from the RCS 
temperature sensors, tank pressure gauges, and attitude sensors. Once placed in their final mission orbits, 
the probes are commanded to deploy the radial EFI wire booms and subsequently the axial EFI booms. 
Calibration measurements of the probe potentials are performed as part of the step-by-step deploy 
sequence. Once the radial wire booms have been deployed, the EFI axial-boom pair undergoes a 
simultaneous deploy. Finally, the spin rate on all probes is adjusted to 20 rpm. 
5.3.1.3 Normal Operations 
Normal operations begin with preparation for the conjunction season. During normal operations, 
communications with each probe are established at least once per day via the primary ground station (BGS) 
to monitor the probe health and safety, to recover stored engineering data, and to collect tracking data for 
precise orbit determination. 
Science data are stored on-board and are transmitted to the primary or secondary ground station during a 
15-30 min pass near perigee. Data transmissions are initiated by time sequence commands stored on-board 
each probe. These commands are part of an Absolute Time Sequence (ATS) load generated individually for 
each probe using the Mission Planning System (MPS). ATS loads are uploaded several times per week and 
cover at least 8 days for P1, 4 days for P2, and 3 days for P3, P4, and P5. The ground stations will transfer 
to the MOC health and safety data in real-time (VC0) as well as a subset of science data (VC2) for 
monitoring instrument performance. Stored engineering (VC1) and science (VC3) data are saved in files 
and are delivered post-pass to the MOC and the SOC. 
During normal operations, the orbits of P1, P2, and P5 are adjusted in a few (2-4/year depending on probe) 
intervals to optimize conjunctions. These short duration adjustments are nominally performed with side 
thrusting. Additionally, orbit maneuvers are performed once per year with P1 and P2 to compensate the 
lunar perturbations on inclination, thus avoiding long shadow periods while optimizing science conjunction 
time. These longer duration burns for P1 and P2 take place outside the main science season and are 
performed with axial thrusting. 
The Berkeley Emergency & Anomaly Response System (BEARS) is used to contact FOT members if 
probe/instrument telemetry out-of-limits conditions are detected or other GDS conditions arise that require 
immediate attention. The BEARS will parse through log files generated by ITOS during real-time passes 
and playback of stored engineering data, and automatically checks for yellow and red telemetry limit 
violations. The BEARS will also act on email warning messages sent to it from other GDS elements. If a 
limit violation or other GDS anomaly is detected, on-call FOT members are alerted via 2-way email pagers 
in order to access and resolve the problem. 
5.3.1.4 End of Mission 
After the second year of operations the probes will be positioned for re-entry course. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
5.4 Flight Dynamics Center (FDC) 
5.4.1 Overview 
The FDC is responsible for supporting all orbit dynamics and maneuver functions, such as generation of 
ephemeris and mission planning products, orbit determination, ACS sensor calibration, attitude 
determination, maneuver planning, and analysis and calibration of thruster performance. 
5.4.2 Software Tools 
Four major software tools are used to generate all ephemeris and mission planning products, perform orbit 
and attitude determination, and carry out maneuver planning functions. These tools are the Goddard 
Trajectory Determination System (GTDS), the General Maneuver Program (GMAN), and the Multi
mission Spin Axis Stabilized Spacecraft (MSASS) attitude determination systems were all developed at 
GSFC. SatTrack is a Commercial-off-the-Shelf (COTS) product. Probe conjunction analysis is 
accomplished with a combination of GTDS and an Interactive Data Language (IDL) based software library 
that was developed in-house at SSL. 
5.4.3 Operations 
The four major functions of these software tools are described below: 
5.4.3.1 Orbit Determination 
GTDS performs high-precision orbit propagation and orbit determination functions for THEMIS. For orbit
determination, GTDS ingests two-way Doppler tracking data collected from the ground stations in UTDF 
format. These tracking data are obtained during regular science data transmissions at ranges of 30,000 km 
or less, and during additional passes at other parts of the orbit for each probe. GTDS estimates new state 
vectors for the five probes and generates an updated ephemeris. Once state vectors have been updated, new 
mission planning products are generated, and the updated vectors are distributed to the ground stations to 
generate new acquisition angles for upcoming pass supports. Routine NORAD orbit determination using 
radar tracking data provides a back up for the primary orbit determination. 
5.4.3.2 Mission Planning Products 
Mission planning products are generated by SatTrack based on GTDS ephemeris output. These include 
ground station view periods, link access periods, eclipse entry and exit times, and other orbit events 
required as input to MPS. Other tools in the SatTrack software suite distribute real-time event messages to 
various ground system elements such as ITOS and the BGS in a fully autonomous client/server network 
environment 
5.4.3.3 Attitude Determination 
Ground based attitude determination of the probes utilizes MSASS to ingest raw sensor data from the 
telemetry stream that are converted into vectors expressed in spacecraft body coordinates. The suite of 
attitude sensors on each probe comprises a V-slit sun sensor, two mini-gyros, and the dual-use three-axis 
FGM. FGM data are utilized during the near-Earth portion of the probe orbits to cross-calibrate the other 
sensors. Reference vectors for conversion from the body frame to the inertial frame are obtained from the 
spacecraft, solar, lunar, and planetary ephemeris, and from the most current International Geophysical 
Reference Model (IGRF) of Earth's magnetic field. Based on these inputs, the MSASS estimator 
determines the inertial attitude vector at any given time for each probe. 
5.4.3.4 Maneuver Planning 
The GMAN tool performs all maneuver planning functions. Based on probe propulsion plus current and 
target state vectors, GMAN generates an optimized mission profile that includes spin-axis reorientation and 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
orbit adjustment maneuvers with coast periods between thruster firings. A typical maneuver scenario 
includes a reorientation of the probe from its mission attitude to the orbit maneuver attitude, followed by 
the orbit maneuver itself and the reorientation maneuver returning attitude back to nominal. Attitude 
reorientation maneuvers may be performed near perigee to take advantage of the magnetometer data that 
allow for independent confirmation of the correct attitude for the subsequent orbit maneuver. Orbit 
maneuvers are executed near perigee and apogee for operational mission orbit insertion and periodic orbit 
maintenance. Maneuver planning functions are performed at the FDC in consultation with GSFC/FDAB. 
5.5 Flight Operations Team (FOT) 
The MOC, FDC, and BGS systems are controlled and maintained by the UCB operations personnel. In the 
MOC, the Flight Operations Team (FOT) uses the Integrated Test and Operations System (ITOS) for probe 
command, control, and Health and Safety (H&S) monitoring. The use of ITOS from I&T through on-orbit 
operations allows FOT members to be trained in bus and instrument operations early on, facilitating a 
smooth transition from I&T to normal operations. 
5.6 Science Operations Center (SOC) 
5.6.1 Overview 
The THEMIS Science Operations Center (SOC) is responsible for Probe and GBO instrument data 
collection, processing, archiving, and distribution functions as well as planning and generating commands 
for instrument operations. The SOC works closely with the co-located MOC to guarantee a seamless 
transfer and processing of probe instrument telemetry data, and proper control and configuration of the 
instruments. The SOC will work with the GBO team in a similar fashion. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
6. Project Data Flow 
6.1 Overview 
The following sections detail the flow of THEMIS Project Data from probe and GBO instrument collection 
through ground processing and product production and availability. 
Figure 20. THEMIS SOC Data Flow 
6.2 Probe Instrument Data 
Figure 14 and 15 detail the major processing steps and timeline associated with the probe instrument data 
and the products produced from them. Please refer to them while reading the following sections. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
Figure 21. Instrument Data Collection and Processing Timeline 
6.2.1 Collection – Time T0 
The data flow begins with collection on board the probe. Collection rates vary based on orbital position and 
local plasma conditions. See Appendix B for details on the exact collection rates. The engineering data are 
stored as VC1, and science data as VC3. 
6.2.2 Recovery – Time T1 (T0+8 Days Maximum) 
The stored engineering and science data (VC1 and VC3) are transmitted to the ground during a 15 to 30 
minute long ground station contact near perigee. After data have been collected on the spacecraft, it could 
take up to 8 days to recover a complete orbits worth of data. This takes into account the probe with the 
longest orbit period (P1 -> 4 days), the number of instrument data transmissions per orbit (1 transmission 
per orbit for P1), and the maximum number of transmissions it might take to recover the full orbit (2 
transmissions for P1). 
In addition to the stored data, the probe will transmit real-time engineering data (VC0) and a subset of the 
science data (VC2). The transmission of the data is controlled by stored commands the FOT had previously 
loaded and/or ground command. The ground stations transfer the VC0 and VC2 to the MOC in real-time. 
All VCs are saved in files and are delivered to the MOC and the SOC approximately 1 hour after the end of 
the ground station contact. 
VC2 will contain a subset of the instrument science data in real-time, which will be useful during 
Integration and Testing (I&T) and Launch and Early Orbit (L&EO) operations when the instruments will 
be heavily tested and configured for normal operations. Also, VC2 will give the FOT an additional avenue 
for checking that instruments are operating properly on a day-to-day basis. The VC0 and VC2 data will 
undergo limit checking by ITOS and will be archived to files that will be available for post-pass processing. 
The results of the limit checking are recorded to a file and passed on to the BEARS for error detection and 
personnel notification. 
6.2.3 Delivery to SOC – T1+1Hr 
At approximately 1 hour after the completion of the ground station contact, the VC1 and VC3 files will be 
sent via FTP to the SOC. The VC1 file is also sent to the MOC where it is processed automatically by an 
ITOS workstation for back-orbit limit checking. Corresponding log files are passed to BEARS for error 
detection and operator notification. The reception of the VC files at the SOC will initiate autonomous 
processing of the data. This includes data quality checks and statistics generation to determine if data gaps 
have occurred both within files and between prior data transmissions. Also, these data files will be 
immediately available to data analysis tools (SDT and IDL routines) used to thoroughly checkout 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
instrument operation during I&T and the L&EO instrument turn on and commissioning phase.  The VC 
files will remain accessible from hard-drive and are backed up on to DVD. 
6.2.4 Level Zero Processing – Time T1+2Hrs 
Following the initial quality checks and statistics generation, the data undergo Level Zero Processing, 
which includes sorting by Application (Packet) Identifier, time ordering, and then mapping the time 
ordered packets into 24 hour Level Zero (L0) data files. Each 24 hour L0 data file will contain all 
engineering and science (slow survey, fast survey, burst) collected during that period. The L0 files are 
stored locally, archived to DVD, and made available to data analysis tools. 
6.2.5 CDF Processing – Time T1+3Hrs 
Once a L0 file is created or updated, it will be converted into Common Data Format (CDF) or Level 1 (L1) 
data. Level 1 CDF files contain raw data from all instruments at the highest temporal resolution. Particle 
moments and distributions and field quantities are obtained by converting raw data into physical quantities 
using instrument calibration data 
6.2.6 Diagnostic Plot Creation – T1+4Hrs 
The completion of CDF processing will trigger the creation of instrument diagnostic plots (T1+4hrs). Each 
day the THEMIS Operations Scientist (Tohban) will review these plots in order to validate nominal 
operation and calibration of each instrument. This includes daily checks of (i) the overall data quality, (ii) 
housekeeping data trends (e.g., detector efficiencies and offsets) and (iii) identification and tabulation of 
geophysical events of special interest. 
6.2.7 Browse/Key Parameter (K0) Data Creation – T1+24Hrs 
The completion of CDF processing will also trigger the creation of Browse or Key Parameter (K0) Data 
which will be available to the public via the UCB website approximately 1 day after instrument data are 
initially received at the SOC (T1+24hrs). Space-based instrument key parameter data includes (all provided 
in physical units and at 3-s resolution): 
•	 3-D magnetic field, DC waveforms and AC spectrograms. 
•	 3-D electric field, DC waveforms and AC spectrograms. 
•	 Core (10eV-40 keV) ion density and velocity moments, temperature and pressure tensor, energy 
spectrogram. 
•	 Core (5 eV-30 keV) electron temperature and pressure tensor, energy spectrogram. 
•	 Energetic ion and electron fluxes. 
•	 Indications when burst data has been collected 
The browse data are designed for monitoring the large-scale particle and field dynamics and for selecting 
time periods of interest. These data are created using IDL scripts provided by instrument investigators and 
are available as both plots (GIFS) and CDF files. They are not routinely checked for accuracy and are 
subject to revision as new data are received and/or updates to calibration and orbit data occur. Key 
parameter data associated with the GBO and E/PO ground based observations will also be produced. 
6.2.8 K1 Data Creation – T1+1Month 
At approximately 1 month (T1+1month) after data reception at the SOC, the first key parameter set (K1) is 
delivered to the Sun-Earth Connection Active Archive (SECAA) and the National Space Science Data 
Center (NSSDC) via the internet. This incorporates updates to data content, calibrations, and orbit data. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
6.2.9 K2 Data Creation – T1+6Months 
At approximately 6 months (T1+6months) after data reception at the SOC, the definitive key parameter set 
(K2) is delivered to SECAA and NSSDC. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
   
                 
              
            
        
 
 
  
 
    
   
              
           
            
          
 
                 
      
             
          
 
       
6.3 GBO Data 
In general, the University of Calgary (UC) shall serve as the primary data distribution hub for the UCB All 
Sky Imager (ASI) and UCLA Ground Magnetometer (GMAG) data, as well as the GBO health and safety 
(H&S) data. The University of Alberta (UA) will recover and process the CGSM and NRCan 
magnetometer data and make it available for download. 
Figure 22. Ground Based Data Flow 
6.3.1 Collection – Time T0 
6.3.1.1 ASI Data 
The ASI will produce full images (Stream2) in PNG format at a frequency of 1-image/5 seconds (nighttime 
only). Each image is made up of 256x256 16-bit values (pixels). PNG compression reduces this to 90 
Kbytes/frame (70%). A 5-second frame rate will produce 150 kbps or 60 Mbytes/hour or 720 images/hour. 
For Stream 2, the expected data volume is 220-290 gigabytes/year/site uncompressed. 
Smaller, low resolution “thumbnail” images (Stream 1) in PGM format will be derived from the full 
images. Each thumbnail is comprised of 20x20 8-bit values (pixels) plus header information (roughly 50 
bytes) for a total of ~450 bytes. GZIP compression reduces this to 270 bytes/frame (60%). A 5-second 
frame rate will produce 430 bps or 190 Kbytes/hour. 
Stream 1 and 2 are stored locally on a hard drive connected to the system computer. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
   
                  
              
  
 
         
 
   
    
     
        
   
   
             
  
        
                
             
                    
           
               
           
   
 
                   
         
              
     
        
             
                
 
 
       
      
    
   
    
     
  
 
       
                      
      
6.3.1.2 GMAG Data 
The GMAG will generate 2 mag vectors every second. Each vector consists of three quantities: Bx, By, and 
Bz, which are measurements of the magnetic field strength along each axis. The data output is expected to 
be 86.4 Kbytes/hour. 
GBO data collection is summarized in the table below. 
Instrument Collection Rate Description 
ASI 1 image every 5 seconds Stream 1: “Thumbnail” low resolution image. 20x20 8
bit values (pixels) plus header information (roughly 50 
bytes) for a total of ~450 bytes. PGM Format. 
Stream 2: Raw image frames. 256x256 16-bit values 
(pixels). PNG format. 
GMAG 2 mag vectors every second Each vector consists of three quantities: Bx, By, and Bz. 
Table 5. GBO Data Acquisition 
6.3.2 Thumbnail Image Recovery by UC – T0+1min 
Stream 1 (thumbnail frames) is transmitted to UC daily and complies with the needs of THEMIS to 
determine the substorm onset to better than 0.5 hours in MLT. Primary means of stream 1 data retrieval is 
through the Internet provider Telesat HIs using a typical TCP/IP connection, with the expected throughput 
rate of 50 kbps. Stream 1 should arrive at UC within a few seconds of acquisition and be available for 
download after review and movement to central storage (~5minutes). A fraction of the high-resolution data 
(Stream 2) will be recovered with Stream 1 (<10%). An Iridium telephone backup is available for recovery 
of stream 1 and health and safety data as well as controlling GBO operations. The Iridium phone link 
operates at 2400 bps. 
As soon as ASI data arrives at UC, it will also enter a sophisticated data access and retrieval system 
developed for NORSTAR's filter camera images and based on keogram summary plots (image North-South 
slices as a function of time). With this interactive system, a user calls up a customized summary of data 
from one or more stations. 
6.3.3 Health and Safety (H&S) Data Recovery – T0+1min 
Health and safety monitoring of sites will also occur over the Telesat Internet link as well as via a back link 
using an Iridium telephone. The H&S data will be available for review from the UC website. This will 
include the following items: 
• CPU and motherboard temperatures, fan speeds 
• Hard drive temperature, error rates 
• Clock status (NTP) 
• Network status 
• Power status (UPS) 
• Free disk space, memory 
• Web-cam 
6.3.4 Thumbnail Image Copied to UCB – T0+5mins 
Once the stream 1 data is moved to central storage at UC it will be mirrored to UCB, where it will be 
available for download via the UCB website and incorporated into Browse Data. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
6.3.5 Raw Magnetometer Data Recovered by UC and Copied to UCLA, UA, and 
UCB - T0+27hrs 
The raw magnetometer data is saved to a file every hour, and these files will then be recovered by UC once 
per day. Once at UC, it will be copied to UCB, UCLA, and UA. 
6.3.6 UCLA Produces Processed GMAG Data – T0+28hrs 
Once UCLA acquires the raw mag data, it will convert it into a more easily used format. This processed 
data will be copied to UCB and UA. 
6.3.7 Keogram Recovery and Distribution – T0+30Hrs 
While the ASI is off (daytime), keograms will be created. These will be recovered and made available from 
the UC website. They will be copied to UCB where they will also be available via the UCB website. 
6.3.8 Inclusion in Key Parameter Data – T0+30Hrs 
The ASI and GMAG data will be included in the Key Parameter data beginning with the Browse data. 
6.3.9 Recovery and Distribution of Full Resolution Images – T0+6Months 
Full recovery of stream 2 data requires physically swapping out GBO hard drives and returning the units to 
UC for disk integrity and status checks and offloading of data to central storage. This will occur once or 
twice a year. The total image data volume (stream 2) amounts to 220-290 gigabytes/year/site 
uncompressed. The full image data set will be copied to UCB. 
6.4 E/PO GMAG Data 
UCLA is responsible for collecting the raw GMAG data from the schools sites and processing the data into 
a more usable format for the science community. 
6.4.1 Collection – Time T0 
The E/PO GMAGS will collect data at the same frequency as the GBO GMAGS – 2 mag vectors per 
second. 
6.4.2 Recovery – Time T0+27Hrs 
UCLA will copy the raw GMAG data from the sites once per day. The raw data will be copied to UCB 
also. 
6.4.3 Processed GMAG Data Produced and Distributed 
UCLA will produce processed GMAG once the raw data is received which will then be copied to UCB and 
UA. 
6.5 SPASE Collaboration 
The THEMIS Project Data set will be described in the terms spelled out by the Space Physics Archive 
Search and Exchange (SPASE) data model, which is used by SEC. Much work has been done on the 
Virtual Solar Observatory (VSO) and Virtual Space Physics Observatory (VSPO) and Virtual 
Observatories in other areas are expected to be established in the near future as well. SPASE will be 
working with each of these groups to unify access within the whole of the space physics community. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
7. Instrument Command and Control 
7.1.1 Overview 
The Flight Operation Team (FOT) carries out instrument command and control functions from the Mission 
Operations Center (MOC) under the direction of the PI and instrument Co-Is in consultation with pertinent 
subsystem engineers. Probe and instrument operation are reviewed and planned in weekly team meetings. 
One of the science team members assumes the rotating role of the scientist-on-duty (Tohban) who forms 
the point of contact between the THEMIS science team and the FOT. The Tohban is responsible for 
overseeing day-to-day science operations, including instrument configuration, data recovery and data 
processing. 
Command and control of the instruments is separated into (4) different operational phases: 
1. 
2. 
3. 
4. 
Instrument Commissioning (L&EO) 
Normal Operations 
Conjunction 
Maneuvers 
7.1.2 Instrument Commissioning 
Instrument commissioning begins with the IDPU turn-on as soon as the probe power system is stable and 
temperatures are below maximum operating limits. From there the instruments are powered up and 
configured based on the THEMIS Launch and Early Orbit Operations Plan [6]. 
7.1.3 Normal Operations 
The instruments will be operated in 4 basic science modes: 
• Slow Survey (SS) 
• Fast Survey (FS) 
• Particle Burst (PB) 
• Wave Burst (WB) 
Selection of these modes is controlled by either stored commands or on-board triggers. 
7.1.3.1 Slow Survey to Fast Survey 
The transition between these two modes will be controlled by instrument commands in the probe Absolute 
Time Sequence (ATS) load. The load is built by the Mission Planning System (MPS) using the ITOS 
Command Database. The timing of the commands will be taken from a special ephemeris event file 
supplied by the FDC and ingested by the MPS. 
7.1.3.2 Particle & Wave Burst Triggers 
For the triggers, the IDPU FSW continuously samples science data in the SSR (e.g., ESA/SST Peak Flux, 
DFB V1-V6 voltages, FGM Bx-Bz) and evaluates the data at 1 spin resolution against a pre-loaded 
threshold value. When the threshold is exceeded, it then calculates a Quality value using 8 selectable 
functions and tags the resulting burst data collected with a value ranging from 0-255. This quality value 
decides whether the burst is kept or overwritten as subsequent bursts are collected. Also, the quality value 
determines the memory readout priority. The particle and wave burst trigger setting will be fine tuned early 
in the mission using ground commands to the IDPU. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
7.1.3.3 Conjunction Operations 
During conjunction season, the FOT will uplink predicted conjunction times to the IDPU. The FSW will 
increase the Quality evaluation near the conjunction. Also, conjunction duration and bias settings are 
configurable. 
7.1.3.4 Maneuver Operations 
During maneuvers the ESA high voltage will be disabled and the SST placed in attenuated mode. 
The probe orbit placement is designed for maximum science return during the prime tail season, a four 
month period centered on February 21 of each year. Over the course of the mission, the orbits of P1 and P2 
are optimized periodically by small orbit trim maneuvers to maximize conjunction time. Also, P1 and P2 
undergo inclination change maneuvers to counteract lunar perturbations. The orbital period of P5 is 
changed from initially 4/5 days to 8/7 and 8/9 days to optimize dayside measurements during the first and 
second year, respectively. P3 and P4 remain in the same orbits throughout the mission. Fuel budgets for P3 
and P4 allow these probes to replace P1 or P2 in the unlikely event of a probe loss. 
After the second year of operations the probes are positioned for re-entry course. All maneuvers occur 
while in contact with a ground station with full a priori and a posteriori (near-real-time) attitude and orbit 
determination. 
7.2 GBO Installation, Monitoring, Control, and Maintenance 
Responsibilities for the various sites are shared between UCB, UC, UCLA, and UA. A breakdown of these 
responsibilities is listed below: 
1.	 UC physically installs and maintains the Canadian GBO's. 
2.	 UCB physically installs and maintains the Alaskan GBO's. 
3.	 UC collects all GBO data (UCB ASI, UCLA GMAG, H&S) and GBO team (UCLA, UCB, UA) 
picks up data from UC. 
4.	 UA recovers CGSM and NRCAN GMAG data. UCB picks up data from UA. 
5.	 UC will have a notification system in place that will react to all high level GBO H&S issues. 
UCB acts in backup capacity for this role. 
6.	 UC maintains the physical status and responds to H&S of the Canadian GBO's 
7.	 UCB maintains the physical status and responds to H&S of the Alaskan GBO's 
8.	 UCLA monitors the data quality of the GMAG data and directs UC to make any configuration or 
calibration changes. Changes are discussed and approved by GBO team. 
9.	 UC monitors the quality of the Canadian ASI data. UC will make changes to instrument 
configuration/calibration after consulting with the GBO team (if the action is not already specified 
in the ops doc). 
10. UCB monitors the quality of the Alaskan ASI data and directs UC to make changes to instrument 
configuration/calibration. 
11. UCLA will recover the E/PO data. UCB picks up data from UCLA. 
12. UCLA will validate data and respond to any H&S issues. 
7.3 E/PO GMAG Control 
UCLA will be responsible for the overall installation of the E/PO GMAGs. Site control, operation and data 
retrieval will be the responsibility of local schools under the guidance of UCLA and UCB. Depending on 
internet access, UCLA will perform configuration and software changes as necessary. The schoolteachers 
associated with each site will carry out a limited degree of maintenance of the units under the direction of 
UCB and UCLA E/PO personnel. Data collection will be the responsibility of UCB and a mirror site will 
be established at UCLA. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
8. Science Data Products 
UCB will receive and process probe instrument telemetry, GBO data, and E/PO GMAG data, producing a 
variety of data products that will be available to the projects scientists and the general public. These 
include: 
• Instrument Data CDF Files 
• Instrument Calibration CDF Files 
• Key Parameter Data 
• GBO ASI Data and Products 
• GBO GMAG Data and Products 
• E/PO GMAG Data and Products 
8.1 Instrument Data CDF Files 
The raw probe engineering and science data that has undergone level zero processing will be used to 
produce daily data CDF files. These files will contain all of the VC1 and VC3 data collected for a GMT 
day (00:00 – 23:59:59). These files are stored locally at UCB. 
Directory Structure: 
File Naming Convention: 
where: 
/data/cdf/probe_id/yyyy/mm/dd 
probe_id.yyyymmdd_sv_pv.cdf 
probe_id = THEMIS_A->THEMIS_E 
yyyy 
= year 
mm 
dd 
sv 
pv 
cdf 
8.2 Instrument Data Calibration Files 
= month (01-12) 
= day (01-31) 
= Software version (00-99) 
= Processing version (00-99) 
= Common Data Format 
Calibration files will exist for each instrument and will be used for processing the raw data quantities that 
are transmitted in VC3. The calibration files will be initially created based on ground based instrument 
testing and evaluation during I&T. 
The following calibration files are planned: 
1) FGM Mounting Orthogonality and Gain 
2) FGM Boom Angles 
3) SCM Boom Angles 
4) SCM Gains 
5) ESA MCP Gains, one per anode for 16 anodes x 2 heads 
6) SST Gains, 3 per telescope x 4 telescopes x 2 shutter conditions (open/closed 
7) EFI 1 current and 3 voltages per sensor x 6 sensors 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
A file will be produced for each day and will be associated with the data for that day. The calibration file 
naming convention and directory structure are listed below. 
Directory Structure: 
File naming convention: 
/data/cal/probe_id/yyyy/mm/dd 
probe_id.type.yyyymmdd.pv.cal 
where: 
probe_id = THEMIS_A -> THEMIS_B 
type 
= calibration type (e.g., SSTG for SST Gains) 
vv 
cal 
= processing version number 
= calibration 
Interprobe calibration will be performed in the early mission phase to confirm individual probe calibrations 
but will not be part of the data analysis efforts thereafter so as not to hold up data dissemination. 
8.3 Key Parameter Data 
Instrument, calibration, and orbit data CDFs, plus GBO and E/PO data, will be used to produce Key 
Parameter data products. The general categories are: 
1) Keograms (GBO Image Data Overview) 
2) Auroral Electrojet Indices (GBO Magnetometer Data Overview) 
3) E/PO Ground Magnetometer Data Overview 
4) P1 data (Probe Overview) 
5) P2 data 
“ 
6) P3 data 
7) P4 data 
8) P5 data 
“ 
“ 
“ 
9) Mission Overview Plot 
The probe overview data will include (all provided in physical units and at 3-s resolution): 
•	 3-D magnetic field, DC waveforms and AC spectrograms. 
•	 3-D electric field, DC waveforms and AC spectrograms. 
•	 Core (10eV-40 keV) ion density and velocity moments, temperature and pressure tensor, energy 
spectrogram. 
•	 Core (5 eV-30 keV) electron temperature and pressure tensor, energy spectrogram. 
•	 Energetic ion and electron fluxes. 
•	 Indications when burst data has been collected 
All key parameter products will be available via the UCB website. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
8.4 GBO Data and Products 
UCB will receive the raw image, GMAG, and calibration data and will produce the following data 
products. 
8.4.1 ASI 
The ASI data and data products available at UCBerkeley will include the following: 
•	 Image Thumbnail frames: 20x20 8-bit values (pixels) plus header information (roughly 50 bytes) 
for a total of ~450bytes. File format = PGM. 
•	 Raw Image Frames: 256x256 16-bit values (pixels). Approximately 131 kbytes in size. File format 
= PNG 
•	 Calibration files 
•	 Keogram summary plots (latitude slice versus time - summary's of one or more stations) 
•	 Composite global views providing both quick overview of data availability and a portal to data 
selection, decommutation, and analysis 
UC will also produce its own ASI data product set which will be available from its own website. In 
addition to the raw image and calibration data, this will include: 
•	 Site status info/plots 
•	 Keograms (latitude slice versus time) for each site 
•	 Merged intensity maps of all stream 1 data 
•	 Merged maps of stream 2 data 
•	 Estimates of visibility based on stars 
•	 Estimates of arc location and orientation (when viewing is good) 
•	 CDF for advanced data products 
•	 raw ASCII and PNG/JPEG for availability lists and summary plots 
8.4.2 GMAG 
The GMAG data will include the following: 
•	 L0 (Level 0) Raw Vector Data 
•	 Calibration Files 
•	 L1 (Level 1 ) Processed Data 
•	 Auroral Electrojet Indices 
Auroral Electrojet indices, already developed for Canopus, give similar, synoptic ground magnetometer 
information (similar to the presentation of NORSTAR data). 
Both UA and UCLA will be producing their own GMAG data products that will be available at their own 
websites. 
8.5 E/PO GMAG Data Products 
When the E/PO GMAG data is received at UCB it will be used to produce Auroral Electrojet Indices 
similar to those produced using the GBO GMAG data. The data will also be folded into mission composite 
summary plots that will display probe, GBO, and E/PO GMAG data. UCLA will also be producing its own 
data product set. 
•	 L0 (Level 0) Raw Vector Data 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
• Calibration Files 
• L1 (Level 1 ) Processed Data 
• Auroral Electrojet Indices 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
9. Data Access 
Access to the THEMIS data and products will be through one of the following: 
1. 
2. 
THEMIS Data Analysis Software Package File Search Tool 
THEMIS Website 
9.1 THEMIS Data Analysis Software Package File Search Tool 
The File Search Tool (FST) is part of the Data Analysis Software Package which will be available for 
download from the THEMIS website. The FST will allow both internal and external users (with permission 
from PI) to query for instrument data and calibration CDF files and associated GBO and E/PO data based 
on probe ID, date, and time. The internal users will copy files over the Local Area Network (LAN) while 
external users will use secure FTP or COPY. 
9.2 Website 
The THEMIS website will allow users to query for instrument data and calibration CDF files, GBO ASI 
and GMAG raw and calibration files, E/PO GMAG raw and calibration files, and all of the associated data 
products. 
10. Data Analysis Software 
10.1 Overview 
UCB will produce and make available a software package for accessing and processing the instrument data 
and calibration CDF files, GBO ASI and GMAG data, and the E/PO GMAG data. For instrument data, 
including slow survey, fast survey, and burst, science data analysis software exists in the form of an 
extensive library of IDL programs and the Science Data Tool (SDT) developed at SSL to analyze data from 
FAST, WIND, CLUSTER_II, and POLAR. The decommutator functions that allow access to the raw and 
CDF formatted data are adapted from those developed to for analysis of FAST science data. In general, 
four IDL-based software suites are planned: 
1) Single probe analysis software, is directly transferable to THEMIS from FAST and WIND. 
2) Multi-point data analysis software from ISTP and CLUSTER_II analysis to compute the flow 
shear/curl and pressure gradient along with their standard error will be directly implemented or 
modified for THEMIS 
3) Ancillary Data Software. An existing distributed database of such data will be upgraded with 
IDL decommutators for plotting them seamlessly, relative to THEMIS quantities 
4) Event Modeling. IDL codes that fly virtual probes within simulation run results under specific, 
idealized solar wind external conditions already exist. These will allow comparisons between 
models and observations. 
For ASI and GMAG data, tools already exist for accessing, evaluating, and comparing the ground 
observatory data with the space-borne measurements. Data will be analyzed using standard IDL-Based 
routines developed from years of experience with NORSTAR and other AGO's. 
The different software for accessing and analyzing the variety of data and data products will be integrated 
into one complete package that will be distributed to Co-Is and guest investigators throughout the mission 
as updates are made. Training sessions in the use of this software are planned. The software package will 
also be freely available from the UCB website. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
10.2 File Search Tool 
The file search tool will be available in both a command line and GUI form. The user will enter from 1 to 5 
probe IDs and the time period of interest. The tool will initially check the users local directory for the data, 
and if not found will initiate a LAN copy or FTP session to download the data. In addition to requesting 
probe data, the user can request ASI and GMAG data for the same period. The user can also go down a 
basic directory path either locally or from the website and download data via local copy, remote copy, http, 
or ftp. 
Once data is downloaded and available on the users workstation they can engage the second layer of data 
decommutation software and/or data analysis software described below. 
10.3 Moments & Fields Tool 
This tool will take data from CDF files and covert them into an ASCII file containing Density (N), Velocity 
(V), Temperature (T), Pressure (P), Mag field (Bxyz), Electric Field (Exyz), and Position (X,Y,Z). For the 
particle instruments, this tool will ingest the CDF file and produce an ASCII file that includes: 
 N = Density 
 V = Velocity 
 T = Temperature 
 P = Pressure 
Partial ion density from the ESA: Niesa 
Partial electron pressure from SST: Pesst 
For the Fluxgate Magnetometer, this tool will produce: 
• Bxyz 
For the Electric Field Instrument, this tool will produce: 
• Exyz 
The filter banks from the EFI and SCM will both produce 8 quantities at spin resolution. The 8 quantities 
represent the electric field power in 8 frequency bins, as well as the magnetic field power in 8 freq bins. 
10.4 Reading and Writing Tools 
These include: 
• Basic read routine 
• Basic write routine 
• Conversion routines like cdf2flat, cdf2asci, and cdf2cdf 
• Processing routines 
• Plotting routines like Specplot, dfplot, lineplot, tplot 
11. Data Archiving and Distribution 
The following data will be available in its entirety on disk drives at UCB as well as being backed up onto 
DVD media: 
1. 
2. 
3. 
VC1 and VC3 files from the ground stations 
24 hour LZP data files 
24 hour instrument CDF data files 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
4. 
5. 
GBO Data (ASI & GMAG) 
E/PO GMAG 
The DVD’s containing the 24-hour instrument CDF data files will be distributed to Co-I’s and NSSDC on a 
monthly basis. 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
      
 
 
 
5
10
15
20
25
30
12. Appendix A. Instrument Data Quantities 
Chan Name Description Sync Bytes 
0 DFB64 0 Filters 2 x 8 bytes at 1/16 to 8 Hz 1/16 Hz 2048 
1 DFB65 1 Fast Survey (A) V1-V6 at 2 to 256 Hz 1 Hz 3072 
2 DFB66 2 Fast Survey (B) V1-V6 at 2 to 256 Hz 1 Hz 3072 
3 DFB67 3 Fast Survey E12DC, E34DC, E56DC at 2 to 256 Hz 1 Hz 1536 
4 DFB68 4 Fast Survey SCM1, SCM2, SCM3 at 2 to 256 Hz 1 Hz 1536 
DFB69 5 Particle Burst (A) V1-V6 at 2 to 256 Hz 1 Hz 3072 
6 DFB70 6 Particle Burst (B) V1-V6 at 2 to 256 Hz 1 Hz 3072 
7 DFB71 7 Particle Burst E12DC, E34DC, E56DC at 2 to 256 Hz 1 Hz 1536 
8 DFB72 8 Particle Burst SCM1, SCM2, SCM3 at 2 to 256 Hz 1 Hz 1536 
9 DFB73 9 Fast Survey Spectra 1 to 4  16-64 pts @1/4-8 Hz 1 Hz 2048 
DFB74 10 Wave Burst Spectra 1 to 4 16-64 pts @1/4-8 Hz 1 Hz 2048 
11 DFB75 11 Wave Burst (A) V1-V6 at 512 to 8192 Hz 32 Hz 3072 
12 DFB76 12 Wave Burst (B) V1-V6 at 512 to 8192 Hz 32 Hz 3072 
13 DFB77 13 Wave Burst E12DC, E34DC, E56DC at 512 to 16384 Hz 32 Hz 3072 
14 DFB78 14 Wave Burst SCM1, SCM2, SCM3 at 512 to 16384 Hz 32 Hz 3072 
DFB79 15 Trigger (and Engineering Data) 28-32 values at 16Hz 1 Hz 512 
16 DFB80 16 Spin Fits (E12, E34, E56, V1, V2, V3, V4 * 128Hz) 1 Hz 1792 
Chan Name Description Sync Bytes 
17 ETC_MOM ESA and SST Moments [13x2x2 + 13x2x2 every spin collected for 16 spins] Spin/16 1664 
18 eESA_FDF ESA Burst Electron 88x32 Angle*Energies Spin 2816 
19 iESA_FDF ESA Burst Ion 88x32 Angle*Energies Spin 2816 
eSST_FDF SST Burst Electron 88x32 Angle*Energies Spin 2816 
21 eSST_FDF SST Burst Ion 88x32 Angle*Energies Spin 2816 
22 eESA_FDF ESA Survey Electron 88x32 Angle*Energies Spin/N 2816 
23 iESA_FDF ESA Survey Ion 88x32 Angle*Energies Spin/N 2816 
24 eSST_FDF SST Survey Electron 88x32 Angle*Energies Spin/N 2816 
eSST_FDF SST Survey Ion 88x32 Angle*Energies Spin/N 2816 
26 eESA_RDF ESA Survey Electron 8x16 Angle*Energies Spin/16 2048 
27 iESA_RDF ESA Survey Ion 8x16 Angle*Energies Spin/16 2048 
28 eSST_RDF SST Survey Electron 8x16 Angle*Energies Spin/16 2048 
29 eSST_RDF SST Survey Ion 8x16 Angle*Energies Spin/16 2048 
FGE_TML FGM X, Y, Z from 4 to 32 Hz. Prescaled to 16-bits each 1/16 Hz 3072 
31 FGE_TMH FGM X, Y, Z fixed at 128 Hz. Prescaled to 16-bits each 1/4 Hz 3072 
32 FGE_TMH FGM X, Y, Z fixed at 128 Hz. Prescaled to 16-bits each 1 Hz 768 
NAS5-02099 File: THEMIS_PDMP_draft_20041018.doc 9/2/15 9:45 AM 
 
 
 
 
 
     
      
 
 
 
   
  
  
 
 
      
  
         
         
          
         
    
 
      
    
 
      
    
       
   
   
 
   
 
   
13. Appendix B. Instrument Data Rates 
S/spin CH/agle-ergies BW 
FGM 1 5 DC-0.333Sps 
SCM 1 8 FB: 1Hz-2kSps 
EFI 1 14 DC-2kSps 
SST 1 525 spin 
mom ESA 1 1422 spin 
mom 
Totals (bits/second) 
Data Rates by category (particles and fields) 
Fields 
Particles 
Slow Survey (Radbelt 
science) bits/s S/spin 
27 16 
21 32 
53 32 
160 1 
208 1 
469 
101 
368 
Survey 
CH/agle-ergies BW [Sps] 
3 5.33 
3 10.67 
4 10.67 
2560 spin RDF 
4224 spin RDF 
Fast Survey (baseline science) 
bits/s 
256 
512 
683 
2816 
7627 
11893 
1451 
10443 
Particle Burst (baseline science, all except Wave Burst (HF modes) 
HF modes) Wave Burst 1 Wave Burst 2 
S/sec CH/agle-ergies BW [Sps] bits/s S/sec CH/agle-ergies BW bits/s S/sec CH/agle-ergies BW bits/s 
32 3 DC-16 Hz 1536 128 3 DC-64 Hz 6144 128 3 DC-64 Hz 6144 
128 35 10Hz-128Sps 6656 1024 67 32-512 Hz 51200 4096 67 32-2048 Hz 198656 
128 36 DC-128Sps 8704 1024 68 32-512 Hz 67584 4096 68 32-2048 Hz 264192 -1 spin FDF 10923 -0 -0 -0 -0 -1 spin FDF 15019 -0 -0 -0 -0 
42837 124928 468992 
16896 124928 468992 
2